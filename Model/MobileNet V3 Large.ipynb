{"cells":[{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom PIL import Image\nimport os\nimport pathlib\nimport numpy as np\nimport pathlib\nimport shutil"],"metadata":{"ExecuteTime":{"end_time":"2021-03-17T04:20:11.325951Z","start_time":"2021-03-17T04:19:34.532762Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2aca74f-2f2d-4475-aee5-19061ce67bc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["BASE_PATH = '/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331'\nDATA_PATH = '/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Pictures'\n\ncheckpoint_path = BASE_PATH + \"/Model/Best_Model_Large.h5py\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n# train_dir = pathlib.Path(DATA_PATH+'/training')\n# val_dir = pathlib.Path(DATA_PATH+'/val')\n\n# train_dir = dbutils.fs.ls(DATA_PATH+\"/train/\")\n# val_dir = dbutils.fs.ls(DATA_PATH+\"/val/\")\nbatch_size = 128\nimg_height = 120\nimg_width = 160\n\nresized_height = 120\nresized_width = 160\n\n# IMG_SHAPE = (224, 224, 3)\n# CLASS_PEOPLE = \"/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/training/people_class\"\nnum_classes = 2\nNUM_EPOCHS = 400\nseed = 12\nlog_dir = BASE_PATH + \"/logs\"\n\nSTEPS_PER_EPOCH = 1508 //batch_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9656def-8fa9-4254-ae11-5c925b7b14ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\ndata_augmentation = tf.keras.Sequential([\n#   layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n  layers.experimental.preprocessing.RandomRotation(0.2),\n  layers.experimental.preprocessing.RandomZoom(0.2),\n  layers.experimental.preprocessing.RandomContrast(0.2),\n#   layers.experimental.preprocessing.RandomContrast(0.2),\n])\n\nresize = tf.keras.layers.experimental.preprocessing.Resizing(resized_height, resized_width)\n\ndef preprocess_input(image):\n    return (image/127.5) - 1\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  DATA_PATH,\n  seed=seed,\n  shuffle=True,\n  validation_split=0.2,\n  subset='training',\n  image_size=(img_height, img_width)\n)\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\ntrain_ds = train_ds.map(\n  lambda x, y: (data_augmentation(x, training=True), y))\n\ntrain_ds = train_ds.map(\n  lambda x, y: (preprocess_input(x), y))\n\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  DATA_PATH,\n  seed=seed,\n  shuffle=True,\n  validation_split=0.2,\n  subset='validation',\n  image_size=(img_height, img_width)\n)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.map(\n  lambda x, y: (preprocess_input(x), y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99ce880e-5476-4f8e-b471-653a76a034d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Found 11163 files belonging to 2 classes.\nUsing 8931 files for training.\nFound 11163 files belonging to 2 classes.\nUsing 2232 files for validation.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Found 11163 files belonging to 2 classes.\nUsing 8931 files for training.\nFound 11163 files belonging to 2 classes.\nUsing 2232 files for validation.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["class CustomLearningRateScheduler(tf.keras.callbacks.Callback):\n    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \"\"\"\n\n    def __init__(self, schedule):\n        super(CustomLearningRateScheduler, self).__init__()\n        self.schedule = schedule\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, \"lr\"):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        # Get the current learning rate from model's optimizer.\n        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        # Call schedule function to get the scheduled learning rate.\n        scheduled_lr = self.schedule(epoch, lr)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n\n\nLR_SCHEDULE = [\n    # (epoch to start, learning rate) tuples\n    (10, 0.005),\n    (30, 0.001),\n    (50, 0.0005)\n]\n\n\ndef lr_schedule(epoch, lr):\n    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n        return lr\n    for i in range(len(LR_SCHEDULE)):\n        if epoch == LR_SCHEDULE[i][0]:\n            return LR_SCHEDULE[i][1]\n    return lr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdecf590-69e1-4548-be8e-746ec23f9612"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, GlobalAveragePooling2D,Input\nfrom tensorflow.keras.layers import Activation, BatchNormalization, Add, Multiply, Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\n# 定义relu6激活函数\ndef relu6(x):\n    return K.relu(x, max_value=6.0)\n# 定义h-swish激活函数\ndef hard_swish(x):\n    return x * K.relu(x + 3.0, max_value=6.0) / 6.0\n# 定义返回的激活函数是relu6还是h-swish\ndef return_activation(x, nl):\n    if nl == 'HS':\n        x = Activation(hard_swish)(x)\n    if nl == 'RE':\n        x = Activation(relu6)(x)\n    return x\n# 定义卷积块(卷积+标准化+激活函数)\ndef conv_block(inputs, filters, kernel, strides, nl):\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n    x = Conv2D(filters, kernel, padding='same', strides=strides)(inputs)\n    x = BatchNormalization(axis=channel_axis)(x)\n    return return_activation(x, nl)\n# 定义注意力机制模块\ndef SE(inputs):\n    input_channels = int(inputs.shape[-1])\n    x = GlobalAveragePooling2D()(inputs)\n    x = Dense(input_channels, activation='relu')(x)\n    x = Dense(input_channels, activation='hard_sigmoid')(x)\n    x = Reshape((1, 1, input_channels))(x)\n    x = Multiply()([inputs, x])\n    return x\n\ndef bottleneck(inputs, filters, kernel, e, s, squeeze, nl,alpha=1.0):\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n    input_shape = K.int_shape(inputs)\n    tchannel = int(e)\n    cchannel = int(alpha * filters)\n    r = s == 1 and input_shape[3] == filters\n    x = conv_block(inputs, tchannel, (1,1), (1,1), nl)\n    x = DepthwiseConv2D(kernel, strides=(s,s), depth_multiplier=1, padding='same')(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = return_activation(x,nl)\n    if squeeze:\n        x = SE(x)\n    x = Conv2D(cchannel, (1,1), strides=(1,1), padding='same')(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    if r:\n        x = Add()([x, inputs])\n    return x\n\n\ndef MobileNetV3_Large(shape = (224,224,3), alpha=1.0):\n    inputs = Input(shape)\n    # conv2d_1 (Conv2D) - activation_1 (Activation)\n    x = conv_block(inputs, 16, (3,3), strides=(2,2), nl='HS')\n    # conv2d_2 (Conv2D) - add_1 (Add)\n    x = bottleneck(x, 16, (3, 3), e=16, s=1, squeeze=False, nl='RE', alpha=alpha)\n    # conv2d_4 (Conv2D) - batch_normalization_7\n    x = bottleneck(x, 24, (3, 3), e=64, s=2, squeeze=False, nl='RE', alpha=alpha)\n    # conv2d_6 (Conv2D) - add_2 (Add)\n    x = bottleneck(x, 24, (3, 3), e=72, s=1, squeeze=False, nl='RE', alpha=alpha)\n    # conv2d_8 (Conv2D) - batch_normalization_13\n    x = bottleneck(x, 40, (5, 5), e=72, s=2, squeeze=True, nl='RE', alpha=alpha)\n    # conv2d_10 (Conv2D) - add_3 (Add)\n    x = bottleneck(x, 40, (5, 5), e=120, s=1, squeeze=True, nl='RE', alpha=alpha)\n    # conv2d_12 (Conv2D) - add_4 (Add)   (None, 52, 52, 40)    70层\n    x = bottleneck(x, 40, (5, 5), e=120, s=1, squeeze=True, nl='RE', alpha=alpha)\n\n\n\n    # conv2d_14 (Conv2D) - batch_normalization_22  (None, 26, 26, 80)\n    x = bottleneck(x, 80, (3, 3), e=240, s=2, squeeze=False, nl='HS', alpha=alpha)\n    # conv2d_16 (Conv2D) - add_5 (Add)   (None, 26, 26, 80)\n    x = bottleneck(x, 80, (3, 3), e=200, s=1, squeeze=False, nl='HS', alpha=alpha)\n    # conv2d_18 (Conv2D) - add_6 (Add)\n    x = bottleneck(x, 80, (3, 3), e=184, s=1, squeeze=False, nl='HS', alpha=alpha)\n    # conv2d_20 (Conv2D) - add_7 (Add)    (None, 26, 26, 80)\n    x = bottleneck(x, 80, (3, 3), e=184, s=1, squeeze=False, nl='HS', alpha=alpha)\n    # conv2d_22 (Conv2D) - batch_normalization_34   (None, 26, 26, 112)\n    # inputs=(26,26,80) filtters=112 所以没有Add\n    x = bottleneck(x, 112, (3, 3), e=480, s=1, squeeze=True, nl='HS', alpha=alpha)\n    # conv2d_24 (Conv2D) - add_8 (Add)   (None, 26, 26, 112)     132层\n    x = bottleneck(x, 112, (3, 3), e=672, s=1, squeeze=True, nl='HS', alpha=alpha)\n\n\n\n    # conv2d_26 (Conv2D) - batch_normalization_40   (None, 13, 13, 160)\n    x = bottleneck(x, 160, (5, 5), e=672, s=2, squeeze=True, nl='HS', alpha=alpha)\n    # conv2d_28 (Conv2D) - add_9 (Add)   (None, 13, 13, 160)\n    x = bottleneck(x, 160, (5, 5), e=960, s=1, squeeze=True, nl='HS', alpha=alpha)\n    # conv2d_30 (Conv2D) - add_10 (Add)  (None, 13, 13, 160)\n    x = bottleneck(x, 160, (5, 5), e=960, s=1, squeeze=True, nl='HS', alpha=alpha)\n    \n    \n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n    \n    \n    model = Model(inputs, x)\n    return model\n\nmodel = MobileNetV3_Large(shape = (img_height, img_width, 3))\nmodel.summary()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcf06549-04fc-4f50-afe0-5fe4d1b476d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Model: &#34;functional_1&#34;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 120, 160, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 60, 80, 16)   448         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 60, 80, 16)   64          conv2d[0][0]                     \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 60, 80, 16)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 60, 80, 16)   272         activation[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 60, 80, 16)   64          conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 60, 80, 16)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d (DepthwiseConv (None, 60, 80, 16)   160         activation_1[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 60, 80, 16)   64          depthwise_conv2d[0][0]           \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 60, 80, 16)   0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 60, 80, 16)   272         activation_2[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 60, 80, 16)   64          conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nadd (Add)                       (None, 60, 80, 16)   0           batch_normalization_3[0][0]      \n                                                                 activation[0][0]                 \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 60, 80, 64)   1088        add[0][0]                        \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 60, 80, 64)   256         conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 60, 80, 64)   0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d_1 (DepthwiseCo (None, 30, 40, 64)   640         activation_3[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 30, 40, 64)   256         depthwise_conv2d_1[0][0]         \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 30, 40, 64)   0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 30, 40, 24)   1560        activation_4[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 30, 40, 24)   96          conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 30, 40, 72)   1800        batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 30, 40, 72)   288         conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 30, 40, 72)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d_2 (DepthwiseCo (None, 30, 40, 72)   720         activation_5[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 30, 40, 72)   288         depthwise_conv2d_2[0][0]         \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 30, 40, 72)   0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 30, 40, 24)   1752        activation_6[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 30, 40, 24)   96          conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 30, 40, 24)   0           batch_normalization_9[0][0]      \n                                                                 batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 30, 40, 72)   1800        add_1[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 30, 40, 72)   288         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 30, 40, 72)   0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_3 (DepthwiseCo (None, 15, 20, 72)   1872        activation_7[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 15, 20, 72)   288         depthwise_conv2d_3[0][0]         \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 15, 20, 72)   0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 72)           0           activation_8[0][0]               \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 72)           5256        global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 72)           5256        dense[0][0]                      \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 1, 1, 72)     0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 15, 20, 72)   0           activation_8[0][0]               \n                                                                 reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 15, 20, 40)   2920        multiply[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 15, 20, 40)   160         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 15, 20, 120)  4920        batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 15, 20, 120)  480         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 15, 20, 120)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_4 (DepthwiseCo (None, 15, 20, 120)  3120        activation_9[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 15, 20, 120)  480         depthwise_conv2d_4[0][0]         \n__________________________________________________________________________________________________\nactivation_10 (Activation)      (None, 15, 20, 120)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_1 (Glo (None, 120)          0           activation_10[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 120)          14520       global_average_pooling2d_1[0][0] \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 120)          14520       dense_2[0][0]                    \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 1, 1, 120)    0           dense_3[0][0]                    \n__________________________________________________________________________________________________\nmultiply_1 (Multiply)           (None, 15, 20, 120)  0           activation_10[0][0]              \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 15, 20, 40)   4840        multiply_1[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 15, 20, 40)   160         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 15, 20, 40)   0           batch_normalization_15[0][0]     \n                                                                 batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 15, 20, 120)  4920        add_2[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 15, 20, 120)  480         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nactivation_11 (Activation)      (None, 15, 20, 120)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_5 (DepthwiseCo (None, 15, 20, 120)  3120        activation_11[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 15, 20, 120)  480         depthwise_conv2d_5[0][0]         \n__________________________________________________________________________________________________\nactivation_12 (Activation)      (None, 15, 20, 120)  0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_2 (Glo (None, 120)          0           activation_12[0][0]              \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 120)          14520       global_average_pooling2d_2[0][0] \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 120)          14520       dense_4[0][0]                    \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 1, 1, 120)    0           dense_5[0][0]                    \n__________________________________________________________________________________________________\nmultiply_2 (Multiply)           (None, 15, 20, 120)  0           activation_12[0][0]              \n                                                                 reshape_2[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 15, 20, 40)   4840        multiply_2[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 15, 20, 40)   160         conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 15, 20, 40)   0           batch_normalization_18[0][0]     \n                                                                 add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 15, 20, 240)  9840        add_3[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 15, 20, 240)  960         conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nactivation_13 (Activation)      (None, 15, 20, 240)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_6 (DepthwiseCo (None, 8, 10, 240)   2400        activation_13[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 8, 10, 240)   960         depthwise_conv2d_6[0][0]         \n__________________________________________________________________________________________________\nactivation_14 (Activation)      (None, 8, 10, 240)   0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 8, 10, 80)    19280       activation_14[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 8, 10, 80)    320         conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 8, 10, 200)   16200       batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 8, 10, 200)   800         conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nactivation_15 (Activation)      (None, 8, 10, 200)   0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_7 (DepthwiseCo (None, 8, 10, 200)   2000        activation_15[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 8, 10, 200)   800         depthwise_conv2d_7[0][0]         \n__________________________________________________________________________________________________\nactivation_16 (Activation)      (None, 8, 10, 200)   0           batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 8, 10, 80)    16080       activation_16[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 8, 10, 80)    320         conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 8, 10, 80)    0           batch_normalization_24[0][0]     \n                                                                 batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 8, 10, 184)   14904       add_4[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 8, 10, 184)   736         conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nactivation_17 (Activation)      (None, 8, 10, 184)   0           batch_normalization_25[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_8 (DepthwiseCo (None, 8, 10, 184)   1840        activation_17[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 8, 10, 184)   736         depthwise_conv2d_8[0][0]         \n__________________________________________________________________________________________________\nactivation_18 (Activation)      (None, 8, 10, 184)   0           batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 8, 10, 80)    14800       activation_18[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 8, 10, 80)    320         conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 8, 10, 80)    0           batch_normalization_27[0][0]     \n                                                                 add_4[0][0]                      \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 8, 10, 184)   14904       add_5[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 8, 10, 184)   736         conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nactivation_19 (Activation)      (None, 8, 10, 184)   0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_9 (DepthwiseCo (None, 8, 10, 184)   1840        activation_19[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 8, 10, 184)   736         depthwise_conv2d_9[0][0]         \n__________________________________________________________________________________________________\nactivation_20 (Activation)      (None, 8, 10, 184)   0           batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 8, 10, 80)    14800       activation_20[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 8, 10, 80)    320         conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 8, 10, 80)    0           batch_normalization_30[0][0]     \n                                                                 add_5[0][0]                      \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 8, 10, 480)   38880       add_6[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 8, 10, 480)   1920        conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nactivation_21 (Activation)      (None, 8, 10, 480)   0           batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_10 (DepthwiseC (None, 8, 10, 480)   4800        activation_21[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 8, 10, 480)   1920        depthwise_conv2d_10[0][0]        \n__________________________________________________________________________________________________\nactivation_22 (Activation)      (None, 8, 10, 480)   0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_3 (Glo (None, 480)          0           activation_22[0][0]              \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 480)          230880      global_average_pooling2d_3[0][0] \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 480)          230880      dense_6[0][0]                    \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 1, 1, 480)    0           dense_7[0][0]                    \n__________________________________________________________________________________________________\nmultiply_3 (Multiply)           (None, 8, 10, 480)   0           activation_22[0][0]              \n                                                                 reshape_3[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 8, 10, 112)   53872       multiply_3[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 8, 10, 112)   448         conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 8, 10, 672)   75936       batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 8, 10, 672)   2688        conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nactivation_23 (Activation)      (None, 8, 10, 672)   0           batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_11 (DepthwiseC (None, 8, 10, 672)   6720        activation_23[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 8, 10, 672)   2688        depthwise_conv2d_11[0][0]        \n__________________________________________________________________________________________________\nactivation_24 (Activation)      (None, 8, 10, 672)   0           batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_4 (Glo (None, 672)          0           activation_24[0][0]              \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 672)          452256      global_average_pooling2d_4[0][0] \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 672)          452256      dense_8[0][0]                    \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, 1, 1, 672)    0           dense_9[0][0]                    \n__________________________________________________________________________________________________\nmultiply_4 (Multiply)           (None, 8, 10, 672)   0           activation_24[0][0]              \n                                                                 reshape_4[0][0]                  \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 8, 10, 112)   75376       multiply_4[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 8, 10, 112)   448         conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 8, 10, 112)   0           batch_normalization_36[0][0]     \n                                                                 batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 8, 10, 672)   75936       add_7[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_37 (BatchNo (None, 8, 10, 672)   2688        conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nactivation_25 (Activation)      (None, 8, 10, 672)   0           batch_normalization_37[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_12 (DepthwiseC (None, 4, 5, 672)    17472       activation_25[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_38 (BatchNo (None, 4, 5, 672)    2688        depthwise_conv2d_12[0][0]        \n__________________________________________________________________________________________________\nactivation_26 (Activation)      (None, 4, 5, 672)    0           batch_normalization_38[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_5 (Glo (None, 672)          0           activation_26[0][0]              \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 672)          452256      global_average_pooling2d_5[0][0] \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 672)          452256      dense_10[0][0]                   \n__________________________________________________________________________________________________\nreshape_5 (Reshape)             (None, 1, 1, 672)    0           dense_11[0][0]                   \n__________________________________________________________________________________________________\nmultiply_5 (Multiply)           (None, 4, 5, 672)    0           activation_26[0][0]              \n                                                                 reshape_5[0][0]                  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 4, 5, 160)    107680      multiply_5[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 4, 5, 160)    640         conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 4, 5, 960)    154560      batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_40 (BatchNo (None, 4, 5, 960)    3840        conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nactivation_27 (Activation)      (None, 4, 5, 960)    0           batch_normalization_40[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_13 (DepthwiseC (None, 4, 5, 960)    24960       activation_27[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_41 (BatchNo (None, 4, 5, 960)    3840        depthwise_conv2d_13[0][0]        \n__________________________________________________________________________________________________\nactivation_28 (Activation)      (None, 4, 5, 960)    0           batch_normalization_41[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_6 (Glo (None, 960)          0           activation_28[0][0]              \n__________________________________________________________________________________________________\ndense_12 (Dense)                (None, 960)          922560      global_average_pooling2d_6[0][0] \n__________________________________________________________________________________________________\ndense_13 (Dense)                (None, 960)          922560      dense_12[0][0]                   \n__________________________________________________________________________________________________\nreshape_6 (Reshape)             (None, 1, 1, 960)    0           dense_13[0][0]                   \n__________________________________________________________________________________________________\nmultiply_6 (Multiply)           (None, 4, 5, 960)    0           activation_28[0][0]              \n                                                                 reshape_6[0][0]                  \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 4, 5, 160)    153760      multiply_6[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_42 (BatchNo (None, 4, 5, 160)    640         conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 4, 5, 160)    0           batch_normalization_42[0][0]     \n                                                                 batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 4, 5, 960)    154560      add_8[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 4, 5, 960)    3840        conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nactivation_29 (Activation)      (None, 4, 5, 960)    0           batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_14 (DepthwiseC (None, 4, 5, 960)    24960       activation_29[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_44 (BatchNo (None, 4, 5, 960)    3840        depthwise_conv2d_14[0][0]        \n__________________________________________________________________________________________________\nactivation_30 (Activation)      (None, 4, 5, 960)    0           batch_normalization_44[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_7 (Glo (None, 960)          0           activation_30[0][0]              \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 960)          922560      global_average_pooling2d_7[0][0] \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 960)          922560      dense_14[0][0]                   \n__________________________________________________________________________________________________\nreshape_7 (Reshape)             (None, 1, 1, 960)    0           dense_15[0][0]                   \n__________________________________________________________________________________________________\nmultiply_7 (Multiply)           (None, 4, 5, 960)    0           activation_30[0][0]              \n                                                                 reshape_7[0][0]                  \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 4, 5, 160)    153760      multiply_7[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_45 (BatchNo (None, 4, 5, 160)    640         conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 4, 5, 160)    0           batch_normalization_45[0][0]     \n                                                                 add_8[0][0]                      \n__________________________________________________________________________________________________\nglobal_average_pooling2d_8 (Glo (None, 160)          0           add_9[0][0]                      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 160)          0           global_average_pooling2d_8[0][0] \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 2)            322         dropout[0][0]                    \n==================================================================================================\nTotal params: 7,368,146\nTrainable params: 7,345,634\nNon-trainable params: 22,512\n__________________________________________________________________________________________________\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model: &#34;functional_1&#34;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 120, 160, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 60, 80, 16)   448         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 60, 80, 16)   64          conv2d[0][0]                     \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 60, 80, 16)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 60, 80, 16)   272         activation[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 60, 80, 16)   64          conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 60, 80, 16)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d (DepthwiseConv (None, 60, 80, 16)   160         activation_1[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 60, 80, 16)   64          depthwise_conv2d[0][0]           \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 60, 80, 16)   0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 60, 80, 16)   272         activation_2[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 60, 80, 16)   64          conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nadd (Add)                       (None, 60, 80, 16)   0           batch_normalization_3[0][0]      \n                                                                 activation[0][0]                 \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 60, 80, 64)   1088        add[0][0]                        \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 60, 80, 64)   256         conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 60, 80, 64)   0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d_1 (DepthwiseCo (None, 30, 40, 64)   640         activation_3[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 30, 40, 64)   256         depthwise_conv2d_1[0][0]         \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 30, 40, 64)   0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 30, 40, 24)   1560        activation_4[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 30, 40, 24)   96          conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 30, 40, 72)   1800        batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 30, 40, 72)   288         conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 30, 40, 72)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\ndepthwise_conv2d_2 (DepthwiseCo (None, 30, 40, 72)   720         activation_5[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 30, 40, 72)   288         depthwise_conv2d_2[0][0]         \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 30, 40, 72)   0           batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 30, 40, 24)   1752        activation_6[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 30, 40, 24)   96          conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 30, 40, 24)   0           batch_normalization_9[0][0]      \n                                                                 batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 30, 40, 72)   1800        add_1[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 30, 40, 72)   288         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 30, 40, 72)   0           batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_3 (DepthwiseCo (None, 15, 20, 72)   1872        activation_7[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 15, 20, 72)   288         depthwise_conv2d_3[0][0]         \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 15, 20, 72)   0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 72)           0           activation_8[0][0]               \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 72)           5256        global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 72)           5256        dense[0][0]                      \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 1, 1, 72)     0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 15, 20, 72)   0           activation_8[0][0]               \n                                                                 reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 15, 20, 40)   2920        multiply[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 15, 20, 40)   160         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 15, 20, 120)  4920        batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 15, 20, 120)  480         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nactivation_9 (Activation)       (None, 15, 20, 120)  0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_4 (DepthwiseCo (None, 15, 20, 120)  3120        activation_9[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 15, 20, 120)  480         depthwise_conv2d_4[0][0]         \n__________________________________________________________________________________________________\nactivation_10 (Activation)      (None, 15, 20, 120)  0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_1 (Glo (None, 120)          0           activation_10[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 120)          14520       global_average_pooling2d_1[0][0] \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 120)          14520       dense_2[0][0]                    \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 1, 1, 120)    0           dense_3[0][0]                    \n__________________________________________________________________________________________________\nmultiply_1 (Multiply)           (None, 15, 20, 120)  0           activation_10[0][0]              \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 15, 20, 40)   4840        multiply_1[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 15, 20, 40)   160         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 15, 20, 40)   0           batch_normalization_15[0][0]     \n                                                                 batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 15, 20, 120)  4920        add_2[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 15, 20, 120)  480         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nactivation_11 (Activation)      (None, 15, 20, 120)  0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_5 (DepthwiseCo (None, 15, 20, 120)  3120        activation_11[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 15, 20, 120)  480         depthwise_conv2d_5[0][0]         \n__________________________________________________________________________________________________\nactivation_12 (Activation)      (None, 15, 20, 120)  0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_2 (Glo (None, 120)          0           activation_12[0][0]              \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 120)          14520       global_average_pooling2d_2[0][0] \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 120)          14520       dense_4[0][0]                    \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 1, 1, 120)    0           dense_5[0][0]                    \n__________________________________________________________________________________________________\nmultiply_2 (Multiply)           (None, 15, 20, 120)  0           activation_12[0][0]              \n                                                                 reshape_2[0][0]                  \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 15, 20, 40)   4840        multiply_2[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 15, 20, 40)   160         conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 15, 20, 40)   0           batch_normalization_18[0][0]     \n                                                                 add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 15, 20, 240)  9840        add_3[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 15, 20, 240)  960         conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nactivation_13 (Activation)      (None, 15, 20, 240)  0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_6 (DepthwiseCo (None, 8, 10, 240)   2400        activation_13[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 8, 10, 240)   960         depthwise_conv2d_6[0][0]         \n__________________________________________________________________________________________________\nactivation_14 (Activation)      (None, 8, 10, 240)   0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 8, 10, 80)    19280       activation_14[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 8, 10, 80)    320         conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 8, 10, 200)   16200       batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 8, 10, 200)   800         conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nactivation_15 (Activation)      (None, 8, 10, 200)   0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_7 (DepthwiseCo (None, 8, 10, 200)   2000        activation_15[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 8, 10, 200)   800         depthwise_conv2d_7[0][0]         \n__________________________________________________________________________________________________\nactivation_16 (Activation)      (None, 8, 10, 200)   0           batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 8, 10, 80)    16080       activation_16[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 8, 10, 80)    320         conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 8, 10, 80)    0           batch_normalization_24[0][0]     \n                                                                 batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 8, 10, 184)   14904       add_4[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 8, 10, 184)   736         conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nactivation_17 (Activation)      (None, 8, 10, 184)   0           batch_normalization_25[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_8 (DepthwiseCo (None, 8, 10, 184)   1840        activation_17[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 8, 10, 184)   736         depthwise_conv2d_8[0][0]         \n__________________________________________________________________________________________________\nactivation_18 (Activation)      (None, 8, 10, 184)   0           batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 8, 10, 80)    14800       activation_18[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 8, 10, 80)    320         conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 8, 10, 80)    0           batch_normalization_27[0][0]     \n                                                                 add_4[0][0]                      \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 8, 10, 184)   14904       add_5[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 8, 10, 184)   736         conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nactivation_19 (Activation)      (None, 8, 10, 184)   0           batch_normalization_28[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_9 (DepthwiseCo (None, 8, 10, 184)   1840        activation_19[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 8, 10, 184)   736         depthwise_conv2d_9[0][0]         \n__________________________________________________________________________________________________\nactivation_20 (Activation)      (None, 8, 10, 184)   0           batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 8, 10, 80)    14800       activation_20[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 8, 10, 80)    320         conv2d_20[0][0]                  \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 8, 10, 80)    0           batch_normalization_30[0][0]     \n                                                                 add_5[0][0]                      \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 8, 10, 480)   38880       add_6[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 8, 10, 480)   1920        conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nactivation_21 (Activation)      (None, 8, 10, 480)   0           batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_10 (DepthwiseC (None, 8, 10, 480)   4800        activation_21[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 8, 10, 480)   1920        depthwise_conv2d_10[0][0]        \n__________________________________________________________________________________________________\nactivation_22 (Activation)      (None, 8, 10, 480)   0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_3 (Glo (None, 480)          0           activation_22[0][0]              \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 480)          230880      global_average_pooling2d_3[0][0] \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 480)          230880      dense_6[0][0]                    \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 1, 1, 480)    0           dense_7[0][0]                    \n__________________________________________________________________________________________________\nmultiply_3 (Multiply)           (None, 8, 10, 480)   0           activation_22[0][0]              \n                                                                 reshape_3[0][0]                  \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 8, 10, 112)   53872       multiply_3[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 8, 10, 112)   448         conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 8, 10, 672)   75936       batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 8, 10, 672)   2688        conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nactivation_23 (Activation)      (None, 8, 10, 672)   0           batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_11 (DepthwiseC (None, 8, 10, 672)   6720        activation_23[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 8, 10, 672)   2688        depthwise_conv2d_11[0][0]        \n__________________________________________________________________________________________________\nactivation_24 (Activation)      (None, 8, 10, 672)   0           batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_4 (Glo (None, 672)          0           activation_24[0][0]              \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 672)          452256      global_average_pooling2d_4[0][0] \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 672)          452256      dense_8[0][0]                    \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, 1, 1, 672)    0           dense_9[0][0]                    \n__________________________________________________________________________________________________\nmultiply_4 (Multiply)           (None, 8, 10, 672)   0           activation_24[0][0]              \n                                                                 reshape_4[0][0]                  \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 8, 10, 112)   75376       multiply_4[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 8, 10, 112)   448         conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 8, 10, 112)   0           batch_normalization_36[0][0]     \n                                                                 batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 8, 10, 672)   75936       add_7[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_37 (BatchNo (None, 8, 10, 672)   2688        conv2d_25[0][0]                  \n__________________________________________________________________________________________________\nactivation_25 (Activation)      (None, 8, 10, 672)   0           batch_normalization_37[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_12 (DepthwiseC (None, 4, 5, 672)    17472       activation_25[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_38 (BatchNo (None, 4, 5, 672)    2688        depthwise_conv2d_12[0][0]        \n__________________________________________________________________________________________________\nactivation_26 (Activation)      (None, 4, 5, 672)    0           batch_normalization_38[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_5 (Glo (None, 672)          0           activation_26[0][0]              \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 672)          452256      global_average_pooling2d_5[0][0] \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 672)          452256      dense_10[0][0]                   \n__________________________________________________________________________________________________\nreshape_5 (Reshape)             (None, 1, 1, 672)    0           dense_11[0][0]                   \n__________________________________________________________________________________________________\nmultiply_5 (Multiply)           (None, 4, 5, 672)    0           activation_26[0][0]              \n                                                                 reshape_5[0][0]                  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 4, 5, 160)    107680      multiply_5[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_39 (BatchNo (None, 4, 5, 160)    640         conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 4, 5, 960)    154560      batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_40 (BatchNo (None, 4, 5, 960)    3840        conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nactivation_27 (Activation)      (None, 4, 5, 960)    0           batch_normalization_40[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_13 (DepthwiseC (None, 4, 5, 960)    24960       activation_27[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_41 (BatchNo (None, 4, 5, 960)    3840        depthwise_conv2d_13[0][0]        \n__________________________________________________________________________________________________\nactivation_28 (Activation)      (None, 4, 5, 960)    0           batch_normalization_41[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_6 (Glo (None, 960)          0           activation_28[0][0]              \n__________________________________________________________________________________________________\ndense_12 (Dense)                (None, 960)          922560      global_average_pooling2d_6[0][0] \n__________________________________________________________________________________________________\ndense_13 (Dense)                (None, 960)          922560      dense_12[0][0]                   \n__________________________________________________________________________________________________\nreshape_6 (Reshape)             (None, 1, 1, 960)    0           dense_13[0][0]                   \n__________________________________________________________________________________________________\nmultiply_6 (Multiply)           (None, 4, 5, 960)    0           activation_28[0][0]              \n                                                                 reshape_6[0][0]                  \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 4, 5, 160)    153760      multiply_6[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_42 (BatchNo (None, 4, 5, 160)    640         conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 4, 5, 160)    0           batch_normalization_42[0][0]     \n                                                                 batch_normalization_39[0][0]     \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 4, 5, 960)    154560      add_8[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_43 (BatchNo (None, 4, 5, 960)    3840        conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nactivation_29 (Activation)      (None, 4, 5, 960)    0           batch_normalization_43[0][0]     \n__________________________________________________________________________________________________\ndepthwise_conv2d_14 (DepthwiseC (None, 4, 5, 960)    24960       activation_29[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_44 (BatchNo (None, 4, 5, 960)    3840        depthwise_conv2d_14[0][0]        \n__________________________________________________________________________________________________\nactivation_30 (Activation)      (None, 4, 5, 960)    0           batch_normalization_44[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d_7 (Glo (None, 960)          0           activation_30[0][0]              \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 960)          922560      global_average_pooling2d_7[0][0] \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 960)          922560      dense_14[0][0]                   \n__________________________________________________________________________________________________\nreshape_7 (Reshape)             (None, 1, 1, 960)    0           dense_15[0][0]                   \n__________________________________________________________________________________________________\nmultiply_7 (Multiply)           (None, 4, 5, 960)    0           activation_30[0][0]              \n                                                                 reshape_7[0][0]                  \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 4, 5, 160)    153760      multiply_7[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_45 (BatchNo (None, 4, 5, 160)    640         conv2d_30[0][0]                  \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 4, 5, 160)    0           batch_normalization_45[0][0]     \n                                                                 add_8[0][0]                      \n__________________________________________________________________________________________________\nglobal_average_pooling2d_8 (Glo (None, 160)          0           add_9[0][0]                      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 160)          0           global_average_pooling2d_8[0][0] \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 2)            322         dropout[0][0]                    \n==================================================================================================\nTotal params: 7,368,146\nTrainable params: 7,345,634\nNon-trainable params: 22,512\n__________________________________________________________________________________________________\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_compiled_model(alpha=1.0, lr=0.001):\n    model = MobileNetV3_Large(shape = (img_height, img_width, 3))\n    \n    model.compile(\n                optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9),\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efb76772-5ba0-4868-9d28-45bd917b4836"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(train_ds, val_ds=None, lr=0.001):\n    model = get_compiled_model()\n#     model = get_compiled_model_2()\n    \n#     model.save_weights(checkpoint_path.format(epoch=0))\n\n    # 创建一个保存模型权重的回调\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                     save_weights_only= False,\n                                                     save_best_only = True,\n                                                     monitor='accuracy',\n                                                     mode='auto', \n                                                     save_freq='epoch',\n                                                     verbose=1)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n    board_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=log_dir,\n        histogram_freq=0,  # How often to log histogram visualizations\n        embeddings_freq=0,  # How often to log embedding visualizations\n        update_freq=\"epoch\",\n    )  # How often to write logs (default: once per epoch)\n    \n#     steps_per_epoch = len(train_ds) // batch_size\n    hist = model.fit(train_ds, \n#                      steps_per_epoch=steps_per_epoch,\n                     epochs=NUM_EPOCHS,\n                     validation_data=val_ds,\n#                      class_weight={0:0.3, 1:0.7},\n#                      validation_steps=validation_steps,\n                     verbose=2,\n                     callbacks=[\n                       cp_callback, \n#                        board_callback, \n                       early_stop,]\n                     )\n#     model.save('saved_model/my_model')\n    return hist,model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36b6f893-7f0c-41ec-b049-985d42543ba7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["gpus = tf.config.experimental.list_physical_devices('GPU')\nprint(gpus)\nif gpus:\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n  try:\n    for gpu in gpus:\n#       tf.config.experimental.set_memory_growth(gpu, True)\n      print('yes')\n#     tf.config.experimental.set_virtual_device_configuration(\n#         gpus[0],\n#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e02e922-b7c8-4368-af37-78a199f4b31c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]\nyes\nyes\n2 Physical GPUs, 2 Logical GPUs\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]\nyes\nyes\n2 Physical GPUs, 2 Logical GPUs\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["tf.debugging.set_log_device_placement(True)\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n  hist,model = train_and_evaluate(train_ds,val_ds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d82f6f97-cdd4-41dc-afec-12b0b69e0a34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:tensorflow:Using MirroredStrategy with devices (&#39;/job:localhost/replica:0/task:0/device:GPU:0&#39;, &#39;/job:localhost/replica:0/task:0/device:GPU:1&#39;)\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nEpoch 1/400\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\nINFO:tensorflow:batch_all_reduce: 218 all-reduces with algorithm = nccl, num_packs = 1\nINFO:tensorflow:batch_all_reduce: 218 all-reduces with algorithm = nccl, num_packs = 1\n\nEpoch 00001: accuracy improved from -inf to 0.54059, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 194s - loss: 0.7047 - accuracy: 0.5406 - val_loss: 0.6931 - val_accuracy: 0.5049\nEpoch 2/400\n\nEpoch 00002: accuracy improved from 0.54059 to 0.62591, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.6468 - accuracy: 0.6259 - val_loss: 0.6752 - val_accuracy: 0.5990\nEpoch 3/400\n\nEpoch 00003: accuracy improved from 0.62591 to 0.70731, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.5851 - accuracy: 0.7073 - val_loss: 0.6351 - val_accuracy: 0.6541\nEpoch 4/400\n\nEpoch 00004: accuracy improved from 0.70731 to 0.76095, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.5368 - accuracy: 0.7609 - val_loss: 0.5803 - val_accuracy: 0.7236\nEpoch 5/400\n\nEpoch 00005: accuracy improved from 0.76095 to 0.78793, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.5148 - accuracy: 0.7879 - val_loss: 0.5195 - val_accuracy: 0.7823\nEpoch 6/400\n\nEpoch 00006: accuracy improved from 0.78793 to 0.81715, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4858 - accuracy: 0.8172 - val_loss: 0.5200 - val_accuracy: 0.7769\nEpoch 7/400\n\nEpoch 00007: accuracy improved from 0.81715 to 0.83350, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4716 - accuracy: 0.8335 - val_loss: 0.4776 - val_accuracy: 0.8333\nEpoch 8/400\n\nEpoch 00008: accuracy improved from 0.83350 to 0.84683, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4593 - accuracy: 0.8468 - val_loss: 0.5180 - val_accuracy: 0.7854\nEpoch 9/400\n\nEpoch 00009: accuracy improved from 0.84683 to 0.84962, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4571 - accuracy: 0.8496 - val_loss: 0.5014 - val_accuracy: 0.8065\nEpoch 10/400\n\nEpoch 00010: accuracy improved from 0.84962 to 0.85869, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4475 - accuracy: 0.8587 - val_loss: 0.4184 - val_accuracy: 0.8880\nEpoch 11/400\n\nEpoch 00011: accuracy improved from 0.85869 to 0.86564, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4432 - accuracy: 0.8656 - val_loss: 0.4472 - val_accuracy: 0.8562\nEpoch 12/400\n\nEpoch 00012: accuracy improved from 0.86564 to 0.87146, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4360 - accuracy: 0.8715 - val_loss: 0.4557 - val_accuracy: 0.8522\nEpoch 13/400\n\nEpoch 00013: accuracy improved from 0.87146 to 0.88142, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4284 - accuracy: 0.8814 - val_loss: 0.4069 - val_accuracy: 0.9010\nEpoch 14/400\n\nEpoch 00014: accuracy improved from 0.88142 to 0.88534, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4239 - accuracy: 0.8853 - val_loss: 0.4216 - val_accuracy: 0.8880\nEpoch 15/400\n\nEpoch 00015: accuracy improved from 0.88534 to 0.88837, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4207 - accuracy: 0.8884 - val_loss: 0.3896 - val_accuracy: 0.9220\nEpoch 16/400\n\nEpoch 00016: accuracy improved from 0.88837 to 0.89049, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4174 - accuracy: 0.8905 - val_loss: 0.4190 - val_accuracy: 0.8907\nEpoch 17/400\n\nEpoch 00017: accuracy improved from 0.89049 to 0.89161, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4163 - accuracy: 0.8916 - val_loss: 0.4234 - val_accuracy: 0.8866\nEpoch 18/400\n\nEpoch 00018: accuracy improved from 0.89161 to 0.89777, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4113 - accuracy: 0.8978 - val_loss: 0.4294 - val_accuracy: 0.8795\nEpoch 19/400\n\nEpoch 00019: accuracy improved from 0.89777 to 0.89912, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.4117 - accuracy: 0.8991 - val_loss: 0.3989 - val_accuracy: 0.9126\nEpoch 20/400\n\nEpoch 00020: accuracy improved from 0.89912 to 0.90046, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4084 - accuracy: 0.9005 - val_loss: 0.3994 - val_accuracy: 0.9113\nEpoch 21/400\n\nEpoch 00021: accuracy improved from 0.90046 to 0.90718, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4033 - accuracy: 0.9072 - val_loss: 0.4081 - val_accuracy: 0.9041\nEpoch 22/400\n\nEpoch 00022: accuracy did not improve from 0.90718\n280/280 - 33s - loss: 0.4061 - accuracy: 0.9027 - val_loss: 0.3908 - val_accuracy: 0.9189\nEpoch 23/400\n\nEpoch 00023: accuracy improved from 0.90718 to 0.90964, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3999 - accuracy: 0.9096 - val_loss: 0.4318 - val_accuracy: 0.8741\nEpoch 24/400\n\nEpoch 00024: accuracy did not improve from 0.90964\n280/280 - 33s - loss: 0.4021 - accuracy: 0.9071 - val_loss: 0.4006 - val_accuracy: 0.9059\nEpoch 25/400\n\nEpoch 00025: accuracy improved from 0.90964 to 0.91345, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3970 - accuracy: 0.9134 - val_loss: 0.4022 - val_accuracy: 0.9091\nEpoch 26/400\n\nEpoch 00026: accuracy did not improve from 0.91345\n280/280 - 33s - loss: 0.3966 - accuracy: 0.9131 - val_loss: 0.3971 - val_accuracy: 0.9131\nEpoch 27/400\n\nEpoch 00027: accuracy improved from 0.91345 to 0.91457, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3953 - accuracy: 0.9146 - val_loss: 0.3878 - val_accuracy: 0.9234\nEpoch 28/400\n\nEpoch 00028: accuracy improved from 0.91457 to 0.91636, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3937 - accuracy: 0.9164 - val_loss: 0.4010 - val_accuracy: 0.9068\nEpoch 29/400\n\nEpoch 00029: accuracy improved from 0.91636 to 0.92420, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3871 - accuracy: 0.9242 - val_loss: 0.3875 - val_accuracy: 0.9234\nEpoch 30/400\n\nEpoch 00030: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3915 - accuracy: 0.9188 - val_loss: 0.3759 - val_accuracy: 0.9332\nEpoch 31/400\n\nEpoch 00031: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3896 - accuracy: 0.9197 - val_loss: 0.3822 - val_accuracy: 0.9279\nEpoch 32/400\n\nEpoch 00032: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3877 - accuracy: 0.9227 - val_loss: 0.3653 - val_accuracy: 0.9467\nEpoch 33/400\n\nEpoch 00033: accuracy improved from 0.92420 to 0.92498, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3858 - accuracy: 0.9250 - val_loss: 0.3784 - val_accuracy: 0.9283\nEpoch 34/400\n\nEpoch 00034: accuracy improved from 0.92498 to 0.92644, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3844 - accuracy: 0.9264 - val_loss: 0.3779 - val_accuracy: 0.9310\nEpoch 35/400\n\nEpoch 00035: accuracy improved from 0.92644 to 0.92722, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3831 - accuracy: 0.9272 - val_loss: 0.3823 - val_accuracy: 0.9265\nEpoch 36/400\n\nEpoch 00036: accuracy did not improve from 0.92722\n280/280 - 32s - loss: 0.3830 - accuracy: 0.9270 - val_loss: 0.4163 - val_accuracy: 0.8920\nEpoch 37/400\n\nEpoch 00037: accuracy improved from 0.92722 to 0.93248, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3790 - accuracy: 0.9325 - val_loss: 0.3622 - val_accuracy: 0.9498\nEpoch 38/400\n\nEpoch 00038: accuracy did not improve from 0.93248\n280/280 - 33s - loss: 0.3803 - accuracy: 0.9291 - val_loss: 0.3803 - val_accuracy: 0.9301\nEpoch 39/400\n\nEpoch 00039: accuracy did not improve from 0.93248\n280/280 - 33s - loss: 0.3789 - accuracy: 0.9321 - val_loss: 0.3649 - val_accuracy: 0.9480\nEpoch 40/400\n\nEpoch 00040: accuracy improved from 0.93248 to 0.93920, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3737 - accuracy: 0.9392 - val_loss: 0.3811 - val_accuracy: 0.9288\nEpoch 41/400\n\nEpoch 00041: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3762 - accuracy: 0.9342 - val_loss: 0.3706 - val_accuracy: 0.9404\nEpoch 42/400\n\nEpoch 00042: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3745 - accuracy: 0.9352 - val_loss: 0.3991 - val_accuracy: 0.9122\nEpoch 43/400\n\nEpoch 00043: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3734 - accuracy: 0.9379 - val_loss: 0.3660 - val_accuracy: 0.9467\nEpoch 44/400\n\nEpoch 00044: accuracy improved from 0.93920 to 0.94099, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3712 - accuracy: 0.9410 - val_loss: 0.3646 - val_accuracy: 0.9467\nEpoch 45/400\n\nEpoch 00045: accuracy improved from 0.94099 to 0.94290, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3697 - accuracy: 0.9429 - val_loss: 0.3581 - val_accuracy: 0.9530\nEpoch 46/400\n\nEpoch 00046: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3709 - accuracy: 0.9408 - val_loss: 0.3897 - val_accuracy: 0.9207\nEpoch 47/400\n\nEpoch 00047: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3739 - accuracy: 0.9372 - val_loss: 0.3673 - val_accuracy: 0.9453\nEpoch 48/400\n\nEpoch 00048: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3735 - accuracy: 0.9381 - val_loss: 0.3677 - val_accuracy: 0.9431\nEpoch 49/400\n\nEpoch 00049: accuracy improved from 0.94290 to 0.94334, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3677 - accuracy: 0.9433 - val_loss: 0.3616 - val_accuracy: 0.9494\nEpoch 50/400\n\nEpoch 00050: accuracy did not improve from 0.94334\n280/280 - 32s - loss: 0.3696 - accuracy: 0.9413 - val_loss: 0.3577 - val_accuracy: 0.9543\nEpoch 51/400\n\nEpoch 00051: accuracy improved from 0.94334 to 0.94413, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3681 - accuracy: 0.9441 - val_loss: 0.3612 - val_accuracy: 0.9489\nEpoch 52/400\n\nEpoch 00052: accuracy did not improve from 0.94413\n280/280 - 32s - loss: 0.3680 - accuracy: 0.9427 - val_loss: 0.3585 - val_accuracy: 0.9530\nEpoch 53/400\n\nEpoch 00053: accuracy did not improve from 0.94413\n280/280 - 34s - loss: 0.3674 - accuracy: 0.9430 - val_loss: 0.3685 - val_accuracy: 0.9476\nEpoch 54/400\n\nEpoch 00054: accuracy improved from 0.94413 to 0.94793, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 68s - loss: 0.3638 - accuracy: 0.9479 - val_loss: 0.3711 - val_accuracy: 0.9395\nEpoch 55/400\n\nEpoch 00055: accuracy did not improve from 0.94793\n280/280 - 33s - loss: 0.3677 - accuracy: 0.9438 - val_loss: 0.3689 - val_accuracy: 0.9395\nEpoch 56/400\n\nEpoch 00056: accuracy did not improve from 0.94793\n280/280 - 32s - loss: 0.3656 - accuracy: 0.9449 - val_loss: 0.3603 - val_accuracy: 0.9525\nEpoch 57/400\n\nEpoch 00057: accuracy did not improve from 0.94793\n280/280 - 32s - loss: 0.3643 - accuracy: 0.9461 - val_loss: 0.3550 - val_accuracy: 0.9570\nEpoch 58/400\n\nEpoch 00058: accuracy improved from 0.94793 to 0.94816, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3640 - accuracy: 0.9482 - val_loss: 0.3808 - val_accuracy: 0.9292\nEpoch 59/400\n\nEpoch 00059: accuracy did not improve from 0.94816\n280/280 - 32s - loss: 0.3639 - accuracy: 0.9476 - val_loss: 0.3589 - val_accuracy: 0.9534\nEpoch 60/400\n\nEpoch 00060: accuracy improved from 0.94816 to 0.95331, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3600 - accuracy: 0.9533 - val_loss: 0.3781 - val_accuracy: 0.9323\nEpoch 61/400\n\nEpoch 00061: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3597 - accuracy: 0.9524 - val_loss: 0.3594 - val_accuracy: 0.9525\nEpoch 62/400\n\nEpoch 00062: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3622 - accuracy: 0.9505 - val_loss: 0.3571 - val_accuracy: 0.9543\nEpoch 63/400\n\nEpoch 00063: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3606 - accuracy: 0.9513 - val_loss: 0.3578 - val_accuracy: 0.9525\nEpoch 64/400\n\nEpoch 00064: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3596 - accuracy: 0.9513 - val_loss: 0.3539 - val_accuracy: 0.9583\nEpoch 65/400\n\nEpoch 00065: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3592 - accuracy: 0.9532 - val_loss: 0.3520 - val_accuracy: 0.9601\nEpoch 66/400\n\nEpoch 00066: accuracy improved from 0.95331 to 0.95488, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3573 - accuracy: 0.9549 - val_loss: 0.3589 - val_accuracy: 0.9516\nEpoch 67/400\n\nEpoch 00067: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3588 - accuracy: 0.9534 - val_loss: 0.3643 - val_accuracy: 0.9462\nEpoch 68/400\n\nEpoch 00068: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3577 - accuracy: 0.9542 - val_loss: 0.3544 - val_accuracy: 0.9583\nEpoch 69/400\n\nEpoch 00069: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3581 - accuracy: 0.9529 - val_loss: 0.3597 - val_accuracy: 0.9525\nEpoch 70/400\n\nEpoch 00070: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3579 - accuracy: 0.9542 - val_loss: 0.3645 - val_accuracy: 0.9471\nEpoch 71/400\n\nEpoch 00071: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3578 - accuracy: 0.9542 - val_loss: 0.3551 - val_accuracy: 0.9552\nEpoch 72/400\n\nEpoch 00072: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3596 - accuracy: 0.9519 - val_loss: 0.3557 - val_accuracy: 0.9565\nEpoch 73/400\n\nEpoch 00073: accuracy improved from 0.95488 to 0.95723, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3546 - accuracy: 0.9572 - val_loss: 0.3622 - val_accuracy: 0.9489\nEpoch 74/400\n\nEpoch 00074: accuracy did not improve from 0.95723\n280/280 - 33s - loss: 0.3551 - accuracy: 0.9567 - val_loss: 0.3611 - val_accuracy: 0.9512\nEpoch 75/400\n\nEpoch 00075: accuracy improved from 0.95723 to 0.95880, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 75s - loss: 0.3545 - accuracy: 0.9588 - val_loss: 0.3685 - val_accuracy: 0.9422\nEpoch 76/400\n\nEpoch 00076: accuracy did not improve from 0.95880\n280/280 - 32s - loss: 0.3547 - accuracy: 0.9581 - val_loss: 0.3458 - val_accuracy: 0.9668\nEpoch 77/400\n\nEpoch 00077: accuracy improved from 0.95880 to 0.96159, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3515 - accuracy: 0.9616 - val_loss: 0.3540 - val_accuracy: 0.9592\nEpoch 78/400\n\nEpoch 00078: accuracy did not improve from 0.96159\n280/280 - 32s - loss: 0.3514 - accuracy: 0.9609 - val_loss: 0.3539 - val_accuracy: 0.9583\nEpoch 79/400\n\nEpoch 00079: accuracy did not improve from 0.96159\n280/280 - 32s - loss: 0.3552 - accuracy: 0.9562 - val_loss: 0.3461 - val_accuracy: 0.9651\nEpoch 80/400\n\nEpoch 00080: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3536 - accuracy: 0.9594 - val_loss: 0.3541 - val_accuracy: 0.9588\nEpoch 81/400\n\nEpoch 00081: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3516 - accuracy: 0.9599 - val_loss: 0.3624 - val_accuracy: 0.9494\nEpoch 82/400\n\nEpoch 00082: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3521 - accuracy: 0.9604 - val_loss: 0.3467 - val_accuracy: 0.9664\nEpoch 83/400\n\nEpoch 00083: accuracy improved from 0.96159 to 0.96361, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3495 - accuracy: 0.9636 - val_loss: 0.3912 - val_accuracy: 0.9220\nEpoch 84/400\n\nEpoch 00084: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3519 - accuracy: 0.9604 - val_loss: 0.3521 - val_accuracy: 0.9588\nEpoch 85/400\n\nEpoch 00085: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3524 - accuracy: 0.9592 - val_loss: 0.3542 - val_accuracy: 0.9556\nEpoch 86/400\n\nEpoch 00086: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3517 - accuracy: 0.9608 - val_loss: 0.3517 - val_accuracy: 0.9624\nEpoch 87/400\n\nEpoch 00087: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3490 - accuracy: 0.9633 - val_loss: 0.3448 - val_accuracy: 0.9668\nEpoch 88/400\n\nEpoch 00088: accuracy improved from 0.96361 to 0.96619, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3466 - accuracy: 0.9662 - val_loss: 0.3453 - val_accuracy: 0.9695\nEpoch 89/400\n\nEpoch 00089: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3491 - accuracy: 0.9632 - val_loss: 0.3660 - val_accuracy: 0.9467\nEpoch 90/400\n\nEpoch 00090: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3490 - accuracy: 0.9644 - val_loss: 0.3510 - val_accuracy: 0.9615\nEpoch 91/400\n\nEpoch 00091: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3482 - accuracy: 0.9651 - val_loss: 0.3586 - val_accuracy: 0.9534\nEpoch 92/400\n\nEpoch 00092: accuracy improved from 0.96619 to 0.96652, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3466 - accuracy: 0.9665 - val_loss: 0.3505 - val_accuracy: 0.9624\nEpoch 93/400\n\nEpoch 00093: accuracy improved from 0.96652 to 0.96798, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3450 - accuracy: 0.9680 - val_loss: 0.3562 - val_accuracy: 0.9570\nEpoch 94/400\n\nEpoch 00094: accuracy did not improve from 0.96798\n280/280 - 33s - loss: 0.3460 - accuracy: 0.9673 - val_loss: 0.3558 - val_accuracy: 0.9561\nEpoch 95/400\n\nEpoch 00095: accuracy improved from 0.96798 to 0.96809, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3448 - accuracy: 0.9681 - val_loss: 0.3423 - val_accuracy: 0.9700\nEpoch 96/400\n\nEpoch 00096: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3473 - accuracy: 0.9652 - val_loss: 0.3450 - val_accuracy: 0.9642\nEpoch 97/400\n\nEpoch 00097: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3464 - accuracy: 0.9666 - val_loss: 0.3544 - val_accuracy: 0.9588\nEpoch 98/400\n\nEpoch 00098: accuracy did not improve from 0.96809\n280/280 - 32s - loss: 0.3451 - accuracy: 0.9676 - val_loss: 0.3450 - val_accuracy: 0.9673\nEpoch 99/400\n\nEpoch 00099: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3462 - accuracy: 0.9665 - val_loss: 0.3527 - val_accuracy: 0.9597\nEpoch 100/400\n\nEpoch 00100: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3444 - accuracy: 0.9681 - val_loss: 0.3494 - val_accuracy: 0.9610\nEpoch 101/400\n\nEpoch 00101: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3453 - accuracy: 0.9671 - val_loss: 0.3519 - val_accuracy: 0.9579\nEpoch 102/400\n\nEpoch 00102: accuracy improved from 0.96809 to 0.96910, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3440 - accuracy: 0.9691 - val_loss: 0.3429 - val_accuracy: 0.9691\nEpoch 103/400\n\nEpoch 00103: accuracy improved from 0.96910 to 0.96966, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3424 - accuracy: 0.9697 - val_loss: 0.3591 - val_accuracy: 0.9525\nEpoch 104/400\n\nEpoch 00104: accuracy did not improve from 0.96966\n280/280 - 33s - loss: 0.3467 - accuracy: 0.9658 - val_loss: 0.3579 - val_accuracy: 0.9556\nEpoch 105/400\n\nEpoch 00105: accuracy did not improve from 0.96966\n280/280 - 33s - loss: 0.3440 - accuracy: 0.9691 - val_loss: 0.3496 - val_accuracy: 0.9606\nEpoch 106/400\n\nEpoch 00106: accuracy improved from 0.96966 to 0.97167, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3411 - accuracy: 0.9717 - val_loss: 0.3700 - val_accuracy: 0.9373\nEpoch 107/400\n\nEpoch 00107: accuracy did not improve from 0.97167\n280/280 - 32s - loss: 0.3427 - accuracy: 0.9700 - val_loss: 0.3550 - val_accuracy: 0.9561\nEpoch 108/400\n\nEpoch 00108: accuracy did not improve from 0.97167\n280/280 - 32s - loss: 0.3418 - accuracy: 0.9714 - val_loss: 0.3428 - val_accuracy: 0.9704\nEpoch 109/400\n\nEpoch 00109: accuracy did not improve from 0.97167\n280/280 - 33s - loss: 0.3422 - accuracy: 0.9703 - val_loss: 0.3507 - val_accuracy: 0.9615\nEpoch 110/400\n\nEpoch 00110: accuracy improved from 0.97167 to 0.97290, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3411 - accuracy: 0.9729 - val_loss: 0.3512 - val_accuracy: 0.9615\nEpoch 111/400\n\nEpoch 00111: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3425 - accuracy: 0.9694 - val_loss: 0.3587 - val_accuracy: 0.9525\nEpoch 112/400\n\nEpoch 00112: accuracy did not improve from 0.97290\n280/280 - 32s - loss: 0.3408 - accuracy: 0.9714 - val_loss: 0.3414 - val_accuracy: 0.9718\nEpoch 113/400\n\nEpoch 00113: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3405 - accuracy: 0.9725 - val_loss: 0.3572 - val_accuracy: 0.9556\nEpoch 114/400\n\nEpoch 00114: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3407 - accuracy: 0.9722 - val_loss: 0.3483 - val_accuracy: 0.9633\nEpoch 115/400\n\nEpoch 00115: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3408 - accuracy: 0.9718 - val_loss: 0.3434 - val_accuracy: 0.9686\nEpoch 116/400\n\nEpoch 00116: accuracy improved from 0.97290 to 0.97346, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3397 - accuracy: 0.9735 - val_loss: 0.3448 - val_accuracy: 0.9673\nEpoch 117/400\n\nEpoch 00117: accuracy did not improve from 0.97346\n280/280 - 32s - loss: 0.3396 - accuracy: 0.9725 - val_loss: 0.3702 - val_accuracy: 0.9413\nEpoch 118/400\n\nEpoch 00118: accuracy improved from 0.97346 to 0.97581, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3373 - accuracy: 0.9758 - val_loss: 0.3545 - val_accuracy: 0.9588\nEpoch 119/400\n\nEpoch 00119: accuracy did not improve from 0.97581\n280/280 - 34s - loss: 0.3393 - accuracy: 0.9732 - val_loss: 0.3462 - val_accuracy: 0.9659\nEpoch 120/400\n\nEpoch 00120: accuracy did not improve from 0.97581\n280/280 - 32s - loss: 0.3382 - accuracy: 0.9757 - val_loss: 0.3411 - val_accuracy: 0.9727\nEpoch 121/400\n\nEpoch 00121: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3376 - accuracy: 0.9741 - val_loss: 0.3445 - val_accuracy: 0.9664\nEpoch 122/400\n\nEpoch 00122: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3397 - accuracy: 0.9723 - val_loss: 0.3421 - val_accuracy: 0.9695\nEpoch 123/400\n\nEpoch 00123: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3396 - accuracy: 0.9736 - val_loss: 0.3474 - val_accuracy: 0.9659\nEpoch 124/400\n\nEpoch 00124: accuracy improved from 0.97581 to 0.97671, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 68s - loss: 0.3370 - accuracy: 0.9767 - val_loss: 0.3463 - val_accuracy: 0.9664\nEpoch 125/400\n\nEpoch 00125: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3394 - accuracy: 0.9730 - val_loss: 0.3455 - val_accuracy: 0.9664\nEpoch 126/400\n\nEpoch 00126: accuracy did not improve from 0.97671\n280/280 - 32s - loss: 0.3365 - accuracy: 0.9763 - val_loss: 0.3490 - val_accuracy: 0.9633\nEpoch 127/400\n\nEpoch 00127: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3383 - accuracy: 0.9735 - val_loss: 0.3469 - val_accuracy: 0.9642\nEpoch 128/400\n\nEpoch 00128: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3406 - accuracy: 0.9714 - val_loss: 0.3518 - val_accuracy: 0.9574\nEpoch 129/400\n\nEpoch 00129: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3395 - accuracy: 0.9731 - val_loss: 0.3589 - val_accuracy: 0.9539\nEpoch 130/400\n\nEpoch 00130: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3372 - accuracy: 0.9762 - val_loss: 0.3489 - val_accuracy: 0.9624\nEpoch 131/400\n\nEpoch 00131: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3385 - accuracy: 0.9745 - val_loss: 0.3481 - val_accuracy: 0.9659\nEpoch 132/400\n\nEpoch 00132: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3377 - accuracy: 0.9757 - val_loss: 0.3422 - val_accuracy: 0.9704\nEpoch 133/400\n\nEpoch 00133: accuracy did not improve from 0.97671\n280/280 - 32s - loss: 0.3371 - accuracy: 0.9760 - val_loss: 0.3430 - val_accuracy: 0.9686\nEpoch 134/400\n\nEpoch 00134: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3372 - accuracy: 0.9758 - val_loss: 0.3432 - val_accuracy: 0.9691\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:tensorflow:Using MirroredStrategy with devices (&#39;/job:localhost/replica:0/task:0/device:GPU:0&#39;, &#39;/job:localhost/replica:0/task:0/device:GPU:1&#39;)\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nEpoch 1/400\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\nINFO:tensorflow:batch_all_reduce: 218 all-reduces with algorithm = nccl, num_packs = 1\nINFO:tensorflow:batch_all_reduce: 218 all-reduces with algorithm = nccl, num_packs = 1\n\nEpoch 00001: accuracy improved from -inf to 0.54059, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 194s - loss: 0.7047 - accuracy: 0.5406 - val_loss: 0.6931 - val_accuracy: 0.5049\nEpoch 2/400\n\nEpoch 00002: accuracy improved from 0.54059 to 0.62591, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.6468 - accuracy: 0.6259 - val_loss: 0.6752 - val_accuracy: 0.5990\nEpoch 3/400\n\nEpoch 00003: accuracy improved from 0.62591 to 0.70731, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.5851 - accuracy: 0.7073 - val_loss: 0.6351 - val_accuracy: 0.6541\nEpoch 4/400\n\nEpoch 00004: accuracy improved from 0.70731 to 0.76095, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.5368 - accuracy: 0.7609 - val_loss: 0.5803 - val_accuracy: 0.7236\nEpoch 5/400\n\nEpoch 00005: accuracy improved from 0.76095 to 0.78793, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.5148 - accuracy: 0.7879 - val_loss: 0.5195 - val_accuracy: 0.7823\nEpoch 6/400\n\nEpoch 00006: accuracy improved from 0.78793 to 0.81715, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4858 - accuracy: 0.8172 - val_loss: 0.5200 - val_accuracy: 0.7769\nEpoch 7/400\n\nEpoch 00007: accuracy improved from 0.81715 to 0.83350, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4716 - accuracy: 0.8335 - val_loss: 0.4776 - val_accuracy: 0.8333\nEpoch 8/400\n\nEpoch 00008: accuracy improved from 0.83350 to 0.84683, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4593 - accuracy: 0.8468 - val_loss: 0.5180 - val_accuracy: 0.7854\nEpoch 9/400\n\nEpoch 00009: accuracy improved from 0.84683 to 0.84962, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4571 - accuracy: 0.8496 - val_loss: 0.5014 - val_accuracy: 0.8065\nEpoch 10/400\n\nEpoch 00010: accuracy improved from 0.84962 to 0.85869, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4475 - accuracy: 0.8587 - val_loss: 0.4184 - val_accuracy: 0.8880\nEpoch 11/400\n\nEpoch 00011: accuracy improved from 0.85869 to 0.86564, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4432 - accuracy: 0.8656 - val_loss: 0.4472 - val_accuracy: 0.8562\nEpoch 12/400\n\nEpoch 00012: accuracy improved from 0.86564 to 0.87146, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4360 - accuracy: 0.8715 - val_loss: 0.4557 - val_accuracy: 0.8522\nEpoch 13/400\n\nEpoch 00013: accuracy improved from 0.87146 to 0.88142, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4284 - accuracy: 0.8814 - val_loss: 0.4069 - val_accuracy: 0.9010\nEpoch 14/400\n\nEpoch 00014: accuracy improved from 0.88142 to 0.88534, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4239 - accuracy: 0.8853 - val_loss: 0.4216 - val_accuracy: 0.8880\nEpoch 15/400\n\nEpoch 00015: accuracy improved from 0.88534 to 0.88837, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4207 - accuracy: 0.8884 - val_loss: 0.3896 - val_accuracy: 0.9220\nEpoch 16/400\n\nEpoch 00016: accuracy improved from 0.88837 to 0.89049, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.4174 - accuracy: 0.8905 - val_loss: 0.4190 - val_accuracy: 0.8907\nEpoch 17/400\n\nEpoch 00017: accuracy improved from 0.89049 to 0.89161, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4163 - accuracy: 0.8916 - val_loss: 0.4234 - val_accuracy: 0.8866\nEpoch 18/400\n\nEpoch 00018: accuracy improved from 0.89161 to 0.89777, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4113 - accuracy: 0.8978 - val_loss: 0.4294 - val_accuracy: 0.8795\nEpoch 19/400\n\nEpoch 00019: accuracy improved from 0.89777 to 0.89912, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.4117 - accuracy: 0.8991 - val_loss: 0.3989 - val_accuracy: 0.9126\nEpoch 20/400\n\nEpoch 00020: accuracy improved from 0.89912 to 0.90046, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4084 - accuracy: 0.9005 - val_loss: 0.3994 - val_accuracy: 0.9113\nEpoch 21/400\n\nEpoch 00021: accuracy improved from 0.90046 to 0.90718, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.4033 - accuracy: 0.9072 - val_loss: 0.4081 - val_accuracy: 0.9041\nEpoch 22/400\n\nEpoch 00022: accuracy did not improve from 0.90718\n280/280 - 33s - loss: 0.4061 - accuracy: 0.9027 - val_loss: 0.3908 - val_accuracy: 0.9189\nEpoch 23/400\n\nEpoch 00023: accuracy improved from 0.90718 to 0.90964, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3999 - accuracy: 0.9096 - val_loss: 0.4318 - val_accuracy: 0.8741\nEpoch 24/400\n\nEpoch 00024: accuracy did not improve from 0.90964\n280/280 - 33s - loss: 0.4021 - accuracy: 0.9071 - val_loss: 0.4006 - val_accuracy: 0.9059\nEpoch 25/400\n\nEpoch 00025: accuracy improved from 0.90964 to 0.91345, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3970 - accuracy: 0.9134 - val_loss: 0.4022 - val_accuracy: 0.9091\nEpoch 26/400\n\nEpoch 00026: accuracy did not improve from 0.91345\n280/280 - 33s - loss: 0.3966 - accuracy: 0.9131 - val_loss: 0.3971 - val_accuracy: 0.9131\nEpoch 27/400\n\nEpoch 00027: accuracy improved from 0.91345 to 0.91457, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3953 - accuracy: 0.9146 - val_loss: 0.3878 - val_accuracy: 0.9234\nEpoch 28/400\n\nEpoch 00028: accuracy improved from 0.91457 to 0.91636, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3937 - accuracy: 0.9164 - val_loss: 0.4010 - val_accuracy: 0.9068\nEpoch 29/400\n\nEpoch 00029: accuracy improved from 0.91636 to 0.92420, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3871 - accuracy: 0.9242 - val_loss: 0.3875 - val_accuracy: 0.9234\nEpoch 30/400\n\nEpoch 00030: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3915 - accuracy: 0.9188 - val_loss: 0.3759 - val_accuracy: 0.9332\nEpoch 31/400\n\nEpoch 00031: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3896 - accuracy: 0.9197 - val_loss: 0.3822 - val_accuracy: 0.9279\nEpoch 32/400\n\nEpoch 00032: accuracy did not improve from 0.92420\n280/280 - 33s - loss: 0.3877 - accuracy: 0.9227 - val_loss: 0.3653 - val_accuracy: 0.9467\nEpoch 33/400\n\nEpoch 00033: accuracy improved from 0.92420 to 0.92498, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3858 - accuracy: 0.9250 - val_loss: 0.3784 - val_accuracy: 0.9283\nEpoch 34/400\n\nEpoch 00034: accuracy improved from 0.92498 to 0.92644, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3844 - accuracy: 0.9264 - val_loss: 0.3779 - val_accuracy: 0.9310\nEpoch 35/400\n\nEpoch 00035: accuracy improved from 0.92644 to 0.92722, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3831 - accuracy: 0.9272 - val_loss: 0.3823 - val_accuracy: 0.9265\nEpoch 36/400\n\nEpoch 00036: accuracy did not improve from 0.92722\n280/280 - 32s - loss: 0.3830 - accuracy: 0.9270 - val_loss: 0.4163 - val_accuracy: 0.8920\nEpoch 37/400\n\nEpoch 00037: accuracy improved from 0.92722 to 0.93248, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3790 - accuracy: 0.9325 - val_loss: 0.3622 - val_accuracy: 0.9498\nEpoch 38/400\n\nEpoch 00038: accuracy did not improve from 0.93248\n280/280 - 33s - loss: 0.3803 - accuracy: 0.9291 - val_loss: 0.3803 - val_accuracy: 0.9301\nEpoch 39/400\n\nEpoch 00039: accuracy did not improve from 0.93248\n280/280 - 33s - loss: 0.3789 - accuracy: 0.9321 - val_loss: 0.3649 - val_accuracy: 0.9480\nEpoch 40/400\n\nEpoch 00040: accuracy improved from 0.93248 to 0.93920, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3737 - accuracy: 0.9392 - val_loss: 0.3811 - val_accuracy: 0.9288\nEpoch 41/400\n\nEpoch 00041: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3762 - accuracy: 0.9342 - val_loss: 0.3706 - val_accuracy: 0.9404\nEpoch 42/400\n\nEpoch 00042: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3745 - accuracy: 0.9352 - val_loss: 0.3991 - val_accuracy: 0.9122\nEpoch 43/400\n\nEpoch 00043: accuracy did not improve from 0.93920\n280/280 - 33s - loss: 0.3734 - accuracy: 0.9379 - val_loss: 0.3660 - val_accuracy: 0.9467\nEpoch 44/400\n\nEpoch 00044: accuracy improved from 0.93920 to 0.94099, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3712 - accuracy: 0.9410 - val_loss: 0.3646 - val_accuracy: 0.9467\nEpoch 45/400\n\nEpoch 00045: accuracy improved from 0.94099 to 0.94290, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3697 - accuracy: 0.9429 - val_loss: 0.3581 - val_accuracy: 0.9530\nEpoch 46/400\n\nEpoch 00046: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3709 - accuracy: 0.9408 - val_loss: 0.3897 - val_accuracy: 0.9207\nEpoch 47/400\n\nEpoch 00047: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3739 - accuracy: 0.9372 - val_loss: 0.3673 - val_accuracy: 0.9453\nEpoch 48/400\n\nEpoch 00048: accuracy did not improve from 0.94290\n280/280 - 33s - loss: 0.3735 - accuracy: 0.9381 - val_loss: 0.3677 - val_accuracy: 0.9431\nEpoch 49/400\n\nEpoch 00049: accuracy improved from 0.94290 to 0.94334, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3677 - accuracy: 0.9433 - val_loss: 0.3616 - val_accuracy: 0.9494\nEpoch 50/400\n\nEpoch 00050: accuracy did not improve from 0.94334\n280/280 - 32s - loss: 0.3696 - accuracy: 0.9413 - val_loss: 0.3577 - val_accuracy: 0.9543\nEpoch 51/400\n\nEpoch 00051: accuracy improved from 0.94334 to 0.94413, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3681 - accuracy: 0.9441 - val_loss: 0.3612 - val_accuracy: 0.9489\nEpoch 52/400\n\nEpoch 00052: accuracy did not improve from 0.94413\n280/280 - 32s - loss: 0.3680 - accuracy: 0.9427 - val_loss: 0.3585 - val_accuracy: 0.9530\nEpoch 53/400\n\nEpoch 00053: accuracy did not improve from 0.94413\n280/280 - 34s - loss: 0.3674 - accuracy: 0.9430 - val_loss: 0.3685 - val_accuracy: 0.9476\nEpoch 54/400\n\nEpoch 00054: accuracy improved from 0.94413 to 0.94793, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 68s - loss: 0.3638 - accuracy: 0.9479 - val_loss: 0.3711 - val_accuracy: 0.9395\nEpoch 55/400\n\nEpoch 00055: accuracy did not improve from 0.94793\n280/280 - 33s - loss: 0.3677 - accuracy: 0.9438 - val_loss: 0.3689 - val_accuracy: 0.9395\nEpoch 56/400\n\nEpoch 00056: accuracy did not improve from 0.94793\n280/280 - 32s - loss: 0.3656 - accuracy: 0.9449 - val_loss: 0.3603 - val_accuracy: 0.9525\nEpoch 57/400\n\nEpoch 00057: accuracy did not improve from 0.94793\n280/280 - 32s - loss: 0.3643 - accuracy: 0.9461 - val_loss: 0.3550 - val_accuracy: 0.9570\nEpoch 58/400\n\nEpoch 00058: accuracy improved from 0.94793 to 0.94816, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3640 - accuracy: 0.9482 - val_loss: 0.3808 - val_accuracy: 0.9292\nEpoch 59/400\n\nEpoch 00059: accuracy did not improve from 0.94816\n280/280 - 32s - loss: 0.3639 - accuracy: 0.9476 - val_loss: 0.3589 - val_accuracy: 0.9534\nEpoch 60/400\n\nEpoch 00060: accuracy improved from 0.94816 to 0.95331, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3600 - accuracy: 0.9533 - val_loss: 0.3781 - val_accuracy: 0.9323\nEpoch 61/400\n\nEpoch 00061: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3597 - accuracy: 0.9524 - val_loss: 0.3594 - val_accuracy: 0.9525\nEpoch 62/400\n\nEpoch 00062: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3622 - accuracy: 0.9505 - val_loss: 0.3571 - val_accuracy: 0.9543\nEpoch 63/400\n\nEpoch 00063: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3606 - accuracy: 0.9513 - val_loss: 0.3578 - val_accuracy: 0.9525\nEpoch 64/400\n\nEpoch 00064: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3596 - accuracy: 0.9513 - val_loss: 0.3539 - val_accuracy: 0.9583\nEpoch 65/400\n\nEpoch 00065: accuracy did not improve from 0.95331\n280/280 - 32s - loss: 0.3592 - accuracy: 0.9532 - val_loss: 0.3520 - val_accuracy: 0.9601\nEpoch 66/400\n\nEpoch 00066: accuracy improved from 0.95331 to 0.95488, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3573 - accuracy: 0.9549 - val_loss: 0.3589 - val_accuracy: 0.9516\nEpoch 67/400\n\nEpoch 00067: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3588 - accuracy: 0.9534 - val_loss: 0.3643 - val_accuracy: 0.9462\nEpoch 68/400\n\nEpoch 00068: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3577 - accuracy: 0.9542 - val_loss: 0.3544 - val_accuracy: 0.9583\nEpoch 69/400\n\nEpoch 00069: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3581 - accuracy: 0.9529 - val_loss: 0.3597 - val_accuracy: 0.9525\nEpoch 70/400\n\nEpoch 00070: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3579 - accuracy: 0.9542 - val_loss: 0.3645 - val_accuracy: 0.9471\nEpoch 71/400\n\nEpoch 00071: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3578 - accuracy: 0.9542 - val_loss: 0.3551 - val_accuracy: 0.9552\nEpoch 72/400\n\nEpoch 00072: accuracy did not improve from 0.95488\n280/280 - 33s - loss: 0.3596 - accuracy: 0.9519 - val_loss: 0.3557 - val_accuracy: 0.9565\nEpoch 73/400\n\nEpoch 00073: accuracy improved from 0.95488 to 0.95723, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3546 - accuracy: 0.9572 - val_loss: 0.3622 - val_accuracy: 0.9489\nEpoch 74/400\n\nEpoch 00074: accuracy did not improve from 0.95723\n280/280 - 33s - loss: 0.3551 - accuracy: 0.9567 - val_loss: 0.3611 - val_accuracy: 0.9512\nEpoch 75/400\n\nEpoch 00075: accuracy improved from 0.95723 to 0.95880, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 75s - loss: 0.3545 - accuracy: 0.9588 - val_loss: 0.3685 - val_accuracy: 0.9422\nEpoch 76/400\n\nEpoch 00076: accuracy did not improve from 0.95880\n280/280 - 32s - loss: 0.3547 - accuracy: 0.9581 - val_loss: 0.3458 - val_accuracy: 0.9668\nEpoch 77/400\n\nEpoch 00077: accuracy improved from 0.95880 to 0.96159, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3515 - accuracy: 0.9616 - val_loss: 0.3540 - val_accuracy: 0.9592\nEpoch 78/400\n\nEpoch 00078: accuracy did not improve from 0.96159\n280/280 - 32s - loss: 0.3514 - accuracy: 0.9609 - val_loss: 0.3539 - val_accuracy: 0.9583\nEpoch 79/400\n\nEpoch 00079: accuracy did not improve from 0.96159\n280/280 - 32s - loss: 0.3552 - accuracy: 0.9562 - val_loss: 0.3461 - val_accuracy: 0.9651\nEpoch 80/400\n\nEpoch 00080: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3536 - accuracy: 0.9594 - val_loss: 0.3541 - val_accuracy: 0.9588\nEpoch 81/400\n\nEpoch 00081: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3516 - accuracy: 0.9599 - val_loss: 0.3624 - val_accuracy: 0.9494\nEpoch 82/400\n\nEpoch 00082: accuracy did not improve from 0.96159\n280/280 - 33s - loss: 0.3521 - accuracy: 0.9604 - val_loss: 0.3467 - val_accuracy: 0.9664\nEpoch 83/400\n\nEpoch 00083: accuracy improved from 0.96159 to 0.96361, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3495 - accuracy: 0.9636 - val_loss: 0.3912 - val_accuracy: 0.9220\nEpoch 84/400\n\nEpoch 00084: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3519 - accuracy: 0.9604 - val_loss: 0.3521 - val_accuracy: 0.9588\nEpoch 85/400\n\nEpoch 00085: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3524 - accuracy: 0.9592 - val_loss: 0.3542 - val_accuracy: 0.9556\nEpoch 86/400\n\nEpoch 00086: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3517 - accuracy: 0.9608 - val_loss: 0.3517 - val_accuracy: 0.9624\nEpoch 87/400\n\nEpoch 00087: accuracy did not improve from 0.96361\n280/280 - 33s - loss: 0.3490 - accuracy: 0.9633 - val_loss: 0.3448 - val_accuracy: 0.9668\nEpoch 88/400\n\nEpoch 00088: accuracy improved from 0.96361 to 0.96619, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3466 - accuracy: 0.9662 - val_loss: 0.3453 - val_accuracy: 0.9695\nEpoch 89/400\n\nEpoch 00089: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3491 - accuracy: 0.9632 - val_loss: 0.3660 - val_accuracy: 0.9467\nEpoch 90/400\n\nEpoch 00090: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3490 - accuracy: 0.9644 - val_loss: 0.3510 - val_accuracy: 0.9615\nEpoch 91/400\n\nEpoch 00091: accuracy did not improve from 0.96619\n280/280 - 33s - loss: 0.3482 - accuracy: 0.9651 - val_loss: 0.3586 - val_accuracy: 0.9534\nEpoch 92/400\n\nEpoch 00092: accuracy improved from 0.96619 to 0.96652, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3466 - accuracy: 0.9665 - val_loss: 0.3505 - val_accuracy: 0.9624\nEpoch 93/400\n\nEpoch 00093: accuracy improved from 0.96652 to 0.96798, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3450 - accuracy: 0.9680 - val_loss: 0.3562 - val_accuracy: 0.9570\nEpoch 94/400\n\nEpoch 00094: accuracy did not improve from 0.96798\n280/280 - 33s - loss: 0.3460 - accuracy: 0.9673 - val_loss: 0.3558 - val_accuracy: 0.9561\nEpoch 95/400\n\nEpoch 00095: accuracy improved from 0.96798 to 0.96809, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3448 - accuracy: 0.9681 - val_loss: 0.3423 - val_accuracy: 0.9700\nEpoch 96/400\n\nEpoch 00096: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3473 - accuracy: 0.9652 - val_loss: 0.3450 - val_accuracy: 0.9642\nEpoch 97/400\n\nEpoch 00097: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3464 - accuracy: 0.9666 - val_loss: 0.3544 - val_accuracy: 0.9588\nEpoch 98/400\n\nEpoch 00098: accuracy did not improve from 0.96809\n280/280 - 32s - loss: 0.3451 - accuracy: 0.9676 - val_loss: 0.3450 - val_accuracy: 0.9673\nEpoch 99/400\n\nEpoch 00099: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3462 - accuracy: 0.9665 - val_loss: 0.3527 - val_accuracy: 0.9597\nEpoch 100/400\n\nEpoch 00100: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3444 - accuracy: 0.9681 - val_loss: 0.3494 - val_accuracy: 0.9610\nEpoch 101/400\n\nEpoch 00101: accuracy did not improve from 0.96809\n280/280 - 33s - loss: 0.3453 - accuracy: 0.9671 - val_loss: 0.3519 - val_accuracy: 0.9579\nEpoch 102/400\n\nEpoch 00102: accuracy improved from 0.96809 to 0.96910, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3440 - accuracy: 0.9691 - val_loss: 0.3429 - val_accuracy: 0.9691\nEpoch 103/400\n\nEpoch 00103: accuracy improved from 0.96910 to 0.96966, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 71s - loss: 0.3424 - accuracy: 0.9697 - val_loss: 0.3591 - val_accuracy: 0.9525\nEpoch 104/400\n\nEpoch 00104: accuracy did not improve from 0.96966\n280/280 - 33s - loss: 0.3467 - accuracy: 0.9658 - val_loss: 0.3579 - val_accuracy: 0.9556\nEpoch 105/400\n\nEpoch 00105: accuracy did not improve from 0.96966\n280/280 - 33s - loss: 0.3440 - accuracy: 0.9691 - val_loss: 0.3496 - val_accuracy: 0.9606\nEpoch 106/400\n\nEpoch 00106: accuracy improved from 0.96966 to 0.97167, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3411 - accuracy: 0.9717 - val_loss: 0.3700 - val_accuracy: 0.9373\nEpoch 107/400\n\nEpoch 00107: accuracy did not improve from 0.97167\n280/280 - 32s - loss: 0.3427 - accuracy: 0.9700 - val_loss: 0.3550 - val_accuracy: 0.9561\nEpoch 108/400\n\nEpoch 00108: accuracy did not improve from 0.97167\n280/280 - 32s - loss: 0.3418 - accuracy: 0.9714 - val_loss: 0.3428 - val_accuracy: 0.9704\nEpoch 109/400\n\nEpoch 00109: accuracy did not improve from 0.97167\n280/280 - 33s - loss: 0.3422 - accuracy: 0.9703 - val_loss: 0.3507 - val_accuracy: 0.9615\nEpoch 110/400\n\nEpoch 00110: accuracy improved from 0.97167 to 0.97290, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3411 - accuracy: 0.9729 - val_loss: 0.3512 - val_accuracy: 0.9615\nEpoch 111/400\n\nEpoch 00111: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3425 - accuracy: 0.9694 - val_loss: 0.3587 - val_accuracy: 0.9525\nEpoch 112/400\n\nEpoch 00112: accuracy did not improve from 0.97290\n280/280 - 32s - loss: 0.3408 - accuracy: 0.9714 - val_loss: 0.3414 - val_accuracy: 0.9718\nEpoch 113/400\n\nEpoch 00113: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3405 - accuracy: 0.9725 - val_loss: 0.3572 - val_accuracy: 0.9556\nEpoch 114/400\n\nEpoch 00114: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3407 - accuracy: 0.9722 - val_loss: 0.3483 - val_accuracy: 0.9633\nEpoch 115/400\n\nEpoch 00115: accuracy did not improve from 0.97290\n280/280 - 33s - loss: 0.3408 - accuracy: 0.9718 - val_loss: 0.3434 - val_accuracy: 0.9686\nEpoch 116/400\n\nEpoch 00116: accuracy improved from 0.97290 to 0.97346, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 70s - loss: 0.3397 - accuracy: 0.9735 - val_loss: 0.3448 - val_accuracy: 0.9673\nEpoch 117/400\n\nEpoch 00117: accuracy did not improve from 0.97346\n280/280 - 32s - loss: 0.3396 - accuracy: 0.9725 - val_loss: 0.3702 - val_accuracy: 0.9413\nEpoch 118/400\n\nEpoch 00118: accuracy improved from 0.97346 to 0.97581, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 69s - loss: 0.3373 - accuracy: 0.9758 - val_loss: 0.3545 - val_accuracy: 0.9588\nEpoch 119/400\n\nEpoch 00119: accuracy did not improve from 0.97581\n280/280 - 34s - loss: 0.3393 - accuracy: 0.9732 - val_loss: 0.3462 - val_accuracy: 0.9659\nEpoch 120/400\n\nEpoch 00120: accuracy did not improve from 0.97581\n280/280 - 32s - loss: 0.3382 - accuracy: 0.9757 - val_loss: 0.3411 - val_accuracy: 0.9727\nEpoch 121/400\n\nEpoch 00121: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3376 - accuracy: 0.9741 - val_loss: 0.3445 - val_accuracy: 0.9664\nEpoch 122/400\n\nEpoch 00122: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3397 - accuracy: 0.9723 - val_loss: 0.3421 - val_accuracy: 0.9695\nEpoch 123/400\n\nEpoch 00123: accuracy did not improve from 0.97581\n280/280 - 33s - loss: 0.3396 - accuracy: 0.9736 - val_loss: 0.3474 - val_accuracy: 0.9659\nEpoch 124/400\n\nEpoch 00124: accuracy improved from 0.97581 to 0.97671, saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py\nINFO:tensorflow:Assets written to: /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0331/Model/Best_Model_Large.h5py/assets\n280/280 - 68s - loss: 0.3370 - accuracy: 0.9767 - val_loss: 0.3463 - val_accuracy: 0.9664\nEpoch 125/400\n\nEpoch 00125: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3394 - accuracy: 0.9730 - val_loss: 0.3455 - val_accuracy: 0.9664\nEpoch 126/400\n\nEpoch 00126: accuracy did not improve from 0.97671\n280/280 - 32s - loss: 0.3365 - accuracy: 0.9763 - val_loss: 0.3490 - val_accuracy: 0.9633\nEpoch 127/400\n\nEpoch 00127: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3383 - accuracy: 0.9735 - val_loss: 0.3469 - val_accuracy: 0.9642\nEpoch 128/400\n\nEpoch 00128: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3406 - accuracy: 0.9714 - val_loss: 0.3518 - val_accuracy: 0.9574\nEpoch 129/400\n\nEpoch 00129: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3395 - accuracy: 0.9731 - val_loss: 0.3589 - val_accuracy: 0.9539\nEpoch 130/400\n\nEpoch 00130: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3372 - accuracy: 0.9762 - val_loss: 0.3489 - val_accuracy: 0.9624\nEpoch 131/400\n\nEpoch 00131: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3385 - accuracy: 0.9745 - val_loss: 0.3481 - val_accuracy: 0.9659\nEpoch 132/400\n\nEpoch 00132: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3377 - accuracy: 0.9757 - val_loss: 0.3422 - val_accuracy: 0.9704\nEpoch 133/400\n\nEpoch 00133: accuracy did not improve from 0.97671\n280/280 - 32s - loss: 0.3371 - accuracy: 0.9760 - val_loss: 0.3430 - val_accuracy: 0.9686\nEpoch 134/400\n\nEpoch 00134: accuracy did not improve from 0.97671\n280/280 - 33s - loss: 0.3372 - accuracy: 0.9758 - val_loss: 0.3432 - val_accuracy: 0.9691\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nacc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\n\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fc10fee-5b2a-4be1-a095-842b0547489a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/727d452b-8aa7-461c-8c74-87e17e329328.png","removedWidgets":[],"addedWidgets":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}}],"execution_count":0},{"cell_type":"code","source":["# !tar -cvf /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0325/Model/Best_Model.tar /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/0325/Model/Best_Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05fb8785-f608-41fd-941d-68fbf41626d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"MobileNet V3 Large","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2679691984810965}},"nbformat":4,"nbformat_minor":0}
