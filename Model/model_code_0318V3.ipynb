{"cells":[{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom PIL import Image\nimport os\nimport pathlib\nimport numpy as np\nimport pathlib\nimport shutil"],"metadata":{"ExecuteTime":{"end_time":"2021-03-17T04:20:11.325951Z","start_time":"2021-03-17T04:19:34.532762Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2aca74f-2f2d-4475-aee5-19061ce67bc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DATA_PATH = '/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Resized_Data'\n# train_dir = pathlib.Path(DATA_PATH+'/training')\n# val_dir = pathlib.Path(DATA_PATH+'/val')\n\n# train_dir = dbutils.fs.ls(DATA_PATH+\"/train/\")\n# val_dir = dbutils.fs.ls(DATA_PATH+\"/val/\")\nbatch_size = 128\nimg_height = 120\nimg_width = 160\n\nresized_height = 120\nresized_width = 160\n\n# IMG_SHAPE = (224, 224, 3)\nCLASS_PEOPLE = \"/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/training/people_class\"\nnum_classes = 2\nNUM_EPOCHS = 120\nseed = 12\nlog_dir = \"/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/logs\"\n\nSTEPS_PER_EPOCH = 1508 //batch_size"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9656def-8fa9-4254-ae11-5c925b7b14ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\ndata_augmentation = tf.keras.Sequential([\n#   layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n  layers.experimental.preprocessing.RandomRotation(0.2),\n  layers.experimental.preprocessing.RandomZoom(0.2),\n  layers.experimental.preprocessing.RandomContrast(0.2),\n#   layers.experimental.preprocessing.RandomContrast(0.2),\n])\n\nresize = tf.keras.layers.experimental.preprocessing.Resizing(resized_height, resized_width)\n\ndef preprocess_input(image):\n    return (image/127.5) - 1\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  DATA_PATH,\n  seed=seed,\n  shuffle=True,\n  validation_split=0.2,\n  subset='training',\n  image_size=(img_height, img_width)\n)\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\ntrain_ds = train_ds.map(\n  lambda x, y: (data_augmentation(x, training=True), y))\n\ntrain_ds = train_ds.map(\n  lambda x, y: (preprocess_input(x), y))\n\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  DATA_PATH,\n  seed=seed,\n  shuffle=True,\n  validation_split=0.2,\n  subset='validation',\n  image_size=(img_height, img_width)\n)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.map(\n  lambda x, y: (preprocess_input(x), y))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99ce880e-5476-4f8e-b471-653a76a034d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Found 7910 files belonging to 2 classes.\nUsing 6328 files for training.\nFound 7910 files belonging to 2 classes.\nUsing 1582 files for validation.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Found 7910 files belonging to 2 classes.\nUsing 6328 files for training.\nFound 7910 files belonging to 2 classes.\nUsing 1582 files for validation.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["list(val_ds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47348a36-05c9-47bb-8b16-0e94968eaa22"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [(&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[ 0.10588241,  0.27843142,  0.11372554],\n           [ 0.12156868,  0.2941177 ,  0.12941182],\n           [ 0.12941182,  0.30196083,  0.13725495],\n           ...,\n           [-0.54509807, -0.41176468, -0.56078434],\n           [-0.5372549 , -0.42745095, -0.62352943],\n           [-0.54509807, -0.44313723, -0.64705884]],\n  \n          [[ 0.09019613,  0.26274514,  0.09803927],\n           [ 0.10588241,  0.27843142,  0.11372554],\n           [ 0.12941182,  0.30196083,  0.13725495],\n           ...,\n           [-0.5294118 , -0.38823527, -0.56078434],\n           [-0.5137255 , -0.41176468, -0.6156863 ],\n           [-0.5294118 , -0.42745095, -0.64705884]],\n  \n          [[ 0.06666672,  0.23921573,  0.07450986],\n           [ 0.09019613,  0.26274514,  0.09803927],\n           [ 0.12156868,  0.2941177 ,  0.12941182],\n           ...,\n           [-0.4980392 , -0.372549  , -0.5764706 ],\n           [-0.49019605, -0.41176468, -0.62352943],\n           [-0.5058824 , -0.41960782, -0.654902  ]],\n  \n          ...,\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5058824 , -0.3960784 , -0.5764706 ],\n           [-0.5058824 , -0.3960784 , -0.5764706 ],\n           [-0.5058824 , -0.3960784 , -0.5764706 ]],\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5058824 , -0.3960784 , -0.5764706 ]],\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375]]],\n  \n  \n         [[[-0.44313723, -0.31764704, -0.40392154],\n           [-0.44313723, -0.31764704, -0.40392154],\n           [-0.4352941 , -0.3098039 , -0.3960784 ],\n           ...,\n           [-0.372549  , -0.26274508, -0.36470586],\n           [-0.372549  , -0.26274508, -0.36470586],\n           [-0.36470586, -0.25490195, -0.35686272]],\n  \n          [[-0.45098037, -0.32549018, -0.41176468],\n           [-0.45098037, -0.32549018, -0.41176468],\n           [-0.44313723, -0.31764704, -0.40392154],\n           ...,\n           [-0.38039213, -0.27058822, -0.372549  ],\n           [-0.3960784 , -0.2862745 , -0.38823527],\n           [-0.3960784 , -0.2862745 , -0.38823527]],\n  \n          [[-0.46666664, -0.34117645, -0.42745095],\n           [-0.4588235 , -0.3333333 , -0.41960782],\n           [-0.45098037, -0.32549018, -0.41176468],\n           ...,\n           [-0.3960784 , -0.2862745 , -0.38823527],\n           [-0.41960782, -0.3098039 , -0.41176468],\n           [-0.42745095, -0.31764704, -0.41960782]],\n  \n          ...,\n  \n          [[-0.6392157 , -0.4980392 , -0.60784316],\n           [-0.62352943, -0.4823529 , -0.5921569 ],\n           [-0.6       , -0.47450978, -0.5764706 ],\n           ...,\n           [-0.5137255 , -0.42745095, -0.47450978],\n           [-0.52156866, -0.45098037, -0.49019605],\n           [-0.5372549 , -0.45098037, -0.4980392 ]],\n  \n          [[-0.6313726 , -0.49019605, -0.6       ],\n           [-0.62352943, -0.4823529 , -0.5921569 ],\n           [-0.5921569 , -0.46666664, -0.5686275 ],\n           ...,\n           [-0.4352941 , -0.36470586, -0.40392154],\n           [-0.4352941 , -0.38823527, -0.41960782],\n           [-0.4588235 , -0.38823527, -0.42745095]],\n  \n          [[-0.6313726 , -0.49019605, -0.6       ],\n           [-0.6156863 , -0.47450978, -0.58431375],\n           [-0.5921569 , -0.46666664, -0.5686275 ],\n           ...,\n           [-0.35686272, -0.3098039 , -0.34117645],\n           [-0.35686272, -0.3098039 , -0.34117645],\n           [-0.35686272, -0.3098039 , -0.34117645]]],\n  \n  \n         [[[-0.75686276, -0.5764706 , -0.6392157 ],\n           [-0.7647059 , -0.58431375, -0.64705884],\n           [-0.77254903, -0.5921569 , -0.654902  ],\n           ...,\n           [-0.9372549 , -0.8117647 , -0.8980392 ],\n           [-0.94509804, -0.81960785, -0.90588236],\n           [-0.94509804, -0.81960785, -0.90588236]],\n  \n          [[-0.73333335, -0.5529412 , -0.6156863 ],\n           [-0.7411765 , -0.56078434, -0.62352943],\n           [-0.7490196 , -0.5686275 , -0.6313726 ],\n           ...,\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.92941177, -0.8039216 , -0.8901961 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]],\n  \n          [[-0.69411767, -0.5137255 , -0.5764706 ],\n           [-0.70980394, -0.5294118 , -0.5921569 ],\n           [-0.7254902 , -0.54509807, -0.60784316],\n           ...,\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.92941177, -0.8039216 , -0.8901961 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]],\n  \n          ...,\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           ...,\n           [-0.8980392 , -0.79607844, -0.8745098 ],\n           [-0.94509804, -0.81960785, -0.90588236],\n           [-0.96862745, -0.84313726, -0.92941177]],\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.92156863, -0.81960785, -0.8745098 ],\n           ...,\n           [-0.8745098 , -0.77254903, -0.8509804 ],\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.94509804, -0.81960785, -0.90588236]],\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.92156863, -0.81960785, -0.8745098 ],\n           ...,\n           [-0.85882354, -0.75686276, -0.8352941 ],\n           [-0.90588236, -0.78039217, -0.8666667 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.32549018, -0.25490195, -0.38823527],\n           [-0.32549018, -0.25490195, -0.38823527],\n           [-0.31764704, -0.24705881, -0.38039213],\n           ...,\n           [-0.4352941 , -0.372549  , -0.5372549 ],\n           [-0.44313723, -0.38039213, -0.54509807],\n           [-0.44313723, -0.38039213, -0.54509807]],\n  \n          [[-0.3490196 , -0.27843136, -0.41176468],\n           [-0.3490196 , -0.27843136, -0.41176468],\n           [-0.34117645, -0.27058822, -0.40392154],\n           ...,\n           [-0.44313723, -0.38039213, -0.54509807],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ]],\n  \n          [[-0.372549  , -0.30196077, -0.4352941 ],\n           [-0.372549  , -0.30196077, -0.4352941 ],\n           [-0.36470586, -0.29411763, -0.42745095],\n           ...,\n           [-0.4588235 , -0.3960784 , -0.56078434],\n           [-0.4588235 , -0.3960784 , -0.56078434],\n           [-0.46666664, -0.40392154, -0.5686275 ]],\n  \n          ...,\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]],\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]],\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]]],\n  \n  \n         [[[-0.05882353,  0.05098045, -0.06666666],\n           [-0.12941176, -0.01960784, -0.1372549 ],\n           [-0.20784312, -0.11372548, -0.2235294 ],\n           ...,\n           [ 0.30980396,  0.47450984,  0.35686278],\n           [ 0.30980396,  0.47450984,  0.35686278],\n           [ 0.30196083,  0.4666667 ,  0.34901965]],\n  \n          [[-0.2235294 , -0.11372548, -0.23137254],\n           [-0.25490195, -0.14509803, -0.26274508],\n           [-0.29411763, -0.19999999, -0.3098039 ],\n           ...,\n           [ 0.2941177 ,  0.45882356,  0.3411765 ],\n           [ 0.28627455,  0.45098042,  0.33333337],\n           [ 0.28627455,  0.45098042,  0.33333337]],\n  \n          [[-0.35686272, -0.24705881, -0.36470586],\n           [-0.36470586, -0.25490195, -0.372549  ],\n           [-0.35686272, -0.26274508, -0.372549  ],\n           ...,\n           [ 0.27843142,  0.4431373 ,  0.32549024],\n           [ 0.27058828,  0.43529415,  0.3176471 ],\n           [ 0.26274514,  0.427451  ,  0.30980396]],\n  \n          ...,\n  \n          [[ 0.6627451 ,  0.8039216 ,  0.70980394],\n           [ 0.6784314 ,  0.81960785,  0.7254902 ],\n           [ 0.7019608 ,  0.827451  ,  0.7411765 ],\n           ...,\n           [ 0.7019608 ,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825]],\n  \n          [[ 0.6784314 ,  0.81960785,  0.7254902 ],\n           [ 0.6862745 ,  0.827451  ,  0.73333335],\n           [ 0.7019608 ,  0.827451  ,  0.7411765 ],\n           ...,\n           [ 0.7019608 ,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825]],\n  \n          [[ 0.6862745 ,  0.827451  ,  0.73333335],\n           [ 0.69411767,  0.8352941 ,  0.7411765 ],\n           [ 0.69411767,  0.81960785,  0.73333335],\n           ...,\n           [ 0.69411767,  0.8352941 ,  0.6627451 ],\n           [ 0.6862745 ,  0.8352941 ,  0.6627451 ],\n           [ 0.6862745 ,  0.8352941 ,  0.6627451 ]]],\n  \n  \n         [[[-0.46666664, -0.31764704, -0.36470586],\n           [-0.46666664, -0.31764704, -0.36470586],\n           [-0.4588235 , -0.3098039 , -0.35686272],\n           ...,\n           [-0.4352941 , -0.30196077, -0.38039213],\n           [-0.4352941 , -0.3098039 , -0.38823527],\n           [-0.4352941 , -0.3098039 , -0.38823527]],\n  \n          [[-0.4823529 , -0.3333333 , -0.38039213],\n           [-0.4823529 , -0.3333333 , -0.38039213],\n           [-0.47450978, -0.32549018, -0.372549  ],\n           ...,\n           [-0.4352941 , -0.30196077, -0.38039213],\n           [-0.4352941 , -0.3098039 , -0.38823527],\n           [-0.44313723, -0.31764704, -0.3960784 ]],\n  \n          [[-0.5137255 , -0.36470586, -0.41176468],\n           [-0.5137255 , -0.36470586, -0.41176468],\n           [-0.5058824 , -0.35686272, -0.40392154],\n           ...,\n           [-0.44313723, -0.3098039 , -0.38823527],\n           [-0.4588235 , -0.32549018, -0.40392154],\n           [-0.46666664, -0.3333333 , -0.41176468]],\n  \n          ...,\n  \n          [[-0.8352941 , -0.56078434, -0.6156863 ],\n           [-0.8352941 , -0.56078434, -0.6156863 ],\n           [-0.8352941 , -0.56078434, -0.6156863 ],\n           ...,\n           [-0.5686275 , -0.4588235 , -0.45098037],\n           [-0.5529412 , -0.46666664, -0.4352941 ],\n           [-0.5529412 , -0.46666664, -0.4352941 ]],\n  \n          [[-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.81960785, -0.56078434, -0.6313726 ],\n           ...,\n           [-0.4823529 , -0.3960784 , -0.38039213],\n           [-0.4823529 , -0.41176468, -0.372549  ],\n           [-0.49019605, -0.41960782, -0.38039213]],\n  \n          [[-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.81960785, -0.5764706 , -0.6392157 ],\n           ...,\n           [-0.40392154, -0.32549018, -0.3098039 ],\n           [-0.41176468, -0.35686272, -0.3098039 ],\n           [-0.41960782, -0.36470586, -0.31764704]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n         0, 0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.81960785, -0.6313726 , -0.75686276],\n           ...,\n           [-0.6       , -0.4588235 , -0.5529412 ],\n           [-0.58431375, -0.44313723, -0.5372549 ],\n           [-0.5686275 , -0.42745095, -0.52156866]],\n  \n          [[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           ...,\n           [-0.6       , -0.4588235 , -0.5529412 ],\n           [-0.58431375, -0.44313723, -0.5372549 ],\n           [-0.5764706 , -0.4352941 , -0.5294118 ]],\n  \n          [[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           ...,\n           [-0.62352943, -0.4823529 , -0.5764706 ],\n           [-0.60784316, -0.46666664, -0.56078434],\n           [-0.6       , -0.4588235 , -0.5529412 ]],\n  \n          ...,\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.81960785, -0.67058825, -0.827451  ],\n           [-0.7882353 , -0.654902  , -0.79607844],\n           ...,\n           [-0.8352941 , -0.67058825, -0.8039216 ],\n           [-0.8509804 , -0.6862745 , -0.81960785],\n           [-0.8666667 , -0.7019608 , -0.8352941 ]],\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.81960785, -0.67058825, -0.827451  ],\n           [-0.7882353 , -0.654902  , -0.79607844],\n           ...,\n           [-0.81960785, -0.654902  , -0.7882353 ],\n           [-0.8352941 , -0.67058825, -0.8039216 ],\n           [-0.84313726, -0.6784314 , -0.8117647 ]],\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.8117647 , -0.6627451 , -0.81960785],\n           [-0.78039217, -0.64705884, -0.7882353 ],\n           ...,\n           [-0.8117647 , -0.64705884, -0.78039217],\n           [-0.81960785, -0.654902  , -0.7882353 ],\n           [-0.8352941 , -0.67058825, -0.8039216 ]]],\n  \n  \n         [[[ 0.30980396,  0.4666667 ,  0.3803922 ],\n           [ 0.33333337,  0.4901961 ,  0.4039216 ],\n           [ 0.36470592,  0.52156866,  0.43529415],\n           ...,\n           [-0.02745098,  0.09803927, -0.00392157],\n           [-0.05882353,  0.082353  , -0.04313725],\n           [-0.00392157,  0.13725495,  0.01176476]],\n  \n          [[ 0.28627455,  0.4431373 ,  0.35686278],\n           [ 0.30980396,  0.4666667 ,  0.3803922 ],\n           [ 0.33333337,  0.4901961 ,  0.4039216 ],\n           ...,\n           [-0.06666666,  0.05882359, -0.04313725],\n           [-0.09803921,  0.04313731, -0.08235294],\n           [-0.06666666,  0.07450986, -0.05098039]],\n  \n          [[ 0.27058828,  0.427451  ,  0.3411765 ],\n           [ 0.28627455,  0.4431373 ,  0.35686278],\n           [ 0.2941177 ,  0.45098042,  0.36470592],\n           ...,\n           [-0.05882353,  0.06666672, -0.03529412],\n           [-0.06666666,  0.07450986, -0.05098039],\n           [-0.06666666,  0.07450986, -0.05098039]],\n  \n          ...,\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ]]],\n  \n  \n         [[[ 0.4039216 ,  0.5764706 ,  0.47450984],\n           [ 0.4039216 ,  0.5764706 ,  0.47450984],\n           [ 0.4039216 ,  0.5764706 ,  0.47450984],\n           ...,\n           [-0.3490196 , -0.16862744, -0.3098039 ],\n           [-0.27843136, -0.10588235, -0.20784312],\n           [-0.1372549 ,  0.03529418, -0.06666666]],\n  \n          [[ 0.3803922 ,  0.5529412 ,  0.45098042],\n           [ 0.3803922 ,  0.5529412 ,  0.45098042],\n           [ 0.3803922 ,  0.5529412 ,  0.45098042],\n           ...,\n           [-0.19999999, -0.01960784, -0.1607843 ],\n           [-0.10588235,  0.07450986, -0.05098039],\n           [ 0.05098045,  0.2313726 ,  0.10588241]],\n  \n          [[ 0.3411765 ,  0.5137255 ,  0.41176474],\n           [ 0.34901965,  0.52156866,  0.41960788],\n           [ 0.34901965,  0.52156866,  0.41960788],\n           ...,\n           [-0.09803921,  0.09019613, -0.08235294],\n           [ 0.0196079 ,  0.20000005,  0.04313731],\n           [ 0.17647064,  0.35686278,  0.21568632]],\n  \n          ...,\n  \n          [[ 0.6156863 ,  0.7490196 ,  0.60784316],\n           [ 0.60784316,  0.7411765 ,  0.6       ],\n           [ 0.6       ,  0.73333335,  0.60784316],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.6862745 ,  0.8352941 ,  0.6862745 ],\n           [ 0.654902  ,  0.8039216 ,  0.654902  ],\n           [ 0.6313726 ,  0.7647059 ,  0.6392157 ],\n           ...,\n           [ 0.654902  ,  0.84313726,  0.7411765 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.7411765 ,  0.8901961 ,  0.7411765 ],\n           [ 0.70980394,  0.85882354,  0.70980394],\n           [ 0.6627451 ,  0.79607844,  0.67058825],\n           ...,\n           [ 0.654902  ,  0.84313726,  0.7411765 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.4352941 , -0.36470586, -0.5764706 ],\n           [-0.4352941 , -0.36470586, -0.5764706 ],\n           [-0.4352941 , -0.36470586, -0.5764706 ],\n           ...,\n           [-0.27058822, -0.19215685, -0.18431371],\n           [-0.26274508, -0.17647058, -0.1607843 ],\n           [-0.25490195, -0.16862744, -0.15294117]],\n  \n          [[-0.41176468, -0.34117645, -0.5529412 ],\n           [-0.41176468, -0.34117645, -0.5529412 ],\n           [-0.41176468, -0.34117645, -0.5529412 ],\n           ...,\n           [-0.2862745 , -0.19215685, -0.19215685],\n           [-0.23137254, -0.14509803, -0.12941176],\n           [-0.20784312, -0.09803921, -0.09019607]],\n  \n          [[-0.38823527, -0.31764704, -0.5294118 ],\n           [-0.38823527, -0.31764704, -0.5294118 ],\n           [-0.38039213, -0.3098039 , -0.52156866],\n           ...,\n           [-0.2235294 , -0.11372548, -0.11372548],\n           [-0.19999999, -0.09019607, -0.08235294],\n           [-0.19215685, -0.06666666, -0.06666666]],\n  \n          ...,\n  \n          [[-0.38039213, -0.3098039 , -0.47450978],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           ...,\n           [-0.27058822, -0.20784312, -0.3960784 ],\n           [-0.2862745 , -0.2235294 , -0.41176468],\n           [-0.2862745 , -0.2235294 , -0.41176468]],\n  \n          [[-0.38823527, -0.31764704, -0.4823529 ],\n           [-0.38823527, -0.31764704, -0.4823529 ],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           ...,\n           [-0.29411763, -0.23137254, -0.41960782],\n           [-0.30196077, -0.23921567, -0.42745095],\n           [-0.30196077, -0.23921567, -0.42745095]],\n  \n          [[-0.3960784 , -0.32549018, -0.49019605],\n           [-0.3960784 , -0.32549018, -0.49019605],\n           [-0.38823527, -0.31764704, -0.4823529 ],\n           ...,\n           [-0.3098039 , -0.24705881, -0.4352941 ],\n           [-0.30196077, -0.23921567, -0.42745095],\n           [-0.2862745 , -0.2235294 , -0.41176468]]],\n  \n  \n         [[[-0.38823527, -0.25490195, -0.3333333 ],\n           [-0.32549018, -0.19215685, -0.27058822],\n           [-0.35686272, -0.2235294 , -0.30196077],\n           ...,\n           [-0.75686276, -0.5921569 , -0.70980394],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.77254903, -0.60784316, -0.7254902 ]],\n  \n          [[-0.38039213, -0.24705881, -0.32549018],\n           [-0.3490196 , -0.21568626, -0.29411763],\n           [-0.3960784 , -0.26274508, -0.34117645],\n           ...,\n           [-0.75686276, -0.5921569 , -0.70980394],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.78039217, -0.6156863 , -0.73333335]],\n  \n          [[-0.36470586, -0.23921567, -0.31764704],\n           [-0.372549  , -0.24705881, -0.32549018],\n           [-0.45098037, -0.32549018, -0.40392154],\n           ...,\n           [-0.7647059 , -0.6       , -0.7176471 ],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.78039217, -0.6156863 , -0.73333335]],\n  \n          ...,\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.6862745 , -0.5764706 , -0.69411767],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.69411767, -0.56078434, -0.6862745 ]],\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.69411767, -0.58431375, -0.7019608 ],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.69411767, -0.56078434, -0.6862745 ]],\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.69411767, -0.58431375, -0.7019608 ],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.6862745 , -0.5529412 , -0.6784314 ]]],\n  \n  \n         [[[-0.78039217, -0.54509807, -0.62352943],\n           [-0.79607844, -0.56078434, -0.6392157 ],\n           [-0.8117647 , -0.5764706 , -0.654902  ],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          [[-0.75686276, -0.52156866, -0.6       ],\n           [-0.78039217, -0.54509807, -0.62352943],\n           [-0.79607844, -0.56078434, -0.6392157 ],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          [[-0.73333335, -0.4980392 , -0.5764706 ],\n           [-0.75686276, -0.52156866, -0.6       ],\n           [-0.7647059 , -0.5529412 , -0.62352943],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          ...,\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.7019608 , -0.6313726 , -0.78039217],\n           [-0.70980394, -0.6392157 , -0.7882353 ]],\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.7019608 , -0.6313726 , -0.78039217],\n           [-0.70980394, -0.6392157 , -0.7882353 ]],\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.70980394, -0.6392157 , -0.7882353 ],\n           [-0.70980394, -0.6392157 , -0.7882353 ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n         1, 0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.79607844, -0.67058825, -0.75686276],\n           [-0.8039216 , -0.6784314 , -0.7647059 ],\n           [-0.8039216 , -0.6784314 , -0.7647059 ],\n           ...,\n           [-0.5764706 , -0.45098037, -0.5372549 ],\n\n*** WARNING: skipped 567691 bytes of output ***\n\n           [-0.5058824 , -0.3960784 , -0.5921569 ],\n           [-0.5058824 , -0.3960784 , -0.5921569 ],\n           [-0.4980392 , -0.38823527, -0.58431375]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n         0, 1, 1, 0, 0, 0, 1, 0, 1, 1], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.1372549 , -0.03529412, -0.16862744],\n           [-0.19999999, -0.09803921, -0.23137254],\n           [-0.19999999, -0.11372548, -0.23921567],\n           ...,\n           [-0.08235294,  0.12941182,  0.07450986],\n           [-0.04313725,  0.1686275 ,  0.11372554],\n           [-0.01176471,  0.20000005,  0.14509809]],\n  \n          [[-0.05882353,  0.03529418, -0.0745098 ],\n           [-0.14509803, -0.04313725, -0.17647058],\n           [-0.19999999, -0.11372548, -0.23921567],\n           ...,\n           [-0.05882353,  0.15294123,  0.09803927],\n           [-0.01960784,  0.19215691,  0.13725495],\n           [ 0.01176476,  0.22352946,  0.1686275 ]],\n  \n          [[ 0.00392163,  0.09803927,  0.00392163],\n           [-0.09019607,  0.00392163, -0.10588235],\n           [-0.19999999, -0.11372548, -0.2235294 ],\n           ...,\n           [-0.02745098,  0.18431377,  0.12941182],\n           [ 0.0196079 ,  0.2313726 ,  0.17647064],\n           [ 0.05098045,  0.26274514,  0.20784318]],\n  \n          ...,\n  \n          [[-0.38823527, -0.34117645, -0.45098037],\n           [-0.38823527, -0.34117645, -0.45098037],\n           [-0.38823527, -0.34117645, -0.45098037],\n           ...,\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.25490195, -0.21568626, -0.372549  ],\n           [-0.25490195, -0.21568626, -0.372549  ]],\n  \n          [[-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.38823527, -0.34117645, -0.45098037],\n           ...,\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.24705881, -0.20784312, -0.36470586]],\n  \n          [[-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           ...,\n           [-0.27843136, -0.23921567, -0.3960784 ],\n           [-0.27843136, -0.23921567, -0.3960784 ],\n           [-0.27843136, -0.23921567, -0.3960784 ]]],\n  \n  \n         [[[-0.4980392 , -0.4352941 , -0.6       ],\n           [-0.4980392 , -0.4352941 , -0.6       ],\n           [-0.4980392 , -0.4352941 , -0.6       ],\n           ...,\n           [-0.41960782, -0.30196077, -0.35686272],\n           [-0.41176468, -0.2862745 , -0.36470586],\n           [-0.41176468, -0.2862745 , -0.36470586]],\n  \n          [[-0.47450978, -0.41176468, -0.5764706 ],\n           [-0.4823529 , -0.41960782, -0.58431375],\n           [-0.4823529 , -0.41960782, -0.58431375],\n           ...,\n           [-0.41176468, -0.29411763, -0.3490196 ],\n           [-0.3490196 , -0.23137254, -0.2862745 ],\n           [-0.3098039 , -0.18431371, -0.26274508]],\n  \n          [[-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           ...,\n           [-0.3490196 , -0.21568626, -0.26274508],\n           [-0.31764704, -0.18431371, -0.23137254],\n           [-0.29411763, -0.1607843 , -0.2235294 ]],\n  \n          ...,\n  \n          [[-0.44313723, -0.3490196 , -0.5058824 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           ...,\n           [-0.3333333 , -0.27058822, -0.4352941 ],\n           [-0.34117645, -0.27843136, -0.44313723],\n           [-0.34117645, -0.27843136, -0.44313723]],\n  \n          [[-0.45098037, -0.35686272, -0.5137255 ],\n           [-0.45098037, -0.35686272, -0.5137255 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           ...,\n           [-0.3490196 , -0.2862745 , -0.45098037],\n           [-0.3490196 , -0.2862745 , -0.45098037],\n           [-0.34117645, -0.27843136, -0.44313723]],\n  \n          [[-0.4588235 , -0.36470586, -0.52156866],\n           [-0.4588235 , -0.36470586, -0.52156866],\n           [-0.45098037, -0.35686272, -0.5137255 ],\n           ...,\n           [-0.36470586, -0.30196077, -0.46666664],\n           [-0.35686272, -0.29411763, -0.4588235 ],\n           [-0.34117645, -0.27843136, -0.44313723]]],\n  \n  \n         [[[ 0.13725495,  0.28627455,  0.12941182],\n           [ 0.13725495,  0.28627455,  0.12941182],\n           [ 0.14509809,  0.2941177 ,  0.13725495],\n           ...,\n           [-0.5372549 , -0.41176468, -0.4980392 ],\n           [-0.5372549 , -0.41176468, -0.49019605],\n           [-0.5372549 , -0.41176468, -0.49019605]],\n  \n          [[ 0.12941182,  0.27843142,  0.12156868],\n           [ 0.12941182,  0.27843142,  0.12156868],\n           [ 0.13725495,  0.28627455,  0.12941182],\n           ...,\n           [-0.52156866, -0.41176468, -0.5137255 ],\n           [-0.52156866, -0.41176468, -0.5137255 ],\n           [-0.52156866, -0.41176468, -0.5137255 ]],\n  \n          [[ 0.12156868,  0.254902  ,  0.10588241],\n           [ 0.12941182,  0.26274514,  0.11372554],\n           [ 0.13725495,  0.27058828,  0.12156868],\n           ...,\n           [-0.49019605, -0.41960782, -0.5529412 ],\n           [-0.49019605, -0.41960782, -0.5529412 ],\n           [-0.49019605, -0.41960782, -0.5529412 ]],\n  \n          ...,\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ]],\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ]],\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.49019605, -0.38039213, -0.56078434]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.44313723, -0.41176468, -0.5921569 ],\n           [-0.44313723, -0.41176468, -0.5921569 ],\n           [-0.44313723, -0.41176468, -0.5921569 ],\n           ...,\n           [-0.38823527, -0.27058822, -0.3098039 ],\n           [-0.38039213, -0.26274508, -0.2862745 ],\n           [-0.38039213, -0.26274508, -0.2862745 ]],\n  \n          [[-0.42745095, -0.3960784 , -0.5764706 ],\n           [-0.4352941 , -0.40392154, -0.58431375],\n           [-0.4352941 , -0.40392154, -0.58431375],\n           ...,\n           [-0.38039213, -0.26274508, -0.2862745 ],\n           [-0.3333333 , -0.21568626, -0.23921567],\n           [-0.3098039 , -0.19215685, -0.21568626]],\n  \n          [[-0.41176468, -0.38039213, -0.56078434],\n           [-0.41176468, -0.38039213, -0.56078434],\n           [-0.41176468, -0.38039213, -0.56078434],\n           ...,\n           [-0.30196077, -0.17647058, -0.18431371],\n           [-0.29411763, -0.16862744, -0.17647058],\n           [-0.2862745 , -0.1607843 , -0.16862744]],\n  \n          ...,\n  \n          [[-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.29411763, -0.23137254, -0.3960784 ],\n           [-0.3098039 , -0.24705881, -0.41176468],\n           [-0.31764704, -0.25490195, -0.41960782]],\n  \n          [[-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.31764704, -0.25490195, -0.41960782]],\n  \n          [[-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.30196077, -0.23921567, -0.40392154],\n           [-0.29411763, -0.23137254, -0.3960784 ]]],\n  \n  \n         [[[-0.3333333 , -0.27058822, -0.38823527],\n           [-0.3333333 , -0.27058822, -0.38823527],\n           [-0.3333333 , -0.27058822, -0.38823527],\n           ...,\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ]],\n  \n          [[-0.34117645, -0.27843136, -0.3960784 ],\n           [-0.34117645, -0.27843136, -0.3960784 ],\n           [-0.34117645, -0.27843136, -0.3960784 ],\n           ...,\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ]],\n  \n          [[-0.3490196 , -0.2862745 , -0.40392154],\n           [-0.3490196 , -0.2862745 , -0.40392154],\n           [-0.3490196 , -0.2862745 , -0.40392154],\n           ...,\n           [-0.42745095, -0.40392154, -0.5372549 ],\n           [-0.42745095, -0.40392154, -0.5372549 ],\n           [-0.42745095, -0.40392154, -0.5372549 ]],\n  \n          ...,\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]],\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]],\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]]],\n  \n  \n         [[[-1.        , -0.8901961 , -0.6784314 ],\n           [-0.99215686, -0.88235295, -0.654902  ],\n           [-0.9843137 , -0.8901961 , -0.5921569 ],\n           ...,\n           [-0.8509804 , -0.7882353 , -0.90588236],\n           [-0.85882354, -0.79607844, -0.9137255 ],\n           [-0.8666667 , -0.8039216 , -0.92156863]],\n  \n          [[-1.        , -0.90588236, -0.7019608 ],\n           [-1.        , -0.90588236, -0.69411767],\n           [-1.        , -0.90588236, -0.6392157 ],\n           ...,\n           [-0.8352941 , -0.77254903, -0.8901961 ],\n           [-0.84313726, -0.78039217, -0.8980392 ],\n           [-0.8509804 , -0.7882353 , -0.90588236]],\n  \n          [[-1.        , -0.92941177, -0.78039217],\n           [-1.        , -0.92941177, -0.7411765 ],\n           [-1.        , -0.92941177, -0.69411767],\n           ...,\n           [-0.827451  , -0.7411765 , -0.8666667 ],\n           [-0.8352941 , -0.7490196 , -0.8745098 ],\n           [-0.84313726, -0.75686276, -0.88235295]],\n  \n          ...,\n  \n          [[-0.654902  , -0.58431375, -0.7176471 ],\n           [-0.6156863 , -0.54509807, -0.6784314 ],\n           [-0.6392157 , -0.58431375, -0.70980394],\n           ...,\n           [-0.8901961 , -0.8666667 , -1.        ],\n           [-0.88235295, -0.85882354, -0.99215686],\n           [-0.8745098 , -0.8509804 , -1.        ]],\n  \n          [[-0.69411767, -0.62352943, -0.75686276],\n           [-0.6156863 , -0.54509807, -0.6784314 ],\n           [-0.6       , -0.54509807, -0.67058825],\n           ...,\n           [-0.88235295, -0.85882354, -1.        ],\n           [-0.8745098 , -0.8509804 , -1.        ],\n           [-0.8666667 , -0.84313726, -1.        ]],\n  \n          [[-0.79607844, -0.7254902 , -0.85882354],\n           [-0.6862745 , -0.6156863 , -0.7490196 ],\n           [-0.64705884, -0.5921569 , -0.7176471 ],\n           ...,\n           [-0.88235295, -0.85882354, -1.        ],\n           [-0.8745098 , -0.8509804 , -1.        ],\n           [-0.8666667 , -0.84313726, -1.        ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n         0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(14, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-1.        , -0.8980392 , -0.6627451 ],\n           [-1.        , -0.8901961 , -0.6392157 ],\n           [-1.        , -0.8980392 , -0.5921569 ],\n           ...,\n           [-0.88235295, -0.7254902 , -0.81960785],\n           [-0.8901961 , -0.73333335, -0.827451  ],\n           [-0.8901961 , -0.73333335, -0.827451  ]],\n  \n          [[-1.        , -0.90588236, -0.6862745 ],\n           [-1.        , -0.90588236, -0.67058825],\n           [-1.        , -0.8980392 , -0.6156863 ],\n           ...,\n           [-0.8666667 , -0.70980394, -0.8039216 ],\n           [-0.8745098 , -0.7176471 , -0.8117647 ],\n           [-0.88235295, -0.7254902 , -0.81960785]],\n  \n          [[-1.        , -0.92156863, -0.7411765 ],\n           [-1.        , -0.9137255 , -0.7176471 ],\n           [-1.        , -0.9137255 , -0.67058825],\n           ...,\n           [-0.8509804 , -0.69411767, -0.7882353 ],\n           [-0.85882354, -0.7019608 , -0.79607844],\n           [-0.85882354, -0.7019608 , -0.79607844]],\n  \n          ...,\n  \n          [[-0.8901961 , -0.7882353 , -0.8666667 ],\n           [-0.7647059 , -0.6627451 , -0.7411765 ],\n           [-0.6156863 , -0.52156866, -0.6156863 ],\n           ...,\n           [-0.8745098 , -0.8117647 , -0.9764706 ],\n           [-0.88235295, -0.827451  , -0.96862745],\n           [-0.88235295, -0.81960785, -0.9843137 ]],\n  \n          [[-0.8901961 , -0.7882353 , -0.8666667 ],\n           [-0.79607844, -0.69411767, -0.77254903],\n           [-0.69411767, -0.58431375, -0.6862745 ],\n           ...,\n           [-0.8666667 , -0.8039216 , -0.9843137 ],\n           [-0.8666667 , -0.8039216 , -0.96862745],\n           [-0.8666667 , -0.8039216 , -0.9843137 ]],\n  \n          [[-0.8509804 , -0.7490196 , -0.827451  ],\n           [-0.8117647 , -0.70980394, -0.7882353 ],\n           [-0.75686276, -0.64705884, -0.7490196 ],\n           ...,\n           [-0.88235295, -0.81960785, -1.        ],\n           [-0.88235295, -0.81960785, -1.        ],\n           [-0.88235295, -0.81960785, -1.        ]]],\n  \n  \n         [[[-0.44313723, -0.3098039 , -0.38823527],\n           [-0.44313723, -0.3098039 , -0.38823527],\n           [-0.4352941 , -0.30196077, -0.38039213],\n           ...,\n           [-0.40392154, -0.26274508, -0.4352941 ],\n           [-0.41176468, -0.27058822, -0.44313723],\n           [-0.41176468, -0.27058822, -0.44313723]],\n  \n          [[-0.46666664, -0.3333333 , -0.41176468],\n           [-0.46666664, -0.3333333 , -0.41176468],\n           [-0.4588235 , -0.32549018, -0.40392154],\n           ...,\n           [-0.41176468, -0.27058822, -0.44313723],\n           [-0.42745095, -0.29411763, -0.44313723],\n           [-0.4352941 , -0.29411763, -0.46666664]],\n  \n          [[-0.4823529 , -0.35686272, -0.44313723],\n           [-0.4823529 , -0.35686272, -0.44313723],\n           [-0.47450978, -0.3490196 , -0.4352941 ],\n           ...,\n           [-0.41960782, -0.2862745 , -0.4352941 ],\n           [-0.45098037, -0.31764704, -0.4588235 ],\n           [-0.4588235 , -0.32549018, -0.47450978]],\n  \n          ...,\n  \n          [[-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.5921569 , -0.69411767],\n           [-0.7019608 , -0.5921569 , -0.69411767],\n           ...,\n           [-0.5294118 , -0.45098037, -0.52156866],\n           [-0.52156866, -0.46666664, -0.5294118 ],\n           [-0.5372549 , -0.4588235 , -0.5294118 ]],\n  \n          [[-0.7019608 , -0.5921569 , -0.69411767],\n           [-0.7019608 , -0.5921569 , -0.70980394],\n           [-0.7019608 , -0.5921569 , -0.70980394],\n           ...,\n           [-0.44313723, -0.38039213, -0.46666664],\n           [-0.4352941 , -0.38823527, -0.46666664],\n           [-0.45098037, -0.38823527, -0.47450978]],\n  \n          [[-0.7019608 , -0.5921569 , -0.70980394],\n           [-0.70980394, -0.6       , -0.7176471 ],\n           [-0.70980394, -0.6       , -0.7176471 ],\n           ...,\n           [-0.36470586, -0.31764704, -0.3960784 ],\n           [-0.36470586, -0.31764704, -0.3960784 ],\n           [-0.36470586, -0.31764704, -0.3960784 ]]],\n  \n  \n         [[[-0.77254903, -0.6627451 , -0.78039217],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           ...,\n           [-0.5686275 , -0.4588235 , -0.56078434],\n           [-0.54509807, -0.4352941 , -0.5372549 ],\n           [-0.52156866, -0.41176468, -0.5137255 ]],\n  \n          [[-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           ...,\n           [-0.58431375, -0.47450978, -0.5764706 ],\n           [-0.56078434, -0.45098037, -0.5529412 ],\n           [-0.54509807, -0.4352941 , -0.5372549 ]],\n  \n          [[-0.75686276, -0.64705884, -0.7647059 ],\n           [-0.75686276, -0.64705884, -0.7647059 ],\n           [-0.75686276, -0.64705884, -0.7647059 ],\n           ...,\n           [-0.60784316, -0.4980392 , -0.6       ],\n           [-0.5921569 , -0.4823529 , -0.58431375],\n           [-0.5764706 , -0.46666664, -0.5686275 ]],\n  \n          ...,\n  \n          [[-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7176471 , -0.6313726 , -0.7647059 ],\n           ...,\n           [-0.84313726, -0.73333335, -0.9137255 ],\n           [-0.84313726, -0.73333335, -0.9137255 ],\n           [-0.84313726, -0.73333335, -0.92941177]],\n  \n          [[-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7176471 , -0.62352943, -0.78039217],\n           [-0.70980394, -0.62352943, -0.75686276],\n           ...,\n           [-0.85882354, -0.7411765 , -0.92156863],\n           [-0.85882354, -0.7411765 , -0.92156863],\n           [-0.85882354, -0.73333335, -0.9372549 ]],\n  \n          [[-0.7176471 , -0.62352943, -0.78039217],\n           [-0.7176471 , -0.62352943, -0.78039217],\n           [-0.70980394, -0.62352943, -0.75686276],\n           ...,\n           [-0.8666667 , -0.7490196 , -0.92941177],\n           [-0.8666667 , -0.7490196 , -0.92941177],\n           [-0.85882354, -0.73333335, -0.9372549 ]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.5686275 , -0.41176468, -0.4980392 ],\n           [-0.5529412 , -0.3960784 , -0.4823529 ],\n           [-0.54509807, -0.38823527, -0.47450978]],\n  \n          [[-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.56078434, -0.40392154, -0.49019605],\n           [-0.5529412 , -0.3960784 , -0.4823529 ],\n           [-0.5529412 , -0.3960784 , -0.4823529 ]],\n  \n          [[-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.58431375, -0.42745095, -0.5137255 ],\n           [-0.58431375, -0.42745095, -0.5137255 ],\n           [-0.58431375, -0.42745095, -0.5137255 ]],\n  \n          ...,\n  \n          [[-0.6       , -0.5137255 , -0.62352943],\n           [-0.6       , -0.5137255 , -0.62352943],\n           [-0.6       , -0.5137255 , -0.62352943],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]],\n  \n          [[-0.5921569 , -0.5058824 , -0.6156863 ],\n           [-0.5921569 , -0.5058824 , -0.6156863 ],\n           [-0.5921569 , -0.5058824 , -0.6156863 ],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]],\n  \n          [[-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]]],\n  \n  \n         [[[ 0.4039216 ,  0.5372549 ,  0.45882356],\n           [ 0.427451  ,  0.56078434,  0.48235297],\n           [ 0.45098042,  0.58431375,  0.5058824 ],\n           ...,\n           [-0.04313725,  0.12156868,  0.00392163],\n           [-0.0745098 ,  0.09019613, -0.02745098],\n           [-0.01176471,  0.15294123,  0.03529418]],\n  \n          [[ 0.38823533,  0.52156866,  0.4431373 ],\n           [ 0.4039216 ,  0.5372549 ,  0.45882356],\n           [ 0.427451  ,  0.56078434,  0.48235297],\n           ...,\n           [-0.0745098 ,  0.09019613, -0.02745098],\n           [-0.08235294,  0.082353  , -0.03529412],\n           [-0.04313725,  0.12156868,  0.00392163]],\n  \n          [[ 0.35686278,  0.4901961 ,  0.41176474],\n           [ 0.3803922 ,  0.5137255 ,  0.43529415],\n           [ 0.4039216 ,  0.5372549 ,  0.45882356],\n           ...,\n           [-0.12156862,  0.04313731, -0.0745098 ],\n           [-0.10588235,  0.05882359, -0.05882353],\n           [-0.08235294,  0.082353  , -0.03529412]],\n  \n          ...,\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.77254903,  0.9764706 ,  0.8745098 ],\n           [ 0.7647059 ,  0.9843137 ,  0.8745098 ],\n           [ 0.7647059 ,  0.9843137 ,  0.8745098 ]],\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.75686276,  0.9607843 ,  0.85882354],\n           [ 0.7490196 ,  0.96862745,  0.85882354],\n           [ 0.7490196 ,  0.96862745,  0.85882354]],\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.7490196 ,  0.9529412 ,  0.8509804 ],\n           [ 0.7411765 ,  0.9607843 ,  0.8509804 ],\n           [ 0.7411765 ,  0.9607843 ,  0.8509804 ]]],\n  \n  \n         [[[-0.4352941 , -0.32549018, -0.44313723],\n           [-0.4352941 , -0.32549018, -0.44313723],\n           [-0.42745095, -0.31764704, -0.4352941 ],\n           ...,\n           [-0.40392154, -0.29411763, -0.42745095],\n           [-0.41176468, -0.30196077, -0.4352941 ],\n           [-0.41176468, -0.30196077, -0.4352941 ]],\n  \n          [[-0.44313723, -0.3333333 , -0.45098037],\n           [-0.44313723, -0.3333333 , -0.45098037],\n           [-0.4352941 , -0.32549018, -0.44313723],\n           ...,\n           [-0.41176468, -0.30196077, -0.4352941 ],\n           [-0.42745095, -0.31764704, -0.45098037],\n           [-0.4352941 , -0.32549018, -0.4588235 ]],\n  \n          [[-0.4588235 , -0.3490196 , -0.46666664],\n           [-0.45098037, -0.34117645, -0.4588235 ],\n           [-0.44313723, -0.3333333 , -0.4352941 ],\n           ...,\n           [-0.41176468, -0.30196077, -0.41960782],\n           [-0.4352941 , -0.32549018, -0.4588235 ],\n           [-0.45098037, -0.34117645, -0.47450978]],\n  \n          ...,\n  \n          [[-0.5686275 , -0.46666664, -0.60784316],\n           [-0.5529412 , -0.45098037, -0.5921569 ],\n           [-0.5372549 , -0.4352941 , -0.5764706 ],\n           ...,\n           [-0.4980392 , -0.41960782, -0.5058824 ],\n           [-0.49019605, -0.4352941 , -0.4980392 ],\n           [-0.5137255 , -0.4352941 , -0.5058824 ]],\n  \n          [[-0.5686275 , -0.46666664, -0.6       ],\n           [-0.5529412 , -0.45098037, -0.58431375],\n           [-0.5372549 , -0.4352941 , -0.5686275 ],\n           ...,\n           [-0.41960782, -0.35686272, -0.44313723],\n           [-0.42745095, -0.38823527, -0.44313723],\n           [-0.45098037, -0.3960784 , -0.4588235 ]],\n  \n          [[-0.5686275 , -0.46666664, -0.6       ],\n           [-0.5529412 , -0.45098037, -0.58431375],\n           [-0.5372549 , -0.4352941 , -0.5686275 ],\n           ...,\n           [-0.3490196 , -0.30196077, -0.38039213],\n           [-0.36470586, -0.32549018, -0.38039213],\n           [-0.38039213, -0.34117645, -0.3960784 ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(14,), dtype=int32, numpy=array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=int32)&gt;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [(&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[ 0.10588241,  0.27843142,  0.11372554],\n           [ 0.12156868,  0.2941177 ,  0.12941182],\n           [ 0.12941182,  0.30196083,  0.13725495],\n           ...,\n           [-0.54509807, -0.41176468, -0.56078434],\n           [-0.5372549 , -0.42745095, -0.62352943],\n           [-0.54509807, -0.44313723, -0.64705884]],\n  \n          [[ 0.09019613,  0.26274514,  0.09803927],\n           [ 0.10588241,  0.27843142,  0.11372554],\n           [ 0.12941182,  0.30196083,  0.13725495],\n           ...,\n           [-0.5294118 , -0.38823527, -0.56078434],\n           [-0.5137255 , -0.41176468, -0.6156863 ],\n           [-0.5294118 , -0.42745095, -0.64705884]],\n  \n          [[ 0.06666672,  0.23921573,  0.07450986],\n           [ 0.09019613,  0.26274514,  0.09803927],\n           [ 0.12156868,  0.2941177 ,  0.12941182],\n           ...,\n           [-0.4980392 , -0.372549  , -0.5764706 ],\n           [-0.49019605, -0.41176468, -0.62352943],\n           [-0.5058824 , -0.41960782, -0.654902  ]],\n  \n          ...,\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5058824 , -0.3960784 , -0.5764706 ],\n           [-0.5058824 , -0.3960784 , -0.5764706 ],\n           [-0.5058824 , -0.3960784 , -0.5764706 ]],\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5058824 , -0.3960784 , -0.5764706 ]],\n  \n          [[-0.5137255 , -0.42745095, -0.56078434],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           [-0.5058824 , -0.41960782, -0.5529412 ],\n           ...,\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375],\n           [-0.5137255 , -0.40392154, -0.58431375]]],\n  \n  \n         [[[-0.44313723, -0.31764704, -0.40392154],\n           [-0.44313723, -0.31764704, -0.40392154],\n           [-0.4352941 , -0.3098039 , -0.3960784 ],\n           ...,\n           [-0.372549  , -0.26274508, -0.36470586],\n           [-0.372549  , -0.26274508, -0.36470586],\n           [-0.36470586, -0.25490195, -0.35686272]],\n  \n          [[-0.45098037, -0.32549018, -0.41176468],\n           [-0.45098037, -0.32549018, -0.41176468],\n           [-0.44313723, -0.31764704, -0.40392154],\n           ...,\n           [-0.38039213, -0.27058822, -0.372549  ],\n           [-0.3960784 , -0.2862745 , -0.38823527],\n           [-0.3960784 , -0.2862745 , -0.38823527]],\n  \n          [[-0.46666664, -0.34117645, -0.42745095],\n           [-0.4588235 , -0.3333333 , -0.41960782],\n           [-0.45098037, -0.32549018, -0.41176468],\n           ...,\n           [-0.3960784 , -0.2862745 , -0.38823527],\n           [-0.41960782, -0.3098039 , -0.41176468],\n           [-0.42745095, -0.31764704, -0.41960782]],\n  \n          ...,\n  \n          [[-0.6392157 , -0.4980392 , -0.60784316],\n           [-0.62352943, -0.4823529 , -0.5921569 ],\n           [-0.6       , -0.47450978, -0.5764706 ],\n           ...,\n           [-0.5137255 , -0.42745095, -0.47450978],\n           [-0.52156866, -0.45098037, -0.49019605],\n           [-0.5372549 , -0.45098037, -0.4980392 ]],\n  \n          [[-0.6313726 , -0.49019605, -0.6       ],\n           [-0.62352943, -0.4823529 , -0.5921569 ],\n           [-0.5921569 , -0.46666664, -0.5686275 ],\n           ...,\n           [-0.4352941 , -0.36470586, -0.40392154],\n           [-0.4352941 , -0.38823527, -0.41960782],\n           [-0.4588235 , -0.38823527, -0.42745095]],\n  \n          [[-0.6313726 , -0.49019605, -0.6       ],\n           [-0.6156863 , -0.47450978, -0.58431375],\n           [-0.5921569 , -0.46666664, -0.5686275 ],\n           ...,\n           [-0.35686272, -0.3098039 , -0.34117645],\n           [-0.35686272, -0.3098039 , -0.34117645],\n           [-0.35686272, -0.3098039 , -0.34117645]]],\n  \n  \n         [[[-0.75686276, -0.5764706 , -0.6392157 ],\n           [-0.7647059 , -0.58431375, -0.64705884],\n           [-0.77254903, -0.5921569 , -0.654902  ],\n           ...,\n           [-0.9372549 , -0.8117647 , -0.8980392 ],\n           [-0.94509804, -0.81960785, -0.90588236],\n           [-0.94509804, -0.81960785, -0.90588236]],\n  \n          [[-0.73333335, -0.5529412 , -0.6156863 ],\n           [-0.7411765 , -0.56078434, -0.62352943],\n           [-0.7490196 , -0.5686275 , -0.6313726 ],\n           ...,\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.92941177, -0.8039216 , -0.8901961 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]],\n  \n          [[-0.69411767, -0.5137255 , -0.5764706 ],\n           [-0.70980394, -0.5294118 , -0.5921569 ],\n           [-0.7254902 , -0.54509807, -0.60784316],\n           ...,\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.92941177, -0.8039216 , -0.8901961 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]],\n  \n          ...,\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           ...,\n           [-0.8980392 , -0.79607844, -0.8745098 ],\n           [-0.94509804, -0.81960785, -0.90588236],\n           [-0.96862745, -0.84313726, -0.92941177]],\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.92156863, -0.81960785, -0.8745098 ],\n           ...,\n           [-0.8745098 , -0.77254903, -0.8509804 ],\n           [-0.92156863, -0.79607844, -0.88235295],\n           [-0.94509804, -0.81960785, -0.90588236]],\n  \n          [[-0.92156863, -0.8352941 , -0.88235295],\n           [-0.9137255 , -0.827451  , -0.8745098 ],\n           [-0.92156863, -0.81960785, -0.8745098 ],\n           ...,\n           [-0.85882354, -0.75686276, -0.8352941 ],\n           [-0.90588236, -0.78039217, -0.8666667 ],\n           [-0.92941177, -0.8039216 , -0.8901961 ]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.32549018, -0.25490195, -0.38823527],\n           [-0.32549018, -0.25490195, -0.38823527],\n           [-0.31764704, -0.24705881, -0.38039213],\n           ...,\n           [-0.4352941 , -0.372549  , -0.5372549 ],\n           [-0.44313723, -0.38039213, -0.54509807],\n           [-0.44313723, -0.38039213, -0.54509807]],\n  \n          [[-0.3490196 , -0.27843136, -0.41176468],\n           [-0.3490196 , -0.27843136, -0.41176468],\n           [-0.34117645, -0.27058822, -0.40392154],\n           ...,\n           [-0.44313723, -0.38039213, -0.54509807],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ]],\n  \n          [[-0.372549  , -0.30196077, -0.4352941 ],\n           [-0.372549  , -0.30196077, -0.4352941 ],\n           [-0.36470586, -0.29411763, -0.42745095],\n           ...,\n           [-0.4588235 , -0.3960784 , -0.56078434],\n           [-0.4588235 , -0.3960784 , -0.56078434],\n           [-0.46666664, -0.40392154, -0.5686275 ]],\n  \n          ...,\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]],\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]],\n  \n          [[-0.52156866, -0.4352941 , -0.54509807],\n           [-0.5137255 , -0.42745095, -0.5372549 ],\n           [-0.5058824 , -0.41960782, -0.5294118 ],\n           ...,\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.5921569 , -0.5058824 , -0.6156863 ]]],\n  \n  \n         [[[-0.05882353,  0.05098045, -0.06666666],\n           [-0.12941176, -0.01960784, -0.1372549 ],\n           [-0.20784312, -0.11372548, -0.2235294 ],\n           ...,\n           [ 0.30980396,  0.47450984,  0.35686278],\n           [ 0.30980396,  0.47450984,  0.35686278],\n           [ 0.30196083,  0.4666667 ,  0.34901965]],\n  \n          [[-0.2235294 , -0.11372548, -0.23137254],\n           [-0.25490195, -0.14509803, -0.26274508],\n           [-0.29411763, -0.19999999, -0.3098039 ],\n           ...,\n           [ 0.2941177 ,  0.45882356,  0.3411765 ],\n           [ 0.28627455,  0.45098042,  0.33333337],\n           [ 0.28627455,  0.45098042,  0.33333337]],\n  \n          [[-0.35686272, -0.24705881, -0.36470586],\n           [-0.36470586, -0.25490195, -0.372549  ],\n           [-0.35686272, -0.26274508, -0.372549  ],\n           ...,\n           [ 0.27843142,  0.4431373 ,  0.32549024],\n           [ 0.27058828,  0.43529415,  0.3176471 ],\n           [ 0.26274514,  0.427451  ,  0.30980396]],\n  \n          ...,\n  \n          [[ 0.6627451 ,  0.8039216 ,  0.70980394],\n           [ 0.6784314 ,  0.81960785,  0.7254902 ],\n           [ 0.7019608 ,  0.827451  ,  0.7411765 ],\n           ...,\n           [ 0.7019608 ,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825]],\n  \n          [[ 0.6784314 ,  0.81960785,  0.7254902 ],\n           [ 0.6862745 ,  0.827451  ,  0.73333335],\n           [ 0.7019608 ,  0.827451  ,  0.7411765 ],\n           ...,\n           [ 0.7019608 ,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825],\n           [ 0.69411767,  0.84313726,  0.67058825]],\n  \n          [[ 0.6862745 ,  0.827451  ,  0.73333335],\n           [ 0.69411767,  0.8352941 ,  0.7411765 ],\n           [ 0.69411767,  0.81960785,  0.73333335],\n           ...,\n           [ 0.69411767,  0.8352941 ,  0.6627451 ],\n           [ 0.6862745 ,  0.8352941 ,  0.6627451 ],\n           [ 0.6862745 ,  0.8352941 ,  0.6627451 ]]],\n  \n  \n         [[[-0.46666664, -0.31764704, -0.36470586],\n           [-0.46666664, -0.31764704, -0.36470586],\n           [-0.4588235 , -0.3098039 , -0.35686272],\n           ...,\n           [-0.4352941 , -0.30196077, -0.38039213],\n           [-0.4352941 , -0.3098039 , -0.38823527],\n           [-0.4352941 , -0.3098039 , -0.38823527]],\n  \n          [[-0.4823529 , -0.3333333 , -0.38039213],\n           [-0.4823529 , -0.3333333 , -0.38039213],\n           [-0.47450978, -0.32549018, -0.372549  ],\n           ...,\n           [-0.4352941 , -0.30196077, -0.38039213],\n           [-0.4352941 , -0.3098039 , -0.38823527],\n           [-0.44313723, -0.31764704, -0.3960784 ]],\n  \n          [[-0.5137255 , -0.36470586, -0.41176468],\n           [-0.5137255 , -0.36470586, -0.41176468],\n           [-0.5058824 , -0.35686272, -0.40392154],\n           ...,\n           [-0.44313723, -0.3098039 , -0.38823527],\n           [-0.4588235 , -0.32549018, -0.40392154],\n           [-0.46666664, -0.3333333 , -0.41176468]],\n  \n          ...,\n  \n          [[-0.8352941 , -0.56078434, -0.6156863 ],\n           [-0.8352941 , -0.56078434, -0.6156863 ],\n           [-0.8352941 , -0.56078434, -0.6156863 ],\n           ...,\n           [-0.5686275 , -0.4588235 , -0.45098037],\n           [-0.5529412 , -0.46666664, -0.4352941 ],\n           [-0.5529412 , -0.46666664, -0.4352941 ]],\n  \n          [[-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.81960785, -0.56078434, -0.6313726 ],\n           ...,\n           [-0.4823529 , -0.3960784 , -0.38039213],\n           [-0.4823529 , -0.41176468, -0.372549  ],\n           [-0.49019605, -0.41960782, -0.38039213]],\n  \n          [[-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.827451  , -0.5686275 , -0.6392157 ],\n           [-0.81960785, -0.5764706 , -0.6392157 ],\n           ...,\n           [-0.40392154, -0.32549018, -0.3098039 ],\n           [-0.41176468, -0.35686272, -0.3098039 ],\n           [-0.41960782, -0.36470586, -0.31764704]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n         0, 0, 0, 1, 1, 0, 1, 1, 1, 1], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.81960785, -0.6313726 , -0.75686276],\n           ...,\n           [-0.6       , -0.4588235 , -0.5529412 ],\n           [-0.58431375, -0.44313723, -0.5372549 ],\n           [-0.5686275 , -0.42745095, -0.52156866]],\n  \n          [[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           ...,\n           [-0.6       , -0.4588235 , -0.5529412 ],\n           [-0.58431375, -0.44313723, -0.5372549 ],\n           [-0.5764706 , -0.4352941 , -0.5294118 ]],\n  \n          [[-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           [-0.827451  , -0.6392157 , -0.7647059 ],\n           ...,\n           [-0.62352943, -0.4823529 , -0.5764706 ],\n           [-0.60784316, -0.46666664, -0.56078434],\n           [-0.6       , -0.4588235 , -0.5529412 ]],\n  \n          ...,\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.81960785, -0.67058825, -0.827451  ],\n           [-0.7882353 , -0.654902  , -0.79607844],\n           ...,\n           [-0.8352941 , -0.67058825, -0.8039216 ],\n           [-0.8509804 , -0.6862745 , -0.81960785],\n           [-0.8666667 , -0.7019608 , -0.8352941 ]],\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.81960785, -0.67058825, -0.827451  ],\n           [-0.7882353 , -0.654902  , -0.79607844],\n           ...,\n           [-0.81960785, -0.654902  , -0.7882353 ],\n           [-0.8352941 , -0.67058825, -0.8039216 ],\n           [-0.84313726, -0.6784314 , -0.8117647 ]],\n  \n          [[-0.8352941 , -0.6862745 , -0.84313726],\n           [-0.8117647 , -0.6627451 , -0.81960785],\n           [-0.78039217, -0.64705884, -0.7882353 ],\n           ...,\n           [-0.8117647 , -0.64705884, -0.78039217],\n           [-0.81960785, -0.654902  , -0.7882353 ],\n           [-0.8352941 , -0.67058825, -0.8039216 ]]],\n  \n  \n         [[[ 0.30980396,  0.4666667 ,  0.3803922 ],\n           [ 0.33333337,  0.4901961 ,  0.4039216 ],\n           [ 0.36470592,  0.52156866,  0.43529415],\n           ...,\n           [-0.02745098,  0.09803927, -0.00392157],\n           [-0.05882353,  0.082353  , -0.04313725],\n           [-0.00392157,  0.13725495,  0.01176476]],\n  \n          [[ 0.28627455,  0.4431373 ,  0.35686278],\n           [ 0.30980396,  0.4666667 ,  0.3803922 ],\n           [ 0.33333337,  0.4901961 ,  0.4039216 ],\n           ...,\n           [-0.06666666,  0.05882359, -0.04313725],\n           [-0.09803921,  0.04313731, -0.08235294],\n           [-0.06666666,  0.07450986, -0.05098039]],\n  \n          [[ 0.27058828,  0.427451  ,  0.3411765 ],\n           [ 0.28627455,  0.4431373 ,  0.35686278],\n           [ 0.2941177 ,  0.45098042,  0.36470592],\n           ...,\n           [-0.05882353,  0.06666672, -0.03529412],\n           [-0.06666666,  0.07450986, -0.05098039],\n           [-0.06666666,  0.07450986, -0.05098039]],\n  \n          ...,\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.62352943,  0.7411765 ,  0.6862745 ],\n           [ 0.6392157 ,  0.7411765 ,  0.6862745 ],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ]]],\n  \n  \n         [[[ 0.4039216 ,  0.5764706 ,  0.47450984],\n           [ 0.4039216 ,  0.5764706 ,  0.47450984],\n           [ 0.4039216 ,  0.5764706 ,  0.47450984],\n           ...,\n           [-0.3490196 , -0.16862744, -0.3098039 ],\n           [-0.27843136, -0.10588235, -0.20784312],\n           [-0.1372549 ,  0.03529418, -0.06666666]],\n  \n          [[ 0.3803922 ,  0.5529412 ,  0.45098042],\n           [ 0.3803922 ,  0.5529412 ,  0.45098042],\n           [ 0.3803922 ,  0.5529412 ,  0.45098042],\n           ...,\n           [-0.19999999, -0.01960784, -0.1607843 ],\n           [-0.10588235,  0.07450986, -0.05098039],\n           [ 0.05098045,  0.2313726 ,  0.10588241]],\n  \n          [[ 0.3411765 ,  0.5137255 ,  0.41176474],\n           [ 0.34901965,  0.52156866,  0.41960788],\n           [ 0.34901965,  0.52156866,  0.41960788],\n           ...,\n           [-0.09803921,  0.09019613, -0.08235294],\n           [ 0.0196079 ,  0.20000005,  0.04313731],\n           [ 0.17647064,  0.35686278,  0.21568632]],\n  \n          ...,\n  \n          [[ 0.6156863 ,  0.7490196 ,  0.60784316],\n           [ 0.60784316,  0.7411765 ,  0.6       ],\n           [ 0.6       ,  0.73333335,  0.60784316],\n           ...,\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.6862745 ,  0.8352941 ,  0.6862745 ],\n           [ 0.654902  ,  0.8039216 ,  0.654902  ],\n           [ 0.6313726 ,  0.7647059 ,  0.6392157 ],\n           ...,\n           [ 0.654902  ,  0.84313726,  0.7411765 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276]],\n  \n          [[ 0.7411765 ,  0.8901961 ,  0.7411765 ],\n           [ 0.70980394,  0.85882354,  0.70980394],\n           [ 0.6627451 ,  0.79607844,  0.67058825],\n           ...,\n           [ 0.654902  ,  0.84313726,  0.7411765 ],\n           [ 0.6627451 ,  0.8509804 ,  0.7490196 ],\n           [ 0.67058825,  0.85882354,  0.75686276]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.4352941 , -0.36470586, -0.5764706 ],\n           [-0.4352941 , -0.36470586, -0.5764706 ],\n           [-0.4352941 , -0.36470586, -0.5764706 ],\n           ...,\n           [-0.27058822, -0.19215685, -0.18431371],\n           [-0.26274508, -0.17647058, -0.1607843 ],\n           [-0.25490195, -0.16862744, -0.15294117]],\n  \n          [[-0.41176468, -0.34117645, -0.5529412 ],\n           [-0.41176468, -0.34117645, -0.5529412 ],\n           [-0.41176468, -0.34117645, -0.5529412 ],\n           ...,\n           [-0.2862745 , -0.19215685, -0.19215685],\n           [-0.23137254, -0.14509803, -0.12941176],\n           [-0.20784312, -0.09803921, -0.09019607]],\n  \n          [[-0.38823527, -0.31764704, -0.5294118 ],\n           [-0.38823527, -0.31764704, -0.5294118 ],\n           [-0.38039213, -0.3098039 , -0.52156866],\n           ...,\n           [-0.2235294 , -0.11372548, -0.11372548],\n           [-0.19999999, -0.09019607, -0.08235294],\n           [-0.19215685, -0.06666666, -0.06666666]],\n  \n          ...,\n  \n          [[-0.38039213, -0.3098039 , -0.47450978],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           ...,\n           [-0.27058822, -0.20784312, -0.3960784 ],\n           [-0.2862745 , -0.2235294 , -0.41176468],\n           [-0.2862745 , -0.2235294 , -0.41176468]],\n  \n          [[-0.38823527, -0.31764704, -0.4823529 ],\n           [-0.38823527, -0.31764704, -0.4823529 ],\n           [-0.38039213, -0.3098039 , -0.47450978],\n           ...,\n           [-0.29411763, -0.23137254, -0.41960782],\n           [-0.30196077, -0.23921567, -0.42745095],\n           [-0.30196077, -0.23921567, -0.42745095]],\n  \n          [[-0.3960784 , -0.32549018, -0.49019605],\n           [-0.3960784 , -0.32549018, -0.49019605],\n           [-0.38823527, -0.31764704, -0.4823529 ],\n           ...,\n           [-0.3098039 , -0.24705881, -0.4352941 ],\n           [-0.30196077, -0.23921567, -0.42745095],\n           [-0.2862745 , -0.2235294 , -0.41176468]]],\n  \n  \n         [[[-0.38823527, -0.25490195, -0.3333333 ],\n           [-0.32549018, -0.19215685, -0.27058822],\n           [-0.35686272, -0.2235294 , -0.30196077],\n           ...,\n           [-0.75686276, -0.5921569 , -0.70980394],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.77254903, -0.60784316, -0.7254902 ]],\n  \n          [[-0.38039213, -0.24705881, -0.32549018],\n           [-0.3490196 , -0.21568626, -0.29411763],\n           [-0.3960784 , -0.26274508, -0.34117645],\n           ...,\n           [-0.75686276, -0.5921569 , -0.70980394],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.78039217, -0.6156863 , -0.73333335]],\n  \n          [[-0.36470586, -0.23921567, -0.31764704],\n           [-0.372549  , -0.24705881, -0.32549018],\n           [-0.45098037, -0.32549018, -0.40392154],\n           ...,\n           [-0.7647059 , -0.6       , -0.7176471 ],\n           [-0.77254903, -0.60784316, -0.7254902 ],\n           [-0.78039217, -0.6156863 , -0.73333335]],\n  \n          ...,\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.6862745 , -0.5764706 , -0.69411767],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.69411767, -0.56078434, -0.6862745 ]],\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.69411767, -0.58431375, -0.7019608 ],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.69411767, -0.56078434, -0.6862745 ]],\n  \n          [[-0.70980394, -0.64705884, -0.73333335],\n           [-0.69411767, -0.6313726 , -0.7176471 ],\n           [-0.6862745 , -0.62352943, -0.70980394],\n           ...,\n           [-0.69411767, -0.58431375, -0.7019608 ],\n           [-0.7019608 , -0.5686275 , -0.69411767],\n           [-0.6862745 , -0.5529412 , -0.6784314 ]]],\n  \n  \n         [[[-0.78039217, -0.54509807, -0.62352943],\n           [-0.79607844, -0.56078434, -0.6392157 ],\n           [-0.8117647 , -0.5764706 , -0.654902  ],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          [[-0.75686276, -0.52156866, -0.6       ],\n           [-0.78039217, -0.54509807, -0.62352943],\n           [-0.79607844, -0.56078434, -0.6392157 ],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          [[-0.73333335, -0.4980392 , -0.5764706 ],\n           [-0.75686276, -0.52156866, -0.6       ],\n           [-0.7647059 , -0.5529412 , -0.62352943],\n           ...,\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.96862745, -0.85882354, -0.99215686],\n           [-0.9764706 , -0.8666667 , -1.        ]],\n  \n          ...,\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.7019608 , -0.6313726 , -0.78039217],\n           [-0.70980394, -0.6392157 , -0.7882353 ]],\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.7019608 , -0.6313726 , -0.78039217],\n           [-0.70980394, -0.6392157 , -0.7882353 ]],\n  \n          [[-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           [-0.7490196 , -0.6627451 , -0.79607844],\n           ...,\n           [-0.69411767, -0.62352943, -0.77254903],\n           [-0.70980394, -0.6392157 , -0.7882353 ],\n           [-0.70980394, -0.6392157 , -0.7882353 ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n         1, 0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.79607844, -0.67058825, -0.75686276],\n           [-0.8039216 , -0.6784314 , -0.7647059 ],\n           [-0.8039216 , -0.6784314 , -0.7647059 ],\n           ...,\n           [-0.5764706 , -0.45098037, -0.5372549 ],\n\n*** WARNING: skipped 567691 bytes of output ***\n\n           [-0.5058824 , -0.3960784 , -0.5921569 ],\n           [-0.5058824 , -0.3960784 , -0.5921569 ],\n           [-0.4980392 , -0.38823527, -0.58431375]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n         0, 1, 1, 0, 0, 0, 1, 0, 1, 1], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(32, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-0.1372549 , -0.03529412, -0.16862744],\n           [-0.19999999, -0.09803921, -0.23137254],\n           [-0.19999999, -0.11372548, -0.23921567],\n           ...,\n           [-0.08235294,  0.12941182,  0.07450986],\n           [-0.04313725,  0.1686275 ,  0.11372554],\n           [-0.01176471,  0.20000005,  0.14509809]],\n  \n          [[-0.05882353,  0.03529418, -0.0745098 ],\n           [-0.14509803, -0.04313725, -0.17647058],\n           [-0.19999999, -0.11372548, -0.23921567],\n           ...,\n           [-0.05882353,  0.15294123,  0.09803927],\n           [-0.01960784,  0.19215691,  0.13725495],\n           [ 0.01176476,  0.22352946,  0.1686275 ]],\n  \n          [[ 0.00392163,  0.09803927,  0.00392163],\n           [-0.09019607,  0.00392163, -0.10588235],\n           [-0.19999999, -0.11372548, -0.2235294 ],\n           ...,\n           [-0.02745098,  0.18431377,  0.12941182],\n           [ 0.0196079 ,  0.2313726 ,  0.17647064],\n           [ 0.05098045,  0.26274514,  0.20784318]],\n  \n          ...,\n  \n          [[-0.38823527, -0.34117645, -0.45098037],\n           [-0.38823527, -0.34117645, -0.45098037],\n           [-0.38823527, -0.34117645, -0.45098037],\n           ...,\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.25490195, -0.21568626, -0.372549  ],\n           [-0.25490195, -0.21568626, -0.372549  ]],\n  \n          [[-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.38823527, -0.34117645, -0.45098037],\n           ...,\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.24705881, -0.20784312, -0.36470586],\n           [-0.24705881, -0.20784312, -0.36470586]],\n  \n          [[-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           [-0.3960784 , -0.3490196 , -0.4588235 ],\n           ...,\n           [-0.27843136, -0.23921567, -0.3960784 ],\n           [-0.27843136, -0.23921567, -0.3960784 ],\n           [-0.27843136, -0.23921567, -0.3960784 ]]],\n  \n  \n         [[[-0.4980392 , -0.4352941 , -0.6       ],\n           [-0.4980392 , -0.4352941 , -0.6       ],\n           [-0.4980392 , -0.4352941 , -0.6       ],\n           ...,\n           [-0.41960782, -0.30196077, -0.35686272],\n           [-0.41176468, -0.2862745 , -0.36470586],\n           [-0.41176468, -0.2862745 , -0.36470586]],\n  \n          [[-0.47450978, -0.41176468, -0.5764706 ],\n           [-0.4823529 , -0.41960782, -0.58431375],\n           [-0.4823529 , -0.41960782, -0.58431375],\n           ...,\n           [-0.41176468, -0.29411763, -0.3490196 ],\n           [-0.3490196 , -0.23137254, -0.2862745 ],\n           [-0.3098039 , -0.18431371, -0.26274508]],\n  \n          [[-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           [-0.45098037, -0.38823527, -0.5529412 ],\n           ...,\n           [-0.3490196 , -0.21568626, -0.26274508],\n           [-0.31764704, -0.18431371, -0.23137254],\n           [-0.29411763, -0.1607843 , -0.2235294 ]],\n  \n          ...,\n  \n          [[-0.44313723, -0.3490196 , -0.5058824 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           ...,\n           [-0.3333333 , -0.27058822, -0.4352941 ],\n           [-0.34117645, -0.27843136, -0.44313723],\n           [-0.34117645, -0.27843136, -0.44313723]],\n  \n          [[-0.45098037, -0.35686272, -0.5137255 ],\n           [-0.45098037, -0.35686272, -0.5137255 ],\n           [-0.44313723, -0.3490196 , -0.5058824 ],\n           ...,\n           [-0.3490196 , -0.2862745 , -0.45098037],\n           [-0.3490196 , -0.2862745 , -0.45098037],\n           [-0.34117645, -0.27843136, -0.44313723]],\n  \n          [[-0.4588235 , -0.36470586, -0.52156866],\n           [-0.4588235 , -0.36470586, -0.52156866],\n           [-0.45098037, -0.35686272, -0.5137255 ],\n           ...,\n           [-0.36470586, -0.30196077, -0.46666664],\n           [-0.35686272, -0.29411763, -0.4588235 ],\n           [-0.34117645, -0.27843136, -0.44313723]]],\n  \n  \n         [[[ 0.13725495,  0.28627455,  0.12941182],\n           [ 0.13725495,  0.28627455,  0.12941182],\n           [ 0.14509809,  0.2941177 ,  0.13725495],\n           ...,\n           [-0.5372549 , -0.41176468, -0.4980392 ],\n           [-0.5372549 , -0.41176468, -0.49019605],\n           [-0.5372549 , -0.41176468, -0.49019605]],\n  \n          [[ 0.12941182,  0.27843142,  0.12156868],\n           [ 0.12941182,  0.27843142,  0.12156868],\n           [ 0.13725495,  0.28627455,  0.12941182],\n           ...,\n           [-0.52156866, -0.41176468, -0.5137255 ],\n           [-0.52156866, -0.41176468, -0.5137255 ],\n           [-0.52156866, -0.41176468, -0.5137255 ]],\n  \n          [[ 0.12156868,  0.254902  ,  0.10588241],\n           [ 0.12941182,  0.26274514,  0.11372554],\n           [ 0.13725495,  0.27058828,  0.12156868],\n           ...,\n           [-0.49019605, -0.41960782, -0.5529412 ],\n           [-0.49019605, -0.41960782, -0.5529412 ],\n           [-0.49019605, -0.41960782, -0.5529412 ]],\n  \n          ...,\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ]],\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ]],\n  \n          [[-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5137255 , -0.41960782, -0.5764706 ],\n           [-0.5058824 , -0.41176468, -0.5686275 ],\n           ...,\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.4823529 , -0.372549  , -0.5529412 ],\n           [-0.49019605, -0.38039213, -0.56078434]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.44313723, -0.41176468, -0.5921569 ],\n           [-0.44313723, -0.41176468, -0.5921569 ],\n           [-0.44313723, -0.41176468, -0.5921569 ],\n           ...,\n           [-0.38823527, -0.27058822, -0.3098039 ],\n           [-0.38039213, -0.26274508, -0.2862745 ],\n           [-0.38039213, -0.26274508, -0.2862745 ]],\n  \n          [[-0.42745095, -0.3960784 , -0.5764706 ],\n           [-0.4352941 , -0.40392154, -0.58431375],\n           [-0.4352941 , -0.40392154, -0.58431375],\n           ...,\n           [-0.38039213, -0.26274508, -0.2862745 ],\n           [-0.3333333 , -0.21568626, -0.23921567],\n           [-0.3098039 , -0.19215685, -0.21568626]],\n  \n          [[-0.41176468, -0.38039213, -0.56078434],\n           [-0.41176468, -0.38039213, -0.56078434],\n           [-0.41176468, -0.38039213, -0.56078434],\n           ...,\n           [-0.30196077, -0.17647058, -0.18431371],\n           [-0.29411763, -0.16862744, -0.17647058],\n           [-0.2862745 , -0.1607843 , -0.16862744]],\n  \n          ...,\n  \n          [[-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.29411763, -0.23137254, -0.3960784 ],\n           [-0.3098039 , -0.24705881, -0.41176468],\n           [-0.31764704, -0.25490195, -0.41960782]],\n  \n          [[-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41176468, -0.31764704, -0.47450978],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.31764704, -0.25490195, -0.41960782]],\n  \n          [[-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41960782, -0.32549018, -0.4823529 ],\n           [-0.41176468, -0.31764704, -0.47450978],\n           ...,\n           [-0.31764704, -0.25490195, -0.41960782],\n           [-0.30196077, -0.23921567, -0.40392154],\n           [-0.29411763, -0.23137254, -0.3960784 ]]],\n  \n  \n         [[[-0.3333333 , -0.27058822, -0.38823527],\n           [-0.3333333 , -0.27058822, -0.38823527],\n           [-0.3333333 , -0.27058822, -0.38823527],\n           ...,\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ]],\n  \n          [[-0.34117645, -0.27843136, -0.3960784 ],\n           [-0.34117645, -0.27843136, -0.3960784 ],\n           [-0.34117645, -0.27843136, -0.3960784 ],\n           ...,\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ],\n           [-0.41960782, -0.3960784 , -0.5294118 ]],\n  \n          [[-0.3490196 , -0.2862745 , -0.40392154],\n           [-0.3490196 , -0.2862745 , -0.40392154],\n           [-0.3490196 , -0.2862745 , -0.40392154],\n           ...,\n           [-0.42745095, -0.40392154, -0.5372549 ],\n           [-0.42745095, -0.40392154, -0.5372549 ],\n           [-0.42745095, -0.40392154, -0.5372549 ]],\n  \n          ...,\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]],\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]],\n  \n          [[-0.4823529 , -0.41960782, -0.5058824 ],\n           [-0.47450978, -0.41176468, -0.4980392 ],\n           [-0.46666664, -0.40392154, -0.49019605],\n           ...,\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375],\n           [-0.56078434, -0.47450978, -0.58431375]]],\n  \n  \n         [[[-1.        , -0.8901961 , -0.6784314 ],\n           [-0.99215686, -0.88235295, -0.654902  ],\n           [-0.9843137 , -0.8901961 , -0.5921569 ],\n           ...,\n           [-0.8509804 , -0.7882353 , -0.90588236],\n           [-0.85882354, -0.79607844, -0.9137255 ],\n           [-0.8666667 , -0.8039216 , -0.92156863]],\n  \n          [[-1.        , -0.90588236, -0.7019608 ],\n           [-1.        , -0.90588236, -0.69411767],\n           [-1.        , -0.90588236, -0.6392157 ],\n           ...,\n           [-0.8352941 , -0.77254903, -0.8901961 ],\n           [-0.84313726, -0.78039217, -0.8980392 ],\n           [-0.8509804 , -0.7882353 , -0.90588236]],\n  \n          [[-1.        , -0.92941177, -0.78039217],\n           [-1.        , -0.92941177, -0.7411765 ],\n           [-1.        , -0.92941177, -0.69411767],\n           ...,\n           [-0.827451  , -0.7411765 , -0.8666667 ],\n           [-0.8352941 , -0.7490196 , -0.8745098 ],\n           [-0.84313726, -0.75686276, -0.88235295]],\n  \n          ...,\n  \n          [[-0.654902  , -0.58431375, -0.7176471 ],\n           [-0.6156863 , -0.54509807, -0.6784314 ],\n           [-0.6392157 , -0.58431375, -0.70980394],\n           ...,\n           [-0.8901961 , -0.8666667 , -1.        ],\n           [-0.88235295, -0.85882354, -0.99215686],\n           [-0.8745098 , -0.8509804 , -1.        ]],\n  \n          [[-0.69411767, -0.62352943, -0.75686276],\n           [-0.6156863 , -0.54509807, -0.6784314 ],\n           [-0.6       , -0.54509807, -0.67058825],\n           ...,\n           [-0.88235295, -0.85882354, -1.        ],\n           [-0.8745098 , -0.8509804 , -1.        ],\n           [-0.8666667 , -0.84313726, -1.        ]],\n  \n          [[-0.79607844, -0.7254902 , -0.85882354],\n           [-0.6862745 , -0.6156863 , -0.7490196 ],\n           [-0.64705884, -0.5921569 , -0.7176471 ],\n           ...,\n           [-0.88235295, -0.85882354, -1.        ],\n           [-0.8745098 , -0.8509804 , -1.        ],\n           [-0.8666667 , -0.84313726, -1.        ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(32,), dtype=int32, numpy=\n  array([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n         0, 0, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int32)&gt;),\n (&lt;tf.Tensor: shape=(14, 120, 160, 3), dtype=float32, numpy=\n  array([[[[-1.        , -0.8980392 , -0.6627451 ],\n           [-1.        , -0.8901961 , -0.6392157 ],\n           [-1.        , -0.8980392 , -0.5921569 ],\n           ...,\n           [-0.88235295, -0.7254902 , -0.81960785],\n           [-0.8901961 , -0.73333335, -0.827451  ],\n           [-0.8901961 , -0.73333335, -0.827451  ]],\n  \n          [[-1.        , -0.90588236, -0.6862745 ],\n           [-1.        , -0.90588236, -0.67058825],\n           [-1.        , -0.8980392 , -0.6156863 ],\n           ...,\n           [-0.8666667 , -0.70980394, -0.8039216 ],\n           [-0.8745098 , -0.7176471 , -0.8117647 ],\n           [-0.88235295, -0.7254902 , -0.81960785]],\n  \n          [[-1.        , -0.92156863, -0.7411765 ],\n           [-1.        , -0.9137255 , -0.7176471 ],\n           [-1.        , -0.9137255 , -0.67058825],\n           ...,\n           [-0.8509804 , -0.69411767, -0.7882353 ],\n           [-0.85882354, -0.7019608 , -0.79607844],\n           [-0.85882354, -0.7019608 , -0.79607844]],\n  \n          ...,\n  \n          [[-0.8901961 , -0.7882353 , -0.8666667 ],\n           [-0.7647059 , -0.6627451 , -0.7411765 ],\n           [-0.6156863 , -0.52156866, -0.6156863 ],\n           ...,\n           [-0.8745098 , -0.8117647 , -0.9764706 ],\n           [-0.88235295, -0.827451  , -0.96862745],\n           [-0.88235295, -0.81960785, -0.9843137 ]],\n  \n          [[-0.8901961 , -0.7882353 , -0.8666667 ],\n           [-0.79607844, -0.69411767, -0.77254903],\n           [-0.69411767, -0.58431375, -0.6862745 ],\n           ...,\n           [-0.8666667 , -0.8039216 , -0.9843137 ],\n           [-0.8666667 , -0.8039216 , -0.96862745],\n           [-0.8666667 , -0.8039216 , -0.9843137 ]],\n  \n          [[-0.8509804 , -0.7490196 , -0.827451  ],\n           [-0.8117647 , -0.70980394, -0.7882353 ],\n           [-0.75686276, -0.64705884, -0.7490196 ],\n           ...,\n           [-0.88235295, -0.81960785, -1.        ],\n           [-0.88235295, -0.81960785, -1.        ],\n           [-0.88235295, -0.81960785, -1.        ]]],\n  \n  \n         [[[-0.44313723, -0.3098039 , -0.38823527],\n           [-0.44313723, -0.3098039 , -0.38823527],\n           [-0.4352941 , -0.30196077, -0.38039213],\n           ...,\n           [-0.40392154, -0.26274508, -0.4352941 ],\n           [-0.41176468, -0.27058822, -0.44313723],\n           [-0.41176468, -0.27058822, -0.44313723]],\n  \n          [[-0.46666664, -0.3333333 , -0.41176468],\n           [-0.46666664, -0.3333333 , -0.41176468],\n           [-0.4588235 , -0.32549018, -0.40392154],\n           ...,\n           [-0.41176468, -0.27058822, -0.44313723],\n           [-0.42745095, -0.29411763, -0.44313723],\n           [-0.4352941 , -0.29411763, -0.46666664]],\n  \n          [[-0.4823529 , -0.35686272, -0.44313723],\n           [-0.4823529 , -0.35686272, -0.44313723],\n           [-0.47450978, -0.3490196 , -0.4352941 ],\n           ...,\n           [-0.41960782, -0.2862745 , -0.4352941 ],\n           [-0.45098037, -0.31764704, -0.4588235 ],\n           [-0.4588235 , -0.32549018, -0.47450978]],\n  \n          ...,\n  \n          [[-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.5921569 , -0.69411767],\n           [-0.7019608 , -0.5921569 , -0.69411767],\n           ...,\n           [-0.5294118 , -0.45098037, -0.52156866],\n           [-0.52156866, -0.46666664, -0.5294118 ],\n           [-0.5372549 , -0.4588235 , -0.5294118 ]],\n  \n          [[-0.7019608 , -0.5921569 , -0.69411767],\n           [-0.7019608 , -0.5921569 , -0.70980394],\n           [-0.7019608 , -0.5921569 , -0.70980394],\n           ...,\n           [-0.44313723, -0.38039213, -0.46666664],\n           [-0.4352941 , -0.38823527, -0.46666664],\n           [-0.45098037, -0.38823527, -0.47450978]],\n  \n          [[-0.7019608 , -0.5921569 , -0.70980394],\n           [-0.70980394, -0.6       , -0.7176471 ],\n           [-0.70980394, -0.6       , -0.7176471 ],\n           ...,\n           [-0.36470586, -0.31764704, -0.3960784 ],\n           [-0.36470586, -0.31764704, -0.3960784 ],\n           [-0.36470586, -0.31764704, -0.3960784 ]]],\n  \n  \n         [[[-0.77254903, -0.6627451 , -0.78039217],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           ...,\n           [-0.5686275 , -0.4588235 , -0.56078434],\n           [-0.54509807, -0.4352941 , -0.5372549 ],\n           [-0.52156866, -0.41176468, -0.5137255 ]],\n  \n          [[-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           [-0.7647059 , -0.654902  , -0.77254903],\n           ...,\n           [-0.58431375, -0.47450978, -0.5764706 ],\n           [-0.56078434, -0.45098037, -0.5529412 ],\n           [-0.54509807, -0.4352941 , -0.5372549 ]],\n  \n          [[-0.75686276, -0.64705884, -0.7647059 ],\n           [-0.75686276, -0.64705884, -0.7647059 ],\n           [-0.75686276, -0.64705884, -0.7647059 ],\n           ...,\n           [-0.60784316, -0.4980392 , -0.6       ],\n           [-0.5921569 , -0.4823529 , -0.58431375],\n           [-0.5764706 , -0.46666664, -0.5686275 ]],\n  \n          ...,\n  \n          [[-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7176471 , -0.6313726 , -0.7647059 ],\n           ...,\n           [-0.84313726, -0.73333335, -0.9137255 ],\n           [-0.84313726, -0.73333335, -0.9137255 ],\n           [-0.84313726, -0.73333335, -0.92941177]],\n  \n          [[-0.7254902 , -0.6313726 , -0.7882353 ],\n           [-0.7176471 , -0.62352943, -0.78039217],\n           [-0.70980394, -0.62352943, -0.75686276],\n           ...,\n           [-0.85882354, -0.7411765 , -0.92156863],\n           [-0.85882354, -0.7411765 , -0.92156863],\n           [-0.85882354, -0.73333335, -0.9372549 ]],\n  \n          [[-0.7176471 , -0.62352943, -0.78039217],\n           [-0.7176471 , -0.62352943, -0.78039217],\n           [-0.70980394, -0.62352943, -0.75686276],\n           ...,\n           [-0.8666667 , -0.7490196 , -0.92941177],\n           [-0.8666667 , -0.7490196 , -0.92941177],\n           [-0.85882354, -0.73333335, -0.9372549 ]]],\n  \n  \n         ...,\n  \n  \n         [[[-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.5686275 , -0.41176468, -0.4980392 ],\n           [-0.5529412 , -0.3960784 , -0.4823529 ],\n           [-0.54509807, -0.38823527, -0.47450978]],\n  \n          [[-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.56078434, -0.40392154, -0.49019605],\n           [-0.5529412 , -0.3960784 , -0.4823529 ],\n           [-0.5529412 , -0.3960784 , -0.4823529 ]],\n  \n          [[-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.70980394, -0.60784316, -0.6862745 ],\n           [-0.7019608 , -0.6       , -0.6784314 ],\n           ...,\n           [-0.58431375, -0.42745095, -0.5137255 ],\n           [-0.58431375, -0.42745095, -0.5137255 ],\n           [-0.58431375, -0.42745095, -0.5137255 ]],\n  \n          ...,\n  \n          [[-0.6       , -0.5137255 , -0.62352943],\n           [-0.6       , -0.5137255 , -0.62352943],\n           [-0.6       , -0.5137255 , -0.62352943],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]],\n  \n          [[-0.5921569 , -0.5058824 , -0.6156863 ],\n           [-0.5921569 , -0.5058824 , -0.6156863 ],\n           [-0.5921569 , -0.5058824 , -0.6156863 ],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]],\n  \n          [[-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           [-0.58431375, -0.4980392 , -0.60784316],\n           ...,\n           [-0.70980394, -0.60784316, -0.7411765 ],\n           [-0.70980394, -0.60784316, -0.7490196 ],\n           [-0.7176471 , -0.6156863 , -0.75686276]]],\n  \n  \n         [[[ 0.4039216 ,  0.5372549 ,  0.45882356],\n           [ 0.427451  ,  0.56078434,  0.48235297],\n           [ 0.45098042,  0.58431375,  0.5058824 ],\n           ...,\n           [-0.04313725,  0.12156868,  0.00392163],\n           [-0.0745098 ,  0.09019613, -0.02745098],\n           [-0.01176471,  0.15294123,  0.03529418]],\n  \n          [[ 0.38823533,  0.52156866,  0.4431373 ],\n           [ 0.4039216 ,  0.5372549 ,  0.45882356],\n           [ 0.427451  ,  0.56078434,  0.48235297],\n           ...,\n           [-0.0745098 ,  0.09019613, -0.02745098],\n           [-0.08235294,  0.082353  , -0.03529412],\n           [-0.04313725,  0.12156868,  0.00392163]],\n  \n          [[ 0.35686278,  0.4901961 ,  0.41176474],\n           [ 0.3803922 ,  0.5137255 ,  0.43529415],\n           [ 0.4039216 ,  0.5372549 ,  0.45882356],\n           ...,\n           [-0.12156862,  0.04313731, -0.0745098 ],\n           [-0.10588235,  0.05882359, -0.05882353],\n           [-0.08235294,  0.082353  , -0.03529412]],\n  \n          ...,\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.77254903,  0.9764706 ,  0.8745098 ],\n           [ 0.7647059 ,  0.9843137 ,  0.8745098 ],\n           [ 0.7647059 ,  0.9843137 ,  0.8745098 ]],\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.75686276,  0.9607843 ,  0.85882354],\n           [ 0.7490196 ,  0.96862745,  0.85882354],\n           [ 0.7490196 ,  0.96862745,  0.85882354]],\n  \n          [[ 0.73333335,  0.8509804 ,  0.79607844],\n           [ 0.7411765 ,  0.85882354,  0.8039216 ],\n           [ 0.75686276,  0.85882354,  0.8039216 ],\n           ...,\n           [ 0.7490196 ,  0.9529412 ,  0.8509804 ],\n           [ 0.7411765 ,  0.9607843 ,  0.8509804 ],\n           [ 0.7411765 ,  0.9607843 ,  0.8509804 ]]],\n  \n  \n         [[[-0.4352941 , -0.32549018, -0.44313723],\n           [-0.4352941 , -0.32549018, -0.44313723],\n           [-0.42745095, -0.31764704, -0.4352941 ],\n           ...,\n           [-0.40392154, -0.29411763, -0.42745095],\n           [-0.41176468, -0.30196077, -0.4352941 ],\n           [-0.41176468, -0.30196077, -0.4352941 ]],\n  \n          [[-0.44313723, -0.3333333 , -0.45098037],\n           [-0.44313723, -0.3333333 , -0.45098037],\n           [-0.4352941 , -0.32549018, -0.44313723],\n           ...,\n           [-0.41176468, -0.30196077, -0.4352941 ],\n           [-0.42745095, -0.31764704, -0.45098037],\n           [-0.4352941 , -0.32549018, -0.4588235 ]],\n  \n          [[-0.4588235 , -0.3490196 , -0.46666664],\n           [-0.45098037, -0.34117645, -0.4588235 ],\n           [-0.44313723, -0.3333333 , -0.4352941 ],\n           ...,\n           [-0.41176468, -0.30196077, -0.41960782],\n           [-0.4352941 , -0.32549018, -0.4588235 ],\n           [-0.45098037, -0.34117645, -0.47450978]],\n  \n          ...,\n  \n          [[-0.5686275 , -0.46666664, -0.60784316],\n           [-0.5529412 , -0.45098037, -0.5921569 ],\n           [-0.5372549 , -0.4352941 , -0.5764706 ],\n           ...,\n           [-0.4980392 , -0.41960782, -0.5058824 ],\n           [-0.49019605, -0.4352941 , -0.4980392 ],\n           [-0.5137255 , -0.4352941 , -0.5058824 ]],\n  \n          [[-0.5686275 , -0.46666664, -0.6       ],\n           [-0.5529412 , -0.45098037, -0.58431375],\n           [-0.5372549 , -0.4352941 , -0.5686275 ],\n           ...,\n           [-0.41960782, -0.35686272, -0.44313723],\n           [-0.42745095, -0.38823527, -0.44313723],\n           [-0.45098037, -0.3960784 , -0.4588235 ]],\n  \n          [[-0.5686275 , -0.46666664, -0.6       ],\n           [-0.5529412 , -0.45098037, -0.58431375],\n           [-0.5372549 , -0.4352941 , -0.5686275 ],\n           ...,\n           [-0.3490196 , -0.30196077, -0.38039213],\n           [-0.36470586, -0.32549018, -0.38039213],\n           [-0.38039213, -0.34117645, -0.3960784 ]]]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(14,), dtype=int32, numpy=array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=int32)&gt;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_model(lr=0.001, alpha=0.35):\n  base_model = tf.keras.applications.MobileNetV2(\n      input_shape=(img_height, img_width, 3),\n      include_top=False, weights='imagenet', alpha=alpha)\n  #     base_model.trainable = False\n  base_model.trainable = True\n  x = base_model.output\n  # Add a new classifier layer for transfer learning\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  x = tf.keras.layers.Dropout(0.2)(x)\n  x = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n  #     prediction_layer = tf.keras.layers.Dense(1, activation='softmax')\n  model = tf.keras.Model(inputs=base_model.inputs, outputs=x)\n  return model\n\ndef get_compiled_model(alpha=0.35, lr=0.001):\n  model = get_model(lr, alpha)\n  #     lr = learning_rate = CustomSchedule(2)\n  model.compile(\n              optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9),\n  #                 optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n  return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efb76772-5ba0-4868-9d28-45bd917b4836"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(train_ds, val_ds, lr=0.001):\n    model = get_compiled_model()\n#     model = get_compiled_model_2()\n    checkpoint_path = \"/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-{epoch:04d}.h5py\"\n    checkpoint_dir = os.path.dirname(checkpoint_path)\n    \n    model.save_weights(checkpoint_path.format(epoch=0))\n\n    # \n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                     save_weights_only=True,\n                                                     verbose=1)\n#                                                      save_freq=5)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n    board_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=log_dir,\n        histogram_freq=0,  # How often to log histogram visualizations\n        embeddings_freq=0,  # How often to log embedding visualizations\n        update_freq=\"epoch\",\n    )  # How often to write logs (default: once per epoch)\n    \n#     steps_per_epoch = len(train_ds) // batch_size\n    hist = model.fit(train_ds, \n#                      steps_per_epoch=steps_per_epoch,\n                     epochs=NUM_EPOCHS,\n                     validation_data=val_ds,\n#                      class_weight={0:0.3, 1:0.7},\n#                      validation_steps=validation_steps,\n                     verbose=2,\n                     callbacks=[\n                       cp_callback, \n                       board_callback, \n                       early_stop,\n#                        CustomLearningRateScheduler(lr_schedule),\n                     ])\n#     model.save('saved_model/my_model')\n    return hist,model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36b6f893-7f0c-41ec-b049-985d42543ba7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["gpus = tf.config.experimental.list_physical_devices('GPU')\nprint(gpus)\nif gpus:\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n  try:\n    for gpu in gpus:\n#       tf.config.experimental.set_memory_growth(gpu, True)\n      print('yes')\n#     tf.config.experimental.set_virtual_device_configuration(\n#         gpus[0],\n#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e02e922-b7c8-4368-af37-78a199f4b31c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]\nyes\nyes\n2 Physical GPUs, 2 Logical GPUs\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)]\nyes\nyes\n2 Physical GPUs, 2 Logical GPUs\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["tf.debugging.set_log_device_placement(True)\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n  hist,model = train_and_evaluate(train_ds, val_ds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d82f6f97-cdd4-41dc-afec-12b0b69e0a34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO:tensorflow:Using MirroredStrategy with devices (&#39;/job:localhost/replica:0/task:0/device:GPU:0&#39;, &#39;/job:localhost/replica:0/task:0/device:GPU:1&#39;)\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_224_no_top.h5\n\r   8192/2019640 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2023424/2019640 [==============================] - 0s 0us/step\nEpoch 1/120\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse `tf.profiler.experimental.stop` instead.\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0781s vs `on_train_batch_end` time: 0.3532s). Check your callbacks.\n\nEpoch 00001: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0001.h5py\n198/198 - 83s - loss: 0.5804 - accuracy: 0.7107 - val_loss: 0.5672 - val_accuracy: 0.7295\nEpoch 2/120\n\nEpoch 00002: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0002.h5py\n198/198 - 14s - loss: 0.4869 - accuracy: 0.8189 - val_loss: 0.4890 - val_accuracy: 0.8217\nEpoch 3/120\n\nEpoch 00003: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0003.h5py\n198/198 - 14s - loss: 0.4443 - accuracy: 0.8663 - val_loss: 0.4551 - val_accuracy: 0.8565\nEpoch 4/120\n\nEpoch 00004: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0004.h5py\n198/198 - 14s - loss: 0.4265 - accuracy: 0.8842 - val_loss: 0.4366 - val_accuracy: 0.8717\nEpoch 5/120\n\nEpoch 00005: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0005.h5py\n198/198 - 14s - loss: 0.4087 - accuracy: 0.9028 - val_loss: 0.4188 - val_accuracy: 0.8932\nEpoch 6/120\n\nEpoch 00006: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0006.h5py\n198/198 - 14s - loss: 0.3981 - accuracy: 0.9126 - val_loss: 0.4788 - val_accuracy: 0.8281\nEpoch 7/120\n\nEpoch 00007: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0007.h5py\n198/198 - 14s - loss: 0.3938 - accuracy: 0.9172 - val_loss: 0.4132 - val_accuracy: 0.8944\nEpoch 8/120\n\nEpoch 00008: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0008.h5py\n198/198 - 14s - loss: 0.3858 - accuracy: 0.9260 - val_loss: 0.3952 - val_accuracy: 0.9166\nEpoch 9/120\n\nEpoch 00009: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0009.h5py\n198/198 - 14s - loss: 0.3796 - accuracy: 0.9325 - val_loss: 0.3767 - val_accuracy: 0.9349\nEpoch 10/120\n\nEpoch 00010: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0010.h5py\n198/198 - 14s - loss: 0.3763 - accuracy: 0.9360 - val_loss: 0.3675 - val_accuracy: 0.9425\nEpoch 11/120\n\nEpoch 00011: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0011.h5py\n198/198 - 14s - loss: 0.3756 - accuracy: 0.9366 - val_loss: 0.3865 - val_accuracy: 0.9254\nEpoch 12/120\n\nEpoch 00012: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0012.h5py\n198/198 - 15s - loss: 0.3696 - accuracy: 0.9420 - val_loss: 0.3698 - val_accuracy: 0.9399\nEpoch 13/120\n\nEpoch 00013: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0013.h5py\n198/198 - 15s - loss: 0.3666 - accuracy: 0.9460 - val_loss: 0.3724 - val_accuracy: 0.9362\nEpoch 14/120\n\nEpoch 00014: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0014.h5py\n198/198 - 14s - loss: 0.3673 - accuracy: 0.9453 - val_loss: 0.3879 - val_accuracy: 0.9254\nEpoch 15/120\n\nEpoch 00015: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0015.h5py\n198/198 - 14s - loss: 0.3639 - accuracy: 0.9479 - val_loss: 0.3618 - val_accuracy: 0.9513\nEpoch 16/120\n\nEpoch 00016: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0016.h5py\n198/198 - 15s - loss: 0.3610 - accuracy: 0.9513 - val_loss: 0.3519 - val_accuracy: 0.9595\nEpoch 17/120\n\nEpoch 00017: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0017.h5py\n198/198 - 15s - loss: 0.3593 - accuracy: 0.9515 - val_loss: 0.3509 - val_accuracy: 0.9608\nEpoch 18/120\n\nEpoch 00018: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0018.h5py\n198/198 - 14s - loss: 0.3596 - accuracy: 0.9523 - val_loss: 0.3495 - val_accuracy: 0.9633\nEpoch 19/120\n\nEpoch 00019: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0019.h5py\n198/198 - 14s - loss: 0.3579 - accuracy: 0.9554 - val_loss: 0.3479 - val_accuracy: 0.9627\nEpoch 20/120\n\nEpoch 00020: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0020.h5py\n198/198 - 14s - loss: 0.3557 - accuracy: 0.9570 - val_loss: 0.3490 - val_accuracy: 0.9640\nEpoch 21/120\n\nEpoch 00021: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0021.h5py\n198/198 - 14s - loss: 0.3556 - accuracy: 0.9569 - val_loss: 0.3471 - val_accuracy: 0.9627\nEpoch 22/120\n\nEpoch 00022: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0022.h5py\n198/198 - 14s - loss: 0.3531 - accuracy: 0.9594 - val_loss: 0.3712 - val_accuracy: 0.9412\nEpoch 23/120\n\nEpoch 00023: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0023.h5py\n198/198 - 14s - loss: 0.3512 - accuracy: 0.9632 - val_loss: 0.3667 - val_accuracy: 0.9431\nEpoch 24/120\n\nEpoch 00024: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0024.h5py\n198/198 - 15s - loss: 0.3516 - accuracy: 0.9614 - val_loss: 0.3475 - val_accuracy: 0.9633\nEpoch 25/120\n\nEpoch 00025: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0025.h5py\n198/198 - 14s - loss: 0.3514 - accuracy: 0.9603 - val_loss: 0.3405 - val_accuracy: 0.9741\nEpoch 26/120\n\nEpoch 00026: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0026.h5py\n198/198 - 14s - loss: 0.3509 - accuracy: 0.9625 - val_loss: 0.3542 - val_accuracy: 0.9627\nEpoch 27/120\n\nEpoch 00027: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0027.h5py\n198/198 - 15s - loss: 0.3492 - accuracy: 0.9630 - val_loss: 0.3430 - val_accuracy: 0.9709\nEpoch 28/120\n\nEpoch 00028: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0028.h5py\n198/198 - 14s - loss: 0.3482 - accuracy: 0.9652 - val_loss: 0.3447 - val_accuracy: 0.9684\nEpoch 29/120\n\nEpoch 00029: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0029.h5py\n198/198 - 14s - loss: 0.3490 - accuracy: 0.9644 - val_loss: 0.3557 - val_accuracy: 0.9551\nEpoch 30/120\n\nEpoch 00030: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0030.h5py\n198/198 - 14s - loss: 0.3451 - accuracy: 0.9690 - val_loss: 0.3381 - val_accuracy: 0.9741\nEpoch 31/120\n\nEpoch 00031: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0031.h5py\n198/198 - 14s - loss: 0.3439 - accuracy: 0.9687 - val_loss: 0.3434 - val_accuracy: 0.9697\nEpoch 32/120\n\nEpoch 00032: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0032.h5py\n198/198 - 14s - loss: 0.3460 - accuracy: 0.9674 - val_loss: 0.3401 - val_accuracy: 0.9716\nEpoch 33/120\n\nEpoch 00033: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0033.h5py\n198/198 - 14s - loss: 0.3457 - accuracy: 0.9673 - val_loss: 0.3543 - val_accuracy: 0.9583\nEpoch 34/120\n\nEpoch 00034: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0034.h5py\n198/198 - 14s - loss: 0.3439 - accuracy: 0.9687 - val_loss: 0.3419 - val_accuracy: 0.9722\nEpoch 35/120\n\nEpoch 00035: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0035.h5py\n198/198 - 14s - loss: 0.3449 - accuracy: 0.9671 - val_loss: 0.3384 - val_accuracy: 0.9716\nEpoch 36/120\n\nEpoch 00036: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0036.h5py\n198/198 - 14s - loss: 0.3426 - accuracy: 0.9706 - val_loss: 0.3388 - val_accuracy: 0.9728\nEpoch 37/120\n\nEpoch 00037: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0037.h5py\n198/198 - 14s - loss: 0.3416 - accuracy: 0.9716 - val_loss: 0.3348 - val_accuracy: 0.9798\nEpoch 38/120\n\nEpoch 00038: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0038.h5py\n198/198 - 14s - loss: 0.3414 - accuracy: 0.9709 - val_loss: 0.3351 - val_accuracy: 0.9772\nEpoch 39/120\n\nEpoch 00039: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0039.h5py\n198/198 - 14s - loss: 0.3409 - accuracy: 0.9725 - val_loss: 0.3343 - val_accuracy: 0.9772\nEpoch 40/120\n\nEpoch 00040: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0040.h5py\n198/198 - 14s - loss: 0.3411 - accuracy: 0.9717 - val_loss: 0.3332 - val_accuracy: 0.9798\nEpoch 41/120\n\nEpoch 00041: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0041.h5py\n198/198 - 14s - loss: 0.3422 - accuracy: 0.9700 - val_loss: 0.3319 - val_accuracy: 0.9823\nEpoch 42/120\n\nEpoch 00042: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0042.h5py\n198/198 - 14s - loss: 0.3413 - accuracy: 0.9717 - val_loss: 0.3318 - val_accuracy: 0.9804\nEpoch 43/120\n\nEpoch 00043: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0043.h5py\n198/198 - 14s - loss: 0.3394 - accuracy: 0.9730 - val_loss: 0.3476 - val_accuracy: 0.9640\nEpoch 44/120\n\nEpoch 00044: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0044.h5py\n198/198 - 14s - loss: 0.3392 - accuracy: 0.9742 - val_loss: 0.3342 - val_accuracy: 0.9791\nEpoch 45/120\n\nEpoch 00045: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0045.h5py\n198/198 - 14s - loss: 0.3370 - accuracy: 0.9757 - val_loss: 0.3343 - val_accuracy: 0.9791\nEpoch 46/120\n\nEpoch 00046: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0046.h5py\n198/198 - 14s - loss: 0.3385 - accuracy: 0.9744 - val_loss: 0.3365 - val_accuracy: 0.9753\nEpoch 47/120\n\nEpoch 00047: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0047.h5py\n198/198 - 14s - loss: 0.3359 - accuracy: 0.9772 - val_loss: 0.3336 - val_accuracy: 0.9791\nEpoch 48/120\n\nEpoch 00048: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0048.h5py\n198/198 - 16s - loss: 0.3356 - accuracy: 0.9779 - val_loss: 0.3465 - val_accuracy: 0.9652\nEpoch 49/120\n\nEpoch 00049: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0049.h5py\n198/198 - 14s - loss: 0.3381 - accuracy: 0.9742 - val_loss: 0.3587 - val_accuracy: 0.9526\nEpoch 50/120\n\nEpoch 00050: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0050.h5py\n198/198 - 14s - loss: 0.3382 - accuracy: 0.9750 - val_loss: 0.3554 - val_accuracy: 0.9558\nEpoch 51/120\n\nEpoch 00051: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0051.h5py\n198/198 - 14s - loss: 0.3351 - accuracy: 0.9788 - val_loss: 0.3399 - val_accuracy: 0.9728\nEpoch 52/120\n\nEpoch 00052: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0052.h5py\n198/198 - 14s - loss: 0.3369 - accuracy: 0.9763 - val_loss: 0.3326 - val_accuracy: 0.9798\nEpoch 53/120\n\nEpoch 00053: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0053.h5py\n198/198 - 14s - loss: 0.3361 - accuracy: 0.9765 - val_loss: 0.3362 - val_accuracy: 0.9760\nEpoch 54/120\n\nEpoch 00054: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0054.h5py\n198/198 - 14s - loss: 0.3326 - accuracy: 0.9814 - val_loss: 0.3445 - val_accuracy: 0.9652\nEpoch 55/120\n\nEpoch 00055: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0055.h5py\n198/198 - 15s - loss: 0.3359 - accuracy: 0.9777 - val_loss: 0.3342 - val_accuracy: 0.9772\nEpoch 56/120\n\nEpoch 00056: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0056.h5py\n198/198 - 14s - loss: 0.3344 - accuracy: 0.9782 - val_loss: 0.3327 - val_accuracy: 0.9798\nEpoch 57/120\n\nEpoch 00057: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0057.h5py\n198/198 - 14s - loss: 0.3328 - accuracy: 0.9804 - val_loss: 0.3299 - val_accuracy: 0.9848\nEpoch 58/120\n\nEpoch 00058: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0058.h5py\n198/198 - 14s - loss: 0.3327 - accuracy: 0.9802 - val_loss: 0.3315 - val_accuracy: 0.9817\nEpoch 59/120\n\nEpoch 00059: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0059.h5py\n198/198 - 14s - loss: 0.3332 - accuracy: 0.9796 - val_loss: 0.3333 - val_accuracy: 0.9785\nEpoch 60/120\n\nEpoch 00060: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0060.h5py\n198/198 - 14s - loss: 0.3317 - accuracy: 0.9814 - val_loss: 0.3321 - val_accuracy: 0.9810\nEpoch 61/120\n\nEpoch 00061: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0061.h5py\n198/198 - 14s - loss: 0.3323 - accuracy: 0.9815 - val_loss: 0.3307 - val_accuracy: 0.9836\nEpoch 62/120\n\nEpoch 00062: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0062.h5py\n198/198 - 14s - loss: 0.3300 - accuracy: 0.9845 - val_loss: 0.3325 - val_accuracy: 0.9817\nEpoch 63/120\n\nEpoch 00063: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0063.h5py\n198/198 - 14s - loss: 0.3342 - accuracy: 0.9782 - val_loss: 0.3316 - val_accuracy: 0.9810\nEpoch 64/120\n\nEpoch 00064: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0064.h5py\n198/198 - 14s - loss: 0.3328 - accuracy: 0.9807 - val_loss: 0.3306 - val_accuracy: 0.9817\nEpoch 65/120\n\nEpoch 00065: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0065.h5py\n198/198 - 14s - loss: 0.3311 - accuracy: 0.9817 - val_loss: 0.3309 - val_accuracy: 0.9817\nEpoch 66/120\n\nEpoch 00066: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0066.h5py\n198/198 - 14s - loss: 0.3319 - accuracy: 0.9809 - val_loss: 0.3291 - val_accuracy: 0.9842\nEpoch 67/120\n\nEpoch 00067: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0067.h5py\n198/198 - 14s - loss: 0.3311 - accuracy: 0.9821 - val_loss: 0.3304 - val_accuracy: 0.9829\nEpoch 68/120\n\nEpoch 00068: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0068.h5py\n198/198 - 14s - loss: 0.3302 - accuracy: 0.9831 - val_loss: 0.3297 - val_accuracy: 0.9836\nEpoch 69/120\n\nEpoch 00069: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0069.h5py\n198/198 - 14s - loss: 0.3316 - accuracy: 0.9810 - val_loss: 0.3334 - val_accuracy: 0.9798\nEpoch 70/120\n\nEpoch 00070: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0070.h5py\n198/198 - 14s - loss: 0.3309 - accuracy: 0.9818 - val_loss: 0.3314 - val_accuracy: 0.9817\nEpoch 71/120\n\nEpoch 00071: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0071.h5py\n198/198 - 14s - loss: 0.3332 - accuracy: 0.9798 - val_loss: 0.3324 - val_accuracy: 0.9804\nEpoch 72/120\n\nEpoch 00072: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0072.h5py\n198/198 - 14s - loss: 0.3312 - accuracy: 0.9814 - val_loss: 0.3305 - val_accuracy: 0.9817\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO:tensorflow:Using MirroredStrategy with devices (&#39;/job:localhost/replica:0/task:0/device:GPU:0&#39;, &#39;/job:localhost/replica:0/task:0/device:GPU:1&#39;)\nWARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to (&#39;/job:localhost/replica:0/task:0/device:CPU:0&#39;,).\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_224_no_top.h5\n\r   8192/2019640 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2023424/2019640 [==============================] - 0s 0us/step\nEpoch 1/120\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse `tf.profiler.experimental.stop` instead.\nWARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0781s vs `on_train_batch_end` time: 0.3532s). Check your callbacks.\n\nEpoch 00001: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0001.h5py\n198/198 - 83s - loss: 0.5804 - accuracy: 0.7107 - val_loss: 0.5672 - val_accuracy: 0.7295\nEpoch 2/120\n\nEpoch 00002: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0002.h5py\n198/198 - 14s - loss: 0.4869 - accuracy: 0.8189 - val_loss: 0.4890 - val_accuracy: 0.8217\nEpoch 3/120\n\nEpoch 00003: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0003.h5py\n198/198 - 14s - loss: 0.4443 - accuracy: 0.8663 - val_loss: 0.4551 - val_accuracy: 0.8565\nEpoch 4/120\n\nEpoch 00004: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0004.h5py\n198/198 - 14s - loss: 0.4265 - accuracy: 0.8842 - val_loss: 0.4366 - val_accuracy: 0.8717\nEpoch 5/120\n\nEpoch 00005: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0005.h5py\n198/198 - 14s - loss: 0.4087 - accuracy: 0.9028 - val_loss: 0.4188 - val_accuracy: 0.8932\nEpoch 6/120\n\nEpoch 00006: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0006.h5py\n198/198 - 14s - loss: 0.3981 - accuracy: 0.9126 - val_loss: 0.4788 - val_accuracy: 0.8281\nEpoch 7/120\n\nEpoch 00007: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0007.h5py\n198/198 - 14s - loss: 0.3938 - accuracy: 0.9172 - val_loss: 0.4132 - val_accuracy: 0.8944\nEpoch 8/120\n\nEpoch 00008: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0008.h5py\n198/198 - 14s - loss: 0.3858 - accuracy: 0.9260 - val_loss: 0.3952 - val_accuracy: 0.9166\nEpoch 9/120\n\nEpoch 00009: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0009.h5py\n198/198 - 14s - loss: 0.3796 - accuracy: 0.9325 - val_loss: 0.3767 - val_accuracy: 0.9349\nEpoch 10/120\n\nEpoch 00010: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0010.h5py\n198/198 - 14s - loss: 0.3763 - accuracy: 0.9360 - val_loss: 0.3675 - val_accuracy: 0.9425\nEpoch 11/120\n\nEpoch 00011: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0011.h5py\n198/198 - 14s - loss: 0.3756 - accuracy: 0.9366 - val_loss: 0.3865 - val_accuracy: 0.9254\nEpoch 12/120\n\nEpoch 00012: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0012.h5py\n198/198 - 15s - loss: 0.3696 - accuracy: 0.9420 - val_loss: 0.3698 - val_accuracy: 0.9399\nEpoch 13/120\n\nEpoch 00013: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0013.h5py\n198/198 - 15s - loss: 0.3666 - accuracy: 0.9460 - val_loss: 0.3724 - val_accuracy: 0.9362\nEpoch 14/120\n\nEpoch 00014: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0014.h5py\n198/198 - 14s - loss: 0.3673 - accuracy: 0.9453 - val_loss: 0.3879 - val_accuracy: 0.9254\nEpoch 15/120\n\nEpoch 00015: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0015.h5py\n198/198 - 14s - loss: 0.3639 - accuracy: 0.9479 - val_loss: 0.3618 - val_accuracy: 0.9513\nEpoch 16/120\n\nEpoch 00016: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0016.h5py\n198/198 - 15s - loss: 0.3610 - accuracy: 0.9513 - val_loss: 0.3519 - val_accuracy: 0.9595\nEpoch 17/120\n\nEpoch 00017: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0017.h5py\n198/198 - 15s - loss: 0.3593 - accuracy: 0.9515 - val_loss: 0.3509 - val_accuracy: 0.9608\nEpoch 18/120\n\nEpoch 00018: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0018.h5py\n198/198 - 14s - loss: 0.3596 - accuracy: 0.9523 - val_loss: 0.3495 - val_accuracy: 0.9633\nEpoch 19/120\n\nEpoch 00019: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0019.h5py\n198/198 - 14s - loss: 0.3579 - accuracy: 0.9554 - val_loss: 0.3479 - val_accuracy: 0.9627\nEpoch 20/120\n\nEpoch 00020: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0020.h5py\n198/198 - 14s - loss: 0.3557 - accuracy: 0.9570 - val_loss: 0.3490 - val_accuracy: 0.9640\nEpoch 21/120\n\nEpoch 00021: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0021.h5py\n198/198 - 14s - loss: 0.3556 - accuracy: 0.9569 - val_loss: 0.3471 - val_accuracy: 0.9627\nEpoch 22/120\n\nEpoch 00022: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0022.h5py\n198/198 - 14s - loss: 0.3531 - accuracy: 0.9594 - val_loss: 0.3712 - val_accuracy: 0.9412\nEpoch 23/120\n\nEpoch 00023: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0023.h5py\n198/198 - 14s - loss: 0.3512 - accuracy: 0.9632 - val_loss: 0.3667 - val_accuracy: 0.9431\nEpoch 24/120\n\nEpoch 00024: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0024.h5py\n198/198 - 15s - loss: 0.3516 - accuracy: 0.9614 - val_loss: 0.3475 - val_accuracy: 0.9633\nEpoch 25/120\n\nEpoch 00025: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0025.h5py\n198/198 - 14s - loss: 0.3514 - accuracy: 0.9603 - val_loss: 0.3405 - val_accuracy: 0.9741\nEpoch 26/120\n\nEpoch 00026: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0026.h5py\n198/198 - 14s - loss: 0.3509 - accuracy: 0.9625 - val_loss: 0.3542 - val_accuracy: 0.9627\nEpoch 27/120\n\nEpoch 00027: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0027.h5py\n198/198 - 15s - loss: 0.3492 - accuracy: 0.9630 - val_loss: 0.3430 - val_accuracy: 0.9709\nEpoch 28/120\n\nEpoch 00028: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0028.h5py\n198/198 - 14s - loss: 0.3482 - accuracy: 0.9652 - val_loss: 0.3447 - val_accuracy: 0.9684\nEpoch 29/120\n\nEpoch 00029: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0029.h5py\n198/198 - 14s - loss: 0.3490 - accuracy: 0.9644 - val_loss: 0.3557 - val_accuracy: 0.9551\nEpoch 30/120\n\nEpoch 00030: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0030.h5py\n198/198 - 14s - loss: 0.3451 - accuracy: 0.9690 - val_loss: 0.3381 - val_accuracy: 0.9741\nEpoch 31/120\n\nEpoch 00031: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0031.h5py\n198/198 - 14s - loss: 0.3439 - accuracy: 0.9687 - val_loss: 0.3434 - val_accuracy: 0.9697\nEpoch 32/120\n\nEpoch 00032: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0032.h5py\n198/198 - 14s - loss: 0.3460 - accuracy: 0.9674 - val_loss: 0.3401 - val_accuracy: 0.9716\nEpoch 33/120\n\nEpoch 00033: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0033.h5py\n198/198 - 14s - loss: 0.3457 - accuracy: 0.9673 - val_loss: 0.3543 - val_accuracy: 0.9583\nEpoch 34/120\n\nEpoch 00034: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0034.h5py\n198/198 - 14s - loss: 0.3439 - accuracy: 0.9687 - val_loss: 0.3419 - val_accuracy: 0.9722\nEpoch 35/120\n\nEpoch 00035: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0035.h5py\n198/198 - 14s - loss: 0.3449 - accuracy: 0.9671 - val_loss: 0.3384 - val_accuracy: 0.9716\nEpoch 36/120\n\nEpoch 00036: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0036.h5py\n198/198 - 14s - loss: 0.3426 - accuracy: 0.9706 - val_loss: 0.3388 - val_accuracy: 0.9728\nEpoch 37/120\n\nEpoch 00037: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0037.h5py\n198/198 - 14s - loss: 0.3416 - accuracy: 0.9716 - val_loss: 0.3348 - val_accuracy: 0.9798\nEpoch 38/120\n\nEpoch 00038: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0038.h5py\n198/198 - 14s - loss: 0.3414 - accuracy: 0.9709 - val_loss: 0.3351 - val_accuracy: 0.9772\nEpoch 39/120\n\nEpoch 00039: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0039.h5py\n198/198 - 14s - loss: 0.3409 - accuracy: 0.9725 - val_loss: 0.3343 - val_accuracy: 0.9772\nEpoch 40/120\n\nEpoch 00040: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0040.h5py\n198/198 - 14s - loss: 0.3411 - accuracy: 0.9717 - val_loss: 0.3332 - val_accuracy: 0.9798\nEpoch 41/120\n\nEpoch 00041: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0041.h5py\n198/198 - 14s - loss: 0.3422 - accuracy: 0.9700 - val_loss: 0.3319 - val_accuracy: 0.9823\nEpoch 42/120\n\nEpoch 00042: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0042.h5py\n198/198 - 14s - loss: 0.3413 - accuracy: 0.9717 - val_loss: 0.3318 - val_accuracy: 0.9804\nEpoch 43/120\n\nEpoch 00043: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0043.h5py\n198/198 - 14s - loss: 0.3394 - accuracy: 0.9730 - val_loss: 0.3476 - val_accuracy: 0.9640\nEpoch 44/120\n\nEpoch 00044: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0044.h5py\n198/198 - 14s - loss: 0.3392 - accuracy: 0.9742 - val_loss: 0.3342 - val_accuracy: 0.9791\nEpoch 45/120\n\nEpoch 00045: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0045.h5py\n198/198 - 14s - loss: 0.3370 - accuracy: 0.9757 - val_loss: 0.3343 - val_accuracy: 0.9791\nEpoch 46/120\n\nEpoch 00046: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0046.h5py\n198/198 - 14s - loss: 0.3385 - accuracy: 0.9744 - val_loss: 0.3365 - val_accuracy: 0.9753\nEpoch 47/120\n\nEpoch 00047: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0047.h5py\n198/198 - 14s - loss: 0.3359 - accuracy: 0.9772 - val_loss: 0.3336 - val_accuracy: 0.9791\nEpoch 48/120\n\nEpoch 00048: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0048.h5py\n198/198 - 16s - loss: 0.3356 - accuracy: 0.9779 - val_loss: 0.3465 - val_accuracy: 0.9652\nEpoch 49/120\n\nEpoch 00049: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0049.h5py\n198/198 - 14s - loss: 0.3381 - accuracy: 0.9742 - val_loss: 0.3587 - val_accuracy: 0.9526\nEpoch 50/120\n\nEpoch 00050: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0050.h5py\n198/198 - 14s - loss: 0.3382 - accuracy: 0.9750 - val_loss: 0.3554 - val_accuracy: 0.9558\nEpoch 51/120\n\nEpoch 00051: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0051.h5py\n198/198 - 14s - loss: 0.3351 - accuracy: 0.9788 - val_loss: 0.3399 - val_accuracy: 0.9728\nEpoch 52/120\n\nEpoch 00052: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0052.h5py\n198/198 - 14s - loss: 0.3369 - accuracy: 0.9763 - val_loss: 0.3326 - val_accuracy: 0.9798\nEpoch 53/120\n\nEpoch 00053: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0053.h5py\n198/198 - 14s - loss: 0.3361 - accuracy: 0.9765 - val_loss: 0.3362 - val_accuracy: 0.9760\nEpoch 54/120\n\nEpoch 00054: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0054.h5py\n198/198 - 14s - loss: 0.3326 - accuracy: 0.9814 - val_loss: 0.3445 - val_accuracy: 0.9652\nEpoch 55/120\n\nEpoch 00055: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0055.h5py\n198/198 - 15s - loss: 0.3359 - accuracy: 0.9777 - val_loss: 0.3342 - val_accuracy: 0.9772\nEpoch 56/120\n\nEpoch 00056: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0056.h5py\n198/198 - 14s - loss: 0.3344 - accuracy: 0.9782 - val_loss: 0.3327 - val_accuracy: 0.9798\nEpoch 57/120\n\nEpoch 00057: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0057.h5py\n198/198 - 14s - loss: 0.3328 - accuracy: 0.9804 - val_loss: 0.3299 - val_accuracy: 0.9848\nEpoch 58/120\n\nEpoch 00058: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0058.h5py\n198/198 - 14s - loss: 0.3327 - accuracy: 0.9802 - val_loss: 0.3315 - val_accuracy: 0.9817\nEpoch 59/120\n\nEpoch 00059: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0059.h5py\n198/198 - 14s - loss: 0.3332 - accuracy: 0.9796 - val_loss: 0.3333 - val_accuracy: 0.9785\nEpoch 60/120\n\nEpoch 00060: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0060.h5py\n198/198 - 14s - loss: 0.3317 - accuracy: 0.9814 - val_loss: 0.3321 - val_accuracy: 0.9810\nEpoch 61/120\n\nEpoch 00061: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0061.h5py\n198/198 - 14s - loss: 0.3323 - accuracy: 0.9815 - val_loss: 0.3307 - val_accuracy: 0.9836\nEpoch 62/120\n\nEpoch 00062: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0062.h5py\n198/198 - 14s - loss: 0.3300 - accuracy: 0.9845 - val_loss: 0.3325 - val_accuracy: 0.9817\nEpoch 63/120\n\nEpoch 00063: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0063.h5py\n198/198 - 14s - loss: 0.3342 - accuracy: 0.9782 - val_loss: 0.3316 - val_accuracy: 0.9810\nEpoch 64/120\n\nEpoch 00064: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0064.h5py\n198/198 - 14s - loss: 0.3328 - accuracy: 0.9807 - val_loss: 0.3306 - val_accuracy: 0.9817\nEpoch 65/120\n\nEpoch 00065: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0065.h5py\n198/198 - 14s - loss: 0.3311 - accuracy: 0.9817 - val_loss: 0.3309 - val_accuracy: 0.9817\nEpoch 66/120\n\nEpoch 00066: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0066.h5py\n198/198 - 14s - loss: 0.3319 - accuracy: 0.9809 - val_loss: 0.3291 - val_accuracy: 0.9842\nEpoch 67/120\n\nEpoch 00067: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0067.h5py\n198/198 - 14s - loss: 0.3311 - accuracy: 0.9821 - val_loss: 0.3304 - val_accuracy: 0.9829\nEpoch 68/120\n\nEpoch 00068: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0068.h5py\n198/198 - 14s - loss: 0.3302 - accuracy: 0.9831 - val_loss: 0.3297 - val_accuracy: 0.9836\nEpoch 69/120\n\nEpoch 00069: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0069.h5py\n198/198 - 14s - loss: 0.3316 - accuracy: 0.9810 - val_loss: 0.3334 - val_accuracy: 0.9798\nEpoch 70/120\n\nEpoch 00070: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0070.h5py\n198/198 - 14s - loss: 0.3309 - accuracy: 0.9818 - val_loss: 0.3314 - val_accuracy: 0.9817\nEpoch 71/120\n\nEpoch 00071: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0071.h5py\n198/198 - 14s - loss: 0.3332 - accuracy: 0.9798 - val_loss: 0.3324 - val_accuracy: 0.9804\nEpoch 72/120\n\nEpoch 00072: saving model to /dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0072.h5py\n198/198 - 14s - loss: 0.3312 - accuracy: 0.9814 - val_loss: 0.3305 - val_accuracy: 0.9817\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def load_model(checkpoint_path):\n  model = get_compiled_model()\n  # load weights\n  model.load_weights(checkpoint_path)\n  return model\n  \ndef predict(img_path, model):\n  test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    img_path,\n#     seed=seed,\n    shuffle=False,\n#     validation_split=0.2,\n#     subset='validation',\n    image_size=(img_height, img_width),\n    batch_size=batch_size)\n\n#   test_ds_2 = test_ds.map(\n#   lambda x, y: (resize(x), y))\n  test_ds_2 = test_ds.map(\n    lambda x, y: (preprocess_input(x), y))\n  \n#     with open(CLASS_PEOPLE + '.pkl','rb') as f:\n#         class_to_index = pickle.load(f)\n\n  result = model.predict(test_ds_2, batch_size=batch_size)\n  scores = []\n  for i in range(len(result)):\n      scores.append(result[i][-1])\n  print(scores)\n  return result, test_ds, scores\n#   for item in result:\n#       print(np.argmax(item))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fc10fee-5b2a-4be1-a095-842b0547489a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["model = load_model('/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/Weight2/version-0072.h5py')\nmodel.summary()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b75209cd-0eff-4cbd-b3a1-729f855c1bd3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\nModel: &#34;functional_5&#34;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 120, 160, 3) 0                                            \n__________________________________________________________________________________________________\nConv1_pad (ZeroPadding2D)       (None, 121, 161, 3)  0           input_3[0][0]                    \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 60, 80, 16)   432         Conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 60, 80, 16)   64          Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 60, 80, 16)   0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 60, 80, 16)   144         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 60, 80, 16)   64          expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 60, 80, 16)   0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 60, 80, 8)    128         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 60, 80, 8)    32          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 60, 80, 48)   384         expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 60, 80, 48)   192         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 60, 80, 48)   0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 61, 81, 48)   0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 30, 40, 48)   432         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 30, 40, 48)   192         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 30, 40, 48)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 30, 40, 8)    384         block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 30, 40, 8)    32          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 30, 40, 48)   384         block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 30, 40, 48)   192         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 30, 40, 48)   0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 30, 40, 48)   432         block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 30, 40, 48)   192         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 30, 40, 48)   0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 30, 40, 8)    384         block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 30, 40, 8)    32          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 30, 40, 8)    0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 30, 40, 48)   384         block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 30, 40, 48)   192         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 30, 40, 48)   0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 31, 41, 48)   0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 15, 20, 48)   432         block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 15, 20, 48)   192         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 15, 20, 48)   0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 15, 20, 16)   768         block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 15, 20, 16)   64          block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 15, 20, 96)   1536        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 15, 20, 96)   864         block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 15, 20, 96)   384         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 15, 20, 96)   0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 15, 20, 16)   1536        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 15, 20, 16)   64          block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 15, 20, 16)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 15, 20, 96)   1536        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 15, 20, 96)   864         block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 15, 20, 96)   384         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 15, 20, 96)   0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 15, 20, 16)   1536        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 15, 20, 16)   64          block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 15, 20, 16)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 15, 20, 96)   1536        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 17, 21, 96)   0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 8, 10, 96)    864         block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 8, 10, 96)    384         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 8, 10, 96)    0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 8, 10, 24)    2304        block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 8, 10, 24)    96          block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 8, 10, 144)   3456        block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 8, 10, 24)    3456        block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 8, 10, 24)    96          block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 8, 10, 24)    0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 8, 10, 144)   3456        block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 8, 10, 24)    3456        block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 8, 10, 24)    96          block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 8, 10, 24)    0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 8, 10, 144)   3456        block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 8, 10, 24)    3456        block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 8, 10, 24)    96          block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 8, 10, 24)    0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 8, 10, 144)   3456        block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 8, 10, 144)   576         block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 8, 10, 144)   0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 8, 10, 144)   1296        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 8, 10, 144)   576         block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 8, 10, 144)   0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 8, 10, 32)    4608        block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 8, 10, 32)    128         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 8, 10, 192)   6144        block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 8, 10, 192)   1728        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 8, 10, 192)   768         block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 8, 10, 192)   0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 8, 10, 32)    6144        block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 8, 10, 32)    128         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 8, 10, 32)    0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 8, 10, 192)   6144        block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 8, 10, 192)   1728        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 8, 10, 192)   768         block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 8, 10, 192)   0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 8, 10, 32)    6144        block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 8, 10, 32)    128         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 8, 10, 32)    0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 8, 10, 192)   6144        block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 9, 11, 192)   0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 4, 5, 192)    1728        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 4, 5, 192)    768         block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 4, 5, 192)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 4, 5, 56)     10752       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 4, 5, 56)     224         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 4, 5, 336)    18816       block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 4, 5, 56)     18816       block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 4, 5, 56)     224         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 4, 5, 56)     0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 4, 5, 336)    18816       block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 4, 5, 56)     18816       block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 4, 5, 56)     224         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 4, 5, 56)     0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 4, 5, 336)    18816       block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 4, 5, 112)    37632       block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 4, 5, 112)    448         block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 4, 5, 1280)   143360      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 4, 5, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 4, 5, 1280)   0           Conv_1_bn[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling2d_2 (Glo (None, 1280)         0           out_relu[0][0]                   \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 1280)         0           global_average_pooling2d_2[0][0] \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 2)            2562        dropout_2[0][0]                  \n==================================================================================================\nTotal params: 412,770\nTrainable params: 398,690\nNon-trainable params: 14,080\n__________________________________________________________________________________________________\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\nModel: &#34;functional_5&#34;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 120, 160, 3) 0                                            \n__________________________________________________________________________________________________\nConv1_pad (ZeroPadding2D)       (None, 121, 161, 3)  0           input_3[0][0]                    \n__________________________________________________________________________________________________\nConv1 (Conv2D)                  (None, 60, 80, 16)   432         Conv1_pad[0][0]                  \n__________________________________________________________________________________________________\nbn_Conv1 (BatchNormalization)   (None, 60, 80, 16)   64          Conv1[0][0]                      \n__________________________________________________________________________________________________\nConv1_relu (ReLU)               (None, 60, 80, 16)   0           bn_Conv1[0][0]                   \n__________________________________________________________________________________________________\nexpanded_conv_depthwise (Depthw (None, 60, 80, 16)   144         Conv1_relu[0][0]                 \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_BN (Bat (None, 60, 80, 16)   64          expanded_conv_depthwise[0][0]    \n__________________________________________________________________________________________________\nexpanded_conv_depthwise_relu (R (None, 60, 80, 16)   0           expanded_conv_depthwise_BN[0][0] \n__________________________________________________________________________________________________\nexpanded_conv_project (Conv2D)  (None, 60, 80, 8)    128         expanded_conv_depthwise_relu[0][0\n__________________________________________________________________________________________________\nexpanded_conv_project_BN (Batch (None, 60, 80, 8)    32          expanded_conv_project[0][0]      \n__________________________________________________________________________________________________\nblock_1_expand (Conv2D)         (None, 60, 80, 48)   384         expanded_conv_project_BN[0][0]   \n__________________________________________________________________________________________________\nblock_1_expand_BN (BatchNormali (None, 60, 80, 48)   192         block_1_expand[0][0]             \n__________________________________________________________________________________________________\nblock_1_expand_relu (ReLU)      (None, 60, 80, 48)   0           block_1_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_1_pad (ZeroPadding2D)     (None, 61, 81, 48)   0           block_1_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_1_depthwise (DepthwiseCon (None, 30, 40, 48)   432         block_1_pad[0][0]                \n__________________________________________________________________________________________________\nblock_1_depthwise_BN (BatchNorm (None, 30, 40, 48)   192         block_1_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_1_depthwise_relu (ReLU)   (None, 30, 40, 48)   0           block_1_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_1_project (Conv2D)        (None, 30, 40, 8)    384         block_1_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_1_project_BN (BatchNormal (None, 30, 40, 8)    32          block_1_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_expand (Conv2D)         (None, 30, 40, 48)   384         block_1_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_2_expand_BN (BatchNormali (None, 30, 40, 48)   192         block_2_expand[0][0]             \n__________________________________________________________________________________________________\nblock_2_expand_relu (ReLU)      (None, 30, 40, 48)   0           block_2_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise (DepthwiseCon (None, 30, 40, 48)   432         block_2_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_2_depthwise_BN (BatchNorm (None, 30, 40, 48)   192         block_2_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_2_depthwise_relu (ReLU)   (None, 30, 40, 48)   0           block_2_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_2_project (Conv2D)        (None, 30, 40, 8)    384         block_2_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_2_project_BN (BatchNormal (None, 30, 40, 8)    32          block_2_project[0][0]            \n__________________________________________________________________________________________________\nblock_2_add (Add)               (None, 30, 40, 8)    0           block_1_project_BN[0][0]         \n                                                                 block_2_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_3_expand (Conv2D)         (None, 30, 40, 48)   384         block_2_add[0][0]                \n__________________________________________________________________________________________________\nblock_3_expand_BN (BatchNormali (None, 30, 40, 48)   192         block_3_expand[0][0]             \n__________________________________________________________________________________________________\nblock_3_expand_relu (ReLU)      (None, 30, 40, 48)   0           block_3_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_3_pad (ZeroPadding2D)     (None, 31, 41, 48)   0           block_3_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_3_depthwise (DepthwiseCon (None, 15, 20, 48)   432         block_3_pad[0][0]                \n__________________________________________________________________________________________________\nblock_3_depthwise_BN (BatchNorm (None, 15, 20, 48)   192         block_3_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_3_depthwise_relu (ReLU)   (None, 15, 20, 48)   0           block_3_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_3_project (Conv2D)        (None, 15, 20, 16)   768         block_3_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_3_project_BN (BatchNormal (None, 15, 20, 16)   64          block_3_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_expand (Conv2D)         (None, 15, 20, 96)   1536        block_3_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_4_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_4_expand[0][0]             \n__________________________________________________________________________________________________\nblock_4_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_4_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise (DepthwiseCon (None, 15, 20, 96)   864         block_4_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_4_depthwise_BN (BatchNorm (None, 15, 20, 96)   384         block_4_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_4_depthwise_relu (ReLU)   (None, 15, 20, 96)   0           block_4_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_4_project (Conv2D)        (None, 15, 20, 16)   1536        block_4_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_4_project_BN (BatchNormal (None, 15, 20, 16)   64          block_4_project[0][0]            \n__________________________________________________________________________________________________\nblock_4_add (Add)               (None, 15, 20, 16)   0           block_3_project_BN[0][0]         \n                                                                 block_4_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_5_expand (Conv2D)         (None, 15, 20, 96)   1536        block_4_add[0][0]                \n__________________________________________________________________________________________________\nblock_5_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_5_expand[0][0]             \n__________________________________________________________________________________________________\nblock_5_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_5_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise (DepthwiseCon (None, 15, 20, 96)   864         block_5_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_5_depthwise_BN (BatchNorm (None, 15, 20, 96)   384         block_5_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_5_depthwise_relu (ReLU)   (None, 15, 20, 96)   0           block_5_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_5_project (Conv2D)        (None, 15, 20, 16)   1536        block_5_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_5_project_BN (BatchNormal (None, 15, 20, 16)   64          block_5_project[0][0]            \n__________________________________________________________________________________________________\nblock_5_add (Add)               (None, 15, 20, 16)   0           block_4_add[0][0]                \n                                                                 block_5_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_6_expand (Conv2D)         (None, 15, 20, 96)   1536        block_5_add[0][0]                \n__________________________________________________________________________________________________\nblock_6_expand_BN (BatchNormali (None, 15, 20, 96)   384         block_6_expand[0][0]             \n__________________________________________________________________________________________________\nblock_6_expand_relu (ReLU)      (None, 15, 20, 96)   0           block_6_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_6_pad (ZeroPadding2D)     (None, 17, 21, 96)   0           block_6_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_6_depthwise (DepthwiseCon (None, 8, 10, 96)    864         block_6_pad[0][0]                \n__________________________________________________________________________________________________\nblock_6_depthwise_BN (BatchNorm (None, 8, 10, 96)    384         block_6_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_6_depthwise_relu (ReLU)   (None, 8, 10, 96)    0           block_6_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_6_project (Conv2D)        (None, 8, 10, 24)    2304        block_6_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_6_project_BN (BatchNormal (None, 8, 10, 24)    96          block_6_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_expand (Conv2D)         (None, 8, 10, 144)   3456        block_6_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_7_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_7_expand[0][0]             \n__________________________________________________________________________________________________\nblock_7_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_7_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_7_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_7_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_7_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_7_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_7_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_7_project (Conv2D)        (None, 8, 10, 24)    3456        block_7_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_7_project_BN (BatchNormal (None, 8, 10, 24)    96          block_7_project[0][0]            \n__________________________________________________________________________________________________\nblock_7_add (Add)               (None, 8, 10, 24)    0           block_6_project_BN[0][0]         \n                                                                 block_7_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_8_expand (Conv2D)         (None, 8, 10, 144)   3456        block_7_add[0][0]                \n__________________________________________________________________________________________________\nblock_8_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_8_expand[0][0]             \n__________________________________________________________________________________________________\nblock_8_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_8_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_8_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_8_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_8_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_8_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_8_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_8_project (Conv2D)        (None, 8, 10, 24)    3456        block_8_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_8_project_BN (BatchNormal (None, 8, 10, 24)    96          block_8_project[0][0]            \n__________________________________________________________________________________________________\nblock_8_add (Add)               (None, 8, 10, 24)    0           block_7_add[0][0]                \n                                                                 block_8_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_9_expand (Conv2D)         (None, 8, 10, 144)   3456        block_8_add[0][0]                \n__________________________________________________________________________________________________\nblock_9_expand_BN (BatchNormali (None, 8, 10, 144)   576         block_9_expand[0][0]             \n__________________________________________________________________________________________________\nblock_9_expand_relu (ReLU)      (None, 8, 10, 144)   0           block_9_expand_BN[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise (DepthwiseCon (None, 8, 10, 144)   1296        block_9_expand_relu[0][0]        \n__________________________________________________________________________________________________\nblock_9_depthwise_BN (BatchNorm (None, 8, 10, 144)   576         block_9_depthwise[0][0]          \n__________________________________________________________________________________________________\nblock_9_depthwise_relu (ReLU)   (None, 8, 10, 144)   0           block_9_depthwise_BN[0][0]       \n__________________________________________________________________________________________________\nblock_9_project (Conv2D)        (None, 8, 10, 24)    3456        block_9_depthwise_relu[0][0]     \n__________________________________________________________________________________________________\nblock_9_project_BN (BatchNormal (None, 8, 10, 24)    96          block_9_project[0][0]            \n__________________________________________________________________________________________________\nblock_9_add (Add)               (None, 8, 10, 24)    0           block_8_add[0][0]                \n                                                                 block_9_project_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_expand (Conv2D)        (None, 8, 10, 144)   3456        block_9_add[0][0]                \n__________________________________________________________________________________________________\nblock_10_expand_BN (BatchNormal (None, 8, 10, 144)   576         block_10_expand[0][0]            \n__________________________________________________________________________________________________\nblock_10_expand_relu (ReLU)     (None, 8, 10, 144)   0           block_10_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise (DepthwiseCo (None, 8, 10, 144)   1296        block_10_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_10_depthwise_BN (BatchNor (None, 8, 10, 144)   576         block_10_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_10_depthwise_relu (ReLU)  (None, 8, 10, 144)   0           block_10_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_10_project (Conv2D)       (None, 8, 10, 32)    4608        block_10_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_10_project_BN (BatchNorma (None, 8, 10, 32)    128         block_10_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_expand (Conv2D)        (None, 8, 10, 192)   6144        block_10_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_11_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_11_expand[0][0]            \n__________________________________________________________________________________________________\nblock_11_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_11_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise (DepthwiseCo (None, 8, 10, 192)   1728        block_11_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_11_depthwise_BN (BatchNor (None, 8, 10, 192)   768         block_11_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_11_depthwise_relu (ReLU)  (None, 8, 10, 192)   0           block_11_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_11_project (Conv2D)       (None, 8, 10, 32)    6144        block_11_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_11_project_BN (BatchNorma (None, 8, 10, 32)    128         block_11_project[0][0]           \n__________________________________________________________________________________________________\nblock_11_add (Add)              (None, 8, 10, 32)    0           block_10_project_BN[0][0]        \n                                                                 block_11_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_12_expand (Conv2D)        (None, 8, 10, 192)   6144        block_11_add[0][0]               \n__________________________________________________________________________________________________\nblock_12_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_12_expand[0][0]            \n__________________________________________________________________________________________________\nblock_12_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_12_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise (DepthwiseCo (None, 8, 10, 192)   1728        block_12_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_12_depthwise_BN (BatchNor (None, 8, 10, 192)   768         block_12_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_12_depthwise_relu (ReLU)  (None, 8, 10, 192)   0           block_12_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_12_project (Conv2D)       (None, 8, 10, 32)    6144        block_12_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_12_project_BN (BatchNorma (None, 8, 10, 32)    128         block_12_project[0][0]           \n__________________________________________________________________________________________________\nblock_12_add (Add)              (None, 8, 10, 32)    0           block_11_add[0][0]               \n                                                                 block_12_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_13_expand (Conv2D)        (None, 8, 10, 192)   6144        block_12_add[0][0]               \n__________________________________________________________________________________________________\nblock_13_expand_BN (BatchNormal (None, 8, 10, 192)   768         block_13_expand[0][0]            \n__________________________________________________________________________________________________\nblock_13_expand_relu (ReLU)     (None, 8, 10, 192)   0           block_13_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_13_pad (ZeroPadding2D)    (None, 9, 11, 192)   0           block_13_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_13_depthwise (DepthwiseCo (None, 4, 5, 192)    1728        block_13_pad[0][0]               \n__________________________________________________________________________________________________\nblock_13_depthwise_BN (BatchNor (None, 4, 5, 192)    768         block_13_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_13_depthwise_relu (ReLU)  (None, 4, 5, 192)    0           block_13_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_13_project (Conv2D)       (None, 4, 5, 56)     10752       block_13_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_13_project_BN (BatchNorma (None, 4, 5, 56)     224         block_13_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_expand (Conv2D)        (None, 4, 5, 336)    18816       block_13_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_14_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_14_expand[0][0]            \n__________________________________________________________________________________________________\nblock_14_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_14_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_14_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_14_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_14_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_14_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_14_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_14_project (Conv2D)       (None, 4, 5, 56)     18816       block_14_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_14_project_BN (BatchNorma (None, 4, 5, 56)     224         block_14_project[0][0]           \n__________________________________________________________________________________________________\nblock_14_add (Add)              (None, 4, 5, 56)     0           block_13_project_BN[0][0]        \n                                                                 block_14_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_15_expand (Conv2D)        (None, 4, 5, 336)    18816       block_14_add[0][0]               \n__________________________________________________________________________________________________\nblock_15_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_15_expand[0][0]            \n__________________________________________________________________________________________________\nblock_15_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_15_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_15_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_15_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_15_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_15_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_15_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_15_project (Conv2D)       (None, 4, 5, 56)     18816       block_15_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_15_project_BN (BatchNorma (None, 4, 5, 56)     224         block_15_project[0][0]           \n__________________________________________________________________________________________________\nblock_15_add (Add)              (None, 4, 5, 56)     0           block_14_add[0][0]               \n                                                                 block_15_project_BN[0][0]        \n__________________________________________________________________________________________________\nblock_16_expand (Conv2D)        (None, 4, 5, 336)    18816       block_15_add[0][0]               \n__________________________________________________________________________________________________\nblock_16_expand_BN (BatchNormal (None, 4, 5, 336)    1344        block_16_expand[0][0]            \n__________________________________________________________________________________________________\nblock_16_expand_relu (ReLU)     (None, 4, 5, 336)    0           block_16_expand_BN[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise (DepthwiseCo (None, 4, 5, 336)    3024        block_16_expand_relu[0][0]       \n__________________________________________________________________________________________________\nblock_16_depthwise_BN (BatchNor (None, 4, 5, 336)    1344        block_16_depthwise[0][0]         \n__________________________________________________________________________________________________\nblock_16_depthwise_relu (ReLU)  (None, 4, 5, 336)    0           block_16_depthwise_BN[0][0]      \n__________________________________________________________________________________________________\nblock_16_project (Conv2D)       (None, 4, 5, 112)    37632       block_16_depthwise_relu[0][0]    \n__________________________________________________________________________________________________\nblock_16_project_BN (BatchNorma (None, 4, 5, 112)    448         block_16_project[0][0]           \n__________________________________________________________________________________________________\nConv_1 (Conv2D)                 (None, 4, 5, 1280)   143360      block_16_project_BN[0][0]        \n__________________________________________________________________________________________________\nConv_1_bn (BatchNormalization)  (None, 4, 5, 1280)   5120        Conv_1[0][0]                     \n__________________________________________________________________________________________________\nout_relu (ReLU)                 (None, 4, 5, 1280)   0           Conv_1_bn[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling2d_2 (Glo (None, 1280)         0           out_relu[0][0]                   \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 1280)         0           global_average_pooling2d_2[0][0] \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 2)            2562        dropout_2[0][0]                  \n==================================================================================================\nTotal params: 412,770\nTrainable params: 398,690\nNon-trainable params: 14,080\n__________________________________________________________________________________________________\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["model = tf.keras.models.load_model('/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f2da92f-3a07-4a93-8624-aabee96c7c6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\nException ignored in: &#39;h5py._objects.ObjectID.__dealloc__&#39;\nTraceback (most recent call last):\n  File &#34;h5py/_objects.pyx&#34;, line 193, in h5py._objects.ObjectID.__dealloc__\nRuntimeError: Can&#39;t decrement id ref count (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a47935d40, total write size = 1728, bytes this sub-write = 1728, bytes actually written = 18446744073709551615, offset = 95040)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\nException ignored in: &#39;h5py._objects.ObjectID.__dealloc__&#39;\nTraceback (most recent call last):\n  File &#34;h5py/_objects.pyx&#34;, line 193, in h5py._objects.ObjectID.__dealloc__\nRuntimeError: Can&#39;t decrement id ref count (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a47935d40, total write size = 1728, bytes this sub-write = 1728, bytes actually written = 18446744073709551615, offset = 95040)\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.ObjectID.__dealloc__</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">RuntimeError</span>: Can&#39;t decrement id ref count (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a47935d40, total write size = 1728, bytes this sub-write = 1728, bytes actually written = 18446744073709551615, offset = 95040)<span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">OSError</span>                                   Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_model_to_hdf5</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>     model_layers <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>layers\n<span class=\"ansi-green-fg\">--&gt; 119</span><span class=\"ansi-red-fg\">     </span>save_weights_to_hdf5_group<span class=\"ansi-blue-fg\">(</span>model_weights_group<span class=\"ansi-blue-fg\">,</span> model_layers<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    120</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_weights_to_hdf5_group</span><span class=\"ansi-blue-fg\">(f, layers)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    644</span>       <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 645</span><span class=\"ansi-red-fg\">         </span>param_dset<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> val\n<span class=\"ansi-green-intense-fg ansi-bold\">    646</span> \n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/h5py/_hl/dataset.py</span> in <span class=\"ansi-cyan-fg\">__setitem__</span><span class=\"ansi-blue-fg\">(self, args, val)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>         <span class=\"ansi-green-fg\">for</span> fspace <span class=\"ansi-green-fg\">in</span> selection<span class=\"ansi-blue-fg\">.</span>broadcast<span class=\"ansi-blue-fg\">(</span>mshape<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 708</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>id<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">(</span>mspace<span class=\"ansi-blue-fg\">,</span> fspace<span class=\"ansi-blue-fg\">,</span> val<span class=\"ansi-blue-fg\">,</span> mtype<span class=\"ansi-blue-fg\">,</span> dxpl<span class=\"ansi-blue-fg\">=</span>self<span class=\"ansi-blue-fg\">.</span>_dxpl<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    709</span> \n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/h5d.pyx</span> in <span class=\"ansi-cyan-fg\">h5py.h5d.DatasetID.write</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_proxy.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._proxy.dset_rw</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_proxy.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._proxy.H5PY_H5Dwrite</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">OSError</span>: Can&#39;t write data (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a457ba190, total write size = 573440, bytes this sub-write = 573440, bytes actually written = 18446744073709551615, offset = 101216)\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-243523631348296&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> model <span class=\"ansi-blue-fg\">=</span> tf<span class=\"ansi-blue-fg\">.</span>keras<span class=\"ansi-blue-fg\">.</span>models<span class=\"ansi-blue-fg\">.</span>load_model<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>model<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, filepath, overwrite, include_optimizer, save_format, signatures, options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1977</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1978</span>     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n<span class=\"ansi-green-fg\">-&gt; 1979</span><span class=\"ansi-red-fg\">                     signatures, options)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1980</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1981</span>   def save_weights(self,\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py</span> in <span class=\"ansi-cyan-fg\">save_model</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer, save_format, signatures, options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>           &#39;or using `save_weights`.&#39;)\n<span class=\"ansi-green-intense-fg ansi-bold\">    130</span>     hdf5_format.save_model_to_hdf5(\n<span class=\"ansi-green-fg\">--&gt; 131</span><span class=\"ansi-red-fg\">         model, filepath, overwrite, include_optimizer)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    132</span>   <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    133</span>     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_model_to_hdf5</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>   <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>     <span class=\"ansi-green-fg\">if</span> opened_new_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 130</span><span class=\"ansi-red-fg\">       </span>f<span class=\"ansi-blue-fg\">.</span>close<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/h5py/_hl/files.py</span> in <span class=\"ansi-cyan-fg\">close</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    441</span>                 <span class=\"ansi-green-fg\">for</span> id_ <span class=\"ansi-green-fg\">in</span> file_list<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>                     <span class=\"ansi-green-fg\">while</span> id_<span class=\"ansi-blue-fg\">.</span>valid<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 443</span><span class=\"ansi-red-fg\">                         </span>h5i<span class=\"ansi-blue-fg\">.</span>dec_ref<span class=\"ansi-blue-fg\">(</span>id_<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    444</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    445</span>                 self<span class=\"ansi-blue-fg\">.</span>id<span class=\"ansi-blue-fg\">.</span>close<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/h5i.pyx</span> in <span class=\"ansi-cyan-fg\">h5py.h5i.dec_ref</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">RuntimeError</span>: Dirty entry flush destroy failed (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a4793e070, total write size = 4096, bytes this sub-write = 4096, bytes actually written = 18446744073709551615, offset = 2048)</div>","errorSummary":"<span class=\"ansi-red-fg\">RuntimeError</span>: Dirty entry flush destroy failed (file write failed: time = Thu Mar 18 09:52:16 2021","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.ObjectID.__dealloc__</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">RuntimeError</span>: Can&#39;t decrement id ref count (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a47935d40, total write size = 1728, bytes this sub-write = 1728, bytes actually written = 18446744073709551615, offset = 95040)<span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">OSError</span>                                   Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_model_to_hdf5</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>     model_layers <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>layers\n<span class=\"ansi-green-fg\">--&gt; 119</span><span class=\"ansi-red-fg\">     </span>save_weights_to_hdf5_group<span class=\"ansi-blue-fg\">(</span>model_weights_group<span class=\"ansi-blue-fg\">,</span> model_layers<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    120</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_weights_to_hdf5_group</span><span class=\"ansi-blue-fg\">(f, layers)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    644</span>       <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 645</span><span class=\"ansi-red-fg\">         </span>param_dset<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> val\n<span class=\"ansi-green-intense-fg ansi-bold\">    646</span> \n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/h5py/_hl/dataset.py</span> in <span class=\"ansi-cyan-fg\">__setitem__</span><span class=\"ansi-blue-fg\">(self, args, val)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>         <span class=\"ansi-green-fg\">for</span> fspace <span class=\"ansi-green-fg\">in</span> selection<span class=\"ansi-blue-fg\">.</span>broadcast<span class=\"ansi-blue-fg\">(</span>mshape<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 708</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>id<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">(</span>mspace<span class=\"ansi-blue-fg\">,</span> fspace<span class=\"ansi-blue-fg\">,</span> val<span class=\"ansi-blue-fg\">,</span> mtype<span class=\"ansi-blue-fg\">,</span> dxpl<span class=\"ansi-blue-fg\">=</span>self<span class=\"ansi-blue-fg\">.</span>_dxpl<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    709</span> \n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/h5d.pyx</span> in <span class=\"ansi-cyan-fg\">h5py.h5d.DatasetID.write</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_proxy.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._proxy.dset_rw</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_proxy.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._proxy.H5PY_H5Dwrite</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">OSError</span>: Can&#39;t write data (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a457ba190, total write size = 573440, bytes this sub-write = 573440, bytes actually written = 18446744073709551615, offset = 101216)\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-243523631348296&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> model <span class=\"ansi-blue-fg\">=</span> tf<span class=\"ansi-blue-fg\">.</span>keras<span class=\"ansi-blue-fg\">.</span>models<span class=\"ansi-blue-fg\">.</span>load_model<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>model<span class=\"ansi-blue-fg\">.</span>save<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py</span> in <span class=\"ansi-cyan-fg\">save</span><span class=\"ansi-blue-fg\">(self, filepath, overwrite, include_optimizer, save_format, signatures, options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1977</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1978</span>     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n<span class=\"ansi-green-fg\">-&gt; 1979</span><span class=\"ansi-red-fg\">                     signatures, options)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1980</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1981</span>   def save_weights(self,\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py</span> in <span class=\"ansi-cyan-fg\">save_model</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer, save_format, signatures, options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>           &#39;or using `save_weights`.&#39;)\n<span class=\"ansi-green-intense-fg ansi-bold\">    130</span>     hdf5_format.save_model_to_hdf5(\n<span class=\"ansi-green-fg\">--&gt; 131</span><span class=\"ansi-red-fg\">         model, filepath, overwrite, include_optimizer)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    132</span>   <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    133</span>     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py</span> in <span class=\"ansi-cyan-fg\">save_model_to_hdf5</span><span class=\"ansi-blue-fg\">(model, filepath, overwrite, include_optimizer)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>   <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>     <span class=\"ansi-green-fg\">if</span> opened_new_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 130</span><span class=\"ansi-red-fg\">       </span>f<span class=\"ansi-blue-fg\">.</span>close<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/h5py/_hl/files.py</span> in <span class=\"ansi-cyan-fg\">close</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    441</span>                 <span class=\"ansi-green-fg\">for</span> id_ <span class=\"ansi-green-fg\">in</span> file_list<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>                     <span class=\"ansi-green-fg\">while</span> id_<span class=\"ansi-blue-fg\">.</span>valid<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 443</span><span class=\"ansi-red-fg\">                         </span>h5i<span class=\"ansi-blue-fg\">.</span>dec_ref<span class=\"ansi-blue-fg\">(</span>id_<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    444</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    445</span>                 self<span class=\"ansi-blue-fg\">.</span>id<span class=\"ansi-blue-fg\">.</span>close<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/_objects.pyx</span> in <span class=\"ansi-cyan-fg\">h5py._objects.with_phil.wrapper</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">h5py/h5i.pyx</span> in <span class=\"ansi-cyan-fg\">h5py.h5i.dec_ref</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">RuntimeError</span>: Dirty entry flush destroy failed (file write failed: time = Thu Mar 18 09:52:16 2021\n, filename = &#39;/dbfs/FileStore/shared_uploads/wu.wenjun@otis.com/Model/best_model.h5&#39;, file descriptor = 33, errno = 95, error message = &#39;Operation not supported&#39;, buf = 0x562a4793e070, total write size = 4096, bytes this sub-write = 4096, bytes actually written = 18446744073709551615, offset = 2048)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["predict = model.predict(val_ds)\npredict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b0a55cc-02da-489b-96e1-f1be81638db0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[33]: array([[1.0352677e-08, 1.0000000e+00],\n       [4.4936731e-11, 1.0000000e+00],\n       [9.9997604e-01, 2.3958348e-05],\n       ...,\n       [9.9999988e-01, 1.2724522e-07],\n       [2.1866257e-05, 9.9997818e-01],\n       [1.0000000e+00, 3.9633860e-10]], dtype=float32)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: array([[1.0352677e-08, 1.0000000e+00],\n       [4.4936731e-11, 1.0000000e+00],\n       [9.9997604e-01, 2.3958348e-05],\n       ...,\n       [9.9999988e-01, 1.2724522e-07],\n       [2.1866257e-05, 9.9997818e-01],\n       [1.0000000e+00, 3.9633860e-10]], dtype=float32)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["predict.shape"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e72fcdef-f7f8-4461-bf85-ae5917e57a3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[34]: (1582, 2)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: (1582, 2)</div>"]}}],"execution_count":0},{"cell_type":"code","source":["y_true = []\ny = []\nval_list = list(val_ds)\nfor length in range(len(val_list)):\n  y_true.append(list(np.array(val_list[length][1])))\nfor i in y_true:\n  for t in i:\n    y.append(t)\ny"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b566cae-3084-4f9b-99c4-fccada63779a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[35]: [1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n ...]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[35]: [1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n ...]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["y_predict = []\nfor i in predict:\n  i = i.tolist()\n  y_predict.append(i.index(max(i)))\ny_predict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56829888-a27a-43f3-93bc-358db7b2fd2c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: [1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n ...]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: [1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 1,\n ...]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\nytest = np.array(y)\ny_pred = np.array(y_predict)\nprint('Accuracy score: ', accuracy_score(ytest,y_pred))\nprint('Precision score: ',precision_score(ytest,y_pred,average='macro'))\nprint('Recall: ', recall_score(ytest,y_pred,average='macro'))\nprint('F1 score: ',f1_score(ytest,y_pred,average='macro'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b1137e7-6cbe-4f96-9e23-b440ef9f2cf7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Accuracy score:  0.9816687737041719\nPrecision score:  0.9816697466420268\nRecall:  0.9816304459120462\nF1 score:  0.9816497028346205\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Accuracy score:  0.9816687737041719\nPrecision score:  0.9816697466420268\nRecall:  0.9816304459120462\nF1 score:  0.9816497028346205\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["base_path = '/dbfs/FileStore/shared_uploads/lilian.lu@otis.com/'\n\nimg_path = DATA_PATH\n# img_path = '/dbfs/FileStore/shared_uploads/lilian.lu@otis.com/images/11_26'\nmodel_path = base_path+\"training/cp-0066.ckpt\"\n# CLASS_PEOPLE = 'checkpoint/people_class'\n\nmodel = load_model(model_path)\nresult, test_ds, scores = predict(img_path, model)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05fb8785-f608-41fd-941d-68fbf41626d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["y_p = []\nfor item in result:\n  y_p.append(np.argmax(item))\n\ny_r = []\nfor x, y in test_ds:\n    y_n = y.numpy()\n    y_r.extend(y_n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ec28651-f9a6-4f44-a2d6-2cdc2327efe4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["m_p = tf.keras.metrics.Precision()\nm_r = tf.keras.metrics.Recall()\n# m_auc = tf.keras.metrics.AUC()\n\nm_p.update_state(y_r, y_p)\nm_r.update_state(y_r, y_p)\n# m_auc.update_state(y_r, y_p)\n\nprecision = m_p.result().numpy()\nrecall = m_r.result().numpy()\n# auc = m_auc().result.numpy()\nprint(\"precision:%s\\nrecall:%s\"%(precision, recall))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20e22343-1f69-4277-93b3-0827b37ef06e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"model_code_0318V3","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2394673246195112}},"nbformat":4,"nbformat_minor":0}
